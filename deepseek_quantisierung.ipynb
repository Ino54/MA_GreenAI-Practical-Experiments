{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ino54/MA_GreenAI-Practical-Experiments/blob/main/deepseek_quantisierung.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Rzsu1BBlSab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce3f7e4-4fb8-4403-a980-a0dae9d9bac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "# ============= Requirements =============\n",
        "%%writefile requirements.txt\n",
        "transformers>=4.44\n",
        "accelerate>=0.33\n",
        "bitsandbytes\n",
        "datasets>=2.20\n",
        "evaluate>=0.4\n",
        "sacrebleu>=2.4\n",
        "codecarbon>=2.5,<3\n",
        "pynvml>=12,<13\n",
        "psutil\n",
        "numpy==2.0.2\n",
        "pandas==2.2.2\n",
        "huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U -r requirements.txt --no-warn-conflicts\n",
        "!pip uninstall -y -q google-genai firebase-admin || true"
      ],
      "metadata": {
        "id": "AW4li4E5lhzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a8681f-d620-4bb7-cb7b-e8006bdf12f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.6/517.6 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.2/291.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= Drive mount + Ordnercheck =============\n",
        "import os, shutil, time, pathlib, platform\n",
        "from google.colab import drive\n",
        "MOUNTPOINT=\"/content/drive\"\n",
        "already=os.path.isdir(os.path.join(MOUNTPOINT,\"MyDrive\"))\n",
        "if not already and os.path.isdir(MOUNTPOINT) and os.listdir(MOUNTPOINT):\n",
        "    backup=f\"/content/drive_stale_{int(time.time())}\"\n",
        "    shutil.move(MOUNTPOINT, backup)\n",
        "    os.makedirs(MOUNTPOINT, exist_ok=True)\n",
        "drive.mount(MOUNTPOINT, force_remount=(not already))\n",
        "\n",
        "work_dir=\"/content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/deepseek_quantisierung\"\n",
        "if not os.path.isdir(work_dir):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Zielordner fehlt: {work_dir}\\n\"\n",
        "        \"Bitte diesen Ordner manuell in Google Drive anlegen und das Notebook erneut starten.\"\n",
        "    )\n",
        "os.chdir(work_dir)\n",
        "project_dir=work_dir\n",
        "print(\"Arbeitsordner:\", os.getcwd())"
      ],
      "metadata": {
        "id": "nCL3QRBHlzDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd653ac3-67d5-412b-be8c-2cf46dcefd07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Arbeitsordner: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/deepseek_quantisierung\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= HF-Login =============\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(hf_token); print(\"Hugging Face Login erfolgreich!\")\n",
        "else:\n",
        "    print(\"WARNUNG: Kein HF_TOKEN gefunden.\")"
      ],
      "metadata": {
        "id": "_uhOvmQ2ly-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b96aa93-f19f-4e7f-853d-cfa82b9676b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face Login erfolgreich!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= Imports & Setup =============\n",
        "import re, math, gc, warnings, inspect\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np, pandas as pd, torch, psutil\n",
        "from contextlib import nullcontext\n",
        "from types import SimpleNamespace\n",
        "from typing import Tuple\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed, GenerationConfig\n",
        ")\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "RESULT_BASENAME = \"deepseek_quant\"\n",
        "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "ALIAS = \"r1q15b\"\n",
        "\n",
        "set_seed(42)\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device==\"cuda\":\n",
        "  gpu_name=torch.cuda.get_device_name(0)\n",
        "  vram_total_gb=torch.cuda.get_device_properties(0).total_memory/(1024**3)\n",
        "  torch.backends.cuda.matmul.allow_tf32=True\n",
        "else:\n",
        "  gpu_name=\"CPU\"; vram_total_gb=0.0\n",
        "import platform as _pf\n",
        "print(f\"Device: {device} | GPU: {gpu_name} | VRAM={vram_total_gb:.1f} GB | Torch {torch.__version__} | Py {_pf.python_version()}\")"
      ],
      "metadata": {
        "id": "Z0-BAfesly6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e86fcc8c-d0c4-4f69-b9c2-d0a42af5fe33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | GPU: NVIDIA A100-SXM4-80GB | VRAM=79.3 GB | Torch 2.8.0+cu126 | Py 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= CodeCarbon Helpers (Berlin) =============\n",
        "import os as _os, time as _time\n",
        "\n",
        "def _cc_supported_kwargs():\n",
        "    base=dict(log_level=\"error\", output_dir=\".\", measure_power_secs=1, tracking_mode=\"process\")\n",
        "    try:\n",
        "        params=inspect.signature(EmissionsTracker.__init__).parameters\n",
        "        if \"cloud_provider\" in params: base[\"cloud_provider\"]=\"google\"\n",
        "        if \"cloud_region\"  in params: base[\"cloud_region\"]=\"europe-west10\"  # GCP Berlin\n",
        "        if \"country_iso_code\" in params: base[\"country_iso_code\"]=\"DEU\"\n",
        "    except Exception: pass\n",
        "    return base\n",
        "\n",
        "def make_tracker(name,out): return EmissionsTracker(project_name=name, output_file=out, **_cc_supported_kwargs())\n",
        "\n",
        "def safe_start(tr):\n",
        "    try: tr.start(); return True\n",
        "    except Exception as e: print(\"[CodeCarbon] Start fehlgeschlagen:\", e); return False\n",
        "\n",
        "def safe_stop(tr, started):\n",
        "    if not started: return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "    try: return tr.stop()\n",
        "    except Exception as e: print(\"[CodeCarbon] Stop fehlgeschlagen:\", e); return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "\n",
        "def unpack(em):\n",
        "    if hasattr(em,\"energy_consumed\") and hasattr(em,\"emissions\"):\n",
        "        try: return float(em.energy_consumed), float(em.emissions)\n",
        "        except: pass\n",
        "    if isinstance(em, dict):\n",
        "        e=em.get(\"energy_consumed\",0.0); c=em.get(\"emissions\", em.get(\"emissions_kg\",0.0))\n",
        "        try: return float(e), float(c)\n",
        "        except: return 0.0,0.0\n",
        "    try: return 0.0, float(em)\n",
        "    except: return 0.0,0.0\n",
        "\n",
        "def read_energy_from_log(path):\n",
        "    try:\n",
        "        if not os.path.exists(path): return 0.0\n",
        "        df=pd.read_csv(path)\n",
        "        for c in [\"energy_consumed\",\"energy_consumed_kwh\",\"energy_consumed (kWh)\",\"energy (kWh)\"]:\n",
        "            if c in df.columns: return float(df[c].iloc[-1])\n",
        "        for c in df.columns:\n",
        "            if \"energy\" in c.lower() and \"kwh\" in c.lower(): return float(df[c].iloc[-1])\n",
        "    except: pass\n",
        "    return 0.0\n",
        "\n",
        "def measure_phase(phase, fn, prefix):\n",
        "    logfile=os.path.join(project_dir, f\"{prefix}_{phase}.csv\")\n",
        "    tr=make_tracker(f\"{prefix}_{phase}\", logfile)\n",
        "    import time as _t\n",
        "    st=safe_start(tr); t0=_t.time(); res=fn(); t1=_t.time()\n",
        "    e=safe_stop(tr, st); ekwh, co2=unpack(e)\n",
        "    if ekwh==0.0:\n",
        "        ek=read_energy_from_log(logfile)\n",
        "        if ek: ekwh=ek\n",
        "    return {\"phase\":phase,\"time_s\":t1-t0,\"energy_kwh\":ekwh,\"co2_kg\":co2}, res"
      ],
      "metadata": {
        "id": "5xFoPDv4ly0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= Eval-Konfiguration =============\n",
        "EVAL={\"max_new_tokens\":32,\n",
        "      \"ppl\":{\"name\":\"wikitext\",\"config\":\"wikitext-2-raw-v1\",\"split\":\"test[:1%]\"},\n",
        "      \"bleu\":{\"name\":\"wmt14\",\"config\":\"de-en\",\"split\":\"test[:32]\"}}\n",
        "\n",
        "def parse_subset_count(s, default=32):\n",
        "  m=re.search(r\":\\s*(\\d+)\\s*\\]$\", s or \"\"); return int(m.group(1)) if m else default\n",
        "BLEU_N=parse_subset_count(EVAL[\"bleu\"][\"split\"], 32)\n",
        "\n",
        "PROMPTS=[\n",
        "  \"Schreibe einen kurzen Absatz über nachhaltige KI.\",\n",
        "  \"Erkläre in einfachen Worten, was Quantisierung in neuronalen Netzen ist.\",\n",
        "  \"Nenne drei Vorteile von Mixture-of-Experts-Modellen.\"\n",
        "]\n",
        "bleu_metric=evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "PzSTnm9TmHvf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2795f556b4f34645aeaabb97bc00cc40",
            "d5de02f16dd44b2ba57da2629d750a36",
            "ee17b6fd40c443ccaa47d9ea18746598",
            "32ad27c199ef47fdb94e985f15022e81",
            "f548cb19c55a4e9f96421dd204af5803",
            "40465849374b4365bf420067a414df82",
            "bb795e2086c04773a6c3f1a1b7006b41",
            "5e785d343abf4344b2771da97126e0d0",
            "f796be3c47c143b395c3996486a2c3e5",
            "47cd5e5179564a5c8defc8ef32d2bf07",
            "6481bf22566742b293c10181723d7c46"
          ]
        },
        "outputId": "92a050da-462e-46f9-c975-6bbcbb266170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2795f556b4f34645aeaabb97bc00cc40"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= Helper-Funktionen =============\n",
        "def autocast_ctx():\n",
        "  return torch.autocast(device_type=\"cuda\", dtype=torch.float16) if device==\"cuda\" else nullcontext()\n",
        "def bytes_to_gb(b): return float(b)/(1024**3)\n",
        "def capture_memory():\n",
        "  ram=psutil.Process().memory_info().rss\n",
        "  valloc=torch.cuda.memory_allocated() if device==\"cuda\" else 0\n",
        "  vres =torch.cuda.memory_reserved()  if device==\"cuda\" else 0\n",
        "  return ram, valloc, vres\n",
        "def safe_max_len(tok, model, fallback=2048, upper=100000):\n",
        "  cand=getattr(tok,\"model_max_length\",None)\n",
        "  if isinstance(cand,int) and 0<cand<upper: return cand\n",
        "  cand=getattr(getattr(model,\"config\",None),\"max_position_embeddings\",None)\n",
        "  if isinstance(cand,int) and 0<cand<upper: return cand\n",
        "  return fallback\n",
        "\n",
        "# Warnungsfreie, deterministische Generation\n",
        "GC_GREEDY = GenerationConfig(\n",
        "    do_sample=False,\n",
        "    temperature=None,\n",
        "    top_p=None,\n",
        "    top_k=None,\n",
        "    num_beams=1,\n",
        ")\n",
        "\n",
        "def warmup(model, tok, max_len):\n",
        "  with torch.no_grad(), autocast_ctx():\n",
        "    x=tok(\"Hello\", return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
        "    _=model.generate(**x, max_new_tokens=1, generation_config=GC_GREEDY, pad_token_id=tok.eos_token_id)\n",
        "\n",
        "def do_gen(model, tok, max_new_tokens):\n",
        "  total, texts=0, []\n",
        "  ml=safe_max_len(tok, model)\n",
        "  for p in PROMPTS:\n",
        "    enc=tok(p, return_tensors=\"pt\", truncation=True, max_length=ml).to(model.device)\n",
        "    room=ml-enc[\"input_ids\"].shape[1]\n",
        "    cur_new=max(1, min(max_new_tokens, int(room)))\n",
        "    with torch.no_grad(), autocast_ctx():\n",
        "      out=model.generate(**enc, max_new_tokens=cur_new, generation_config=GC_GREEDY, pad_token_id=tok.eos_token_id)\n",
        "    total+=int(out.shape[1]-enc[\"input_ids\"].shape[1])\n",
        "    texts.append(tok.decode(out[0], skip_special_tokens=True))\n",
        "  return texts, total\n",
        "\n",
        "def do_ppl(model, tok, ds_cfg):\n",
        "  ds=load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "  ml=safe_max_len(tok, model); losses=[]\n",
        "  with torch.no_grad():\n",
        "    for t in ds[\"text\"]:\n",
        "      if not isinstance(t,str) or len(t.strip())<4: continue\n",
        "      enc=tok(t, return_tensors=\"pt\", truncation=True, max_length=ml).to(model.device)\n",
        "      with autocast_ctx():\n",
        "        out=model(enc[\"input_ids\"], labels=enc[\"input_ids\"])\n",
        "      losses.append(float(out.loss.detach().cpu()))\n",
        "  return math.exp(np.mean(losses)) if losses else None\n",
        "\n",
        "def do_bleu(model, tok, ds_cfg, max_new_tokens):\n",
        "  ds=load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "  ml=safe_max_len(tok, model); preds, refs=[], []\n",
        "  with torch.no_grad():\n",
        "    for ex in ds:\n",
        "      de,en=ex[\"translation\"][\"de\"], ex[\"translation\"][\"en\"]\n",
        "      prompt=f\"Translate to English:\\nGerman: {de}\\nEnglish:\"\n",
        "      enc=tok(prompt, return_tensors=\"pt\", truncation=True, max_length=ml).to(model.device)\n",
        "      room=ml-enc[\"input_ids\"].shape[1]\n",
        "      cur_new=max(1, min(max_new_tokens, int(room)))\n",
        "      with autocast_ctx():\n",
        "        out=model.generate(**enc, max_new_tokens=cur_new, generation_config=GC_GREEDY, pad_token_id=tok.eos_token_id)\n",
        "      gen=tok.decode(out[0], skip_special_tokens=True)\n",
        "      hyp=gen.split(\"English:\")[-1].strip().split(\"\\n\")[0].strip() or gen.strip()\n",
        "      preds.append(hyp); refs.append([en])\n",
        "  return float(bleu_metric.compute(predictions=preds, references=refs)[\"score\"])"
      ],
      "metadata": {
        "id": "aiCZ7jlxmJA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= Quantisiertes Laden (8-bit / 4-bit) =============\n",
        "def load_quantized(model_id: str, variant: str) -> Tuple[AutoTokenizer, AutoModelForCausalLM, str]:\n",
        "  tok=AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "  tok.padding_side=\"left\"\n",
        "  if tok.pad_token_id is None:\n",
        "    tok.pad_token=tok.eos_token\n",
        "  if variant==\"8bit\":\n",
        "    bnb=BitsAndBytesConfig(load_in_8bit=True)\n",
        "  elif variant==\"4bit\":\n",
        "    bnb=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "                           bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float16)\n",
        "  else:\n",
        "    raise ValueError(\"variant must be '8bit' or '4bit'\")\n",
        "  if device==\"cuda\": torch.cuda.empty_cache(); gc.collect()\n",
        "  model=AutoModelForCausalLM.from_pretrained(\n",
        "      model_id, device_map=\"auto\", quantization_config=bnb, attn_implementation=\"sdpa\"\n",
        "  )\n",
        "  return tok, model, variant"
      ],
      "metadata": {
        "id": "ZyKGIxt_mI8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= Einzellauf (gen/ppl/bleu) =============\n",
        "def run_once(model_id, alias, variant):\n",
        "  tok, model, prec=load_quantized(model_id, variant)\n",
        "  ml=safe_max_len(tok, model); warmup(model, tok, ml)\n",
        "  prefix=f\"{RESULT_BASENAME}_{alias}_{variant}\"\n",
        "\n",
        "  m_gen,(samples,n_tok)=measure_phase(\"gen\",  lambda: do_gen(model,tok,EVAL[\"max_new_tokens\"]), prefix)\n",
        "  m_ppl, p            =measure_phase(\"ppl\",  lambda: do_ppl(model,tok,EVAL[\"ppl\"]),             prefix)\n",
        "  if device==\"cuda\": torch.cuda.empty_cache()\n",
        "  m_bleu, b           =measure_phase(\"bleu\", lambda: do_bleu(model,tok,EVAL[\"bleu\"],EVAL[\"max_new_tokens\"]), prefix)\n",
        "\n",
        "  time_s=m_gen[\"time_s\"]+m_ppl[\"time_s\"]+m_bleu[\"time_s\"]\n",
        "  energy=m_gen[\"energy_kwh\"]+m_ppl[\"energy_kwh\"]+m_bleu[\"energy_kwh\"]\n",
        "  co2   =m_gen[\"co2_kg\"]+m_ppl[\"co2_kg\"]+m_bleu[\"co2_kg\"]\n",
        "  ram, valloc, vres = capture_memory()\n",
        "\n",
        "  row=dict(model_id=model_id, alias=alias, variant=variant, precision=prec,\n",
        "           time_s=time_s, energy_kwh=energy, co2_kg=co2,\n",
        "           kg_per_kwh=(co2/energy if energy else None),\n",
        "           tokens_out=int(n_tok), ppl=p, bleu=b,\n",
        "           ram_GB=bytes_to_gb(ram), vram_alloc_GB=bytes_to_gb(valloc), vram_reserved_GB=bytes_to_gb(vres),\n",
        "           notes=f\"GPU={gpu_name}, VRAM={vram_total_gb:.1f} GB\")\n",
        "  return row, samples, (m_gen,m_ppl,m_bleu)"
      ],
      "metadata": {
        "id": "gGx_re9kmI3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= Ausführung (8-bit & 4-bit) + Speichern =============\n",
        "variants = [\"8bit\",\"4bit\"]\n",
        "all_rows, phase_tables = [], []\n",
        "\n",
        "for variant in variants:\n",
        "  print(f\"\\n### {ALIAS} – {variant}\")\n",
        "  row, samples, phases = run_once(MODEL_ID, ALIAS, variant)\n",
        "  all_rows.append(row)\n",
        "  dfp=pd.DataFrame(list(phases)); dfp[\"alias\"]=ALIAS; dfp[\"variant\"]=variant\n",
        "  dfp[\"kg_per_kwh\"]=(dfp[\"co2_kg\"]/dfp[\"energy_kwh\"]).replace([np.inf,-np.inf],np.nan)\n",
        "  phase_tables.append(dfp)\n",
        "  with open(os.path.join(project_dir, f\"{RESULT_BASENAME}_samples_{ALIAS}_{variant}.txt\"),\"w\",encoding=\"utf-8\") as f:\n",
        "    for i,t in enumerate(samples,1): f.write(f\"--- Beispiel {i} ---\\n{t}\\n\\n\")\n",
        "\n",
        "df=pd.DataFrame(all_rows)\n",
        "df_phase=pd.concat(phase_tables, ignore_index=True)\n",
        "df.to_csv(os.path.join(project_dir, f\"{RESULT_BASENAME}_results.csv\"), index=False)\n",
        "df_phase.to_csv(os.path.join(project_dir, f\"{RESULT_BASENAME}_results_per_phase.csv\"), index=False)\n",
        "print(\"\\nErgebnisse:\\n\", df)\n",
        "print(\"\\nPer-Phase:\\n\", df_phase)\n",
        "print(\"Gespeichert:\", os.path.join(project_dir, f\"{RESULT_BASENAME}_results.csv\"))"
      ],
      "metadata": {
        "id": "GYg898BymIxz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c8f41c378554487d9dbcb780d36c149f",
            "0a83451f5d144dcd9e028eb624b0a115",
            "114a3f8677b043a395ade1707132e76a",
            "05f873c3495d4e74b839e3f3b442ed2d",
            "718bd1ed178742a09ffae4ad25f12a3a",
            "6d80a4611ff54827a225f7810dc0a955",
            "26fe3014fa0945a3823f235c6e98ff9e",
            "2fa562a6e0d3419d9461941e0f19d30e",
            "d60537165983486ca0f3beb5c870efa1",
            "eb70611e964c44ab9d207b70ebaefe4f",
            "e145b43b9dee4a2095ff82d5b7fbfed9",
            "13c1a071033e468e813713aad1a74792",
            "aeb3def4aa4648e9a4aa189b5d335f0b",
            "61ef0d15be8e4470908b646bae3c880a",
            "971acf2e2ece42f3bc6bad78084c4bb7",
            "41115678eddb4ac6a42ac490a781ebeb",
            "a7034ba353ed40589203e2f44534d910",
            "ae5eba7d8b984c91b435e0d332002be0",
            "30a16daf2caf4439b0f8cf31b5e7ca7e",
            "b056e9f2d8514cf5b8590946618e0c81",
            "cba0f759f08443f5b3c5a8f5217d0f24",
            "f7578ca284d74cf18434b71fa2864bec",
            "0d3316c8da0140e79615c369ab9bb146",
            "6832f416c0be41d3b245c0d7154e0793",
            "01a5d20dfb7c4e128e7cc91c8caad7f1",
            "11111a45ed5d48c48dee4c65d696aa5f",
            "134a23fbfb8e4057a8d7cd6646686506",
            "9189c6c013474c53b00386a95859273c",
            "75e1c6537cd84777be281dde10666e29",
            "bb70493b340a4814bf7601e1831a753c",
            "fcae250796c04d1b8c5baf7e8b92cb49",
            "dd6a5bc587c44f2291659a1062614cca",
            "1e8425e0a8c04dc09030f57d470dbc8e",
            "05fe33da804e49ac8f9f3eeab666fdd8",
            "a329fb4459e64b1abab414b0c81125cc",
            "4e566bdb3d464f8f98abfc8625b35725",
            "1f49bc90c60049719ee3fcb50af7beb4",
            "a52b25940b2543fda183574a4b228fc6",
            "a998306cffce4b35963b958b2777e0dd",
            "c35c04d6127240bc9c8ec05cb3ebf79b",
            "0dfc6738b0d64b82865032efe6d3d4a9",
            "514fc3f59a73405796a9d38423a4be3a",
            "179f1303edf24e1c9cd8d63284a2963f",
            "ae64a2b387bb40f2ad6071ac8cc9a482",
            "e21b9a53ab8e46a0bf471fcebe9b6f32",
            "22ab643ef513406293a1709798e9e6de",
            "a593ec4138fe42b0bf2a71dcc079018f",
            "ebd90c717d4842f184db4d1ba34eadac",
            "2ec750d829054259a64b2c7328b51fc8",
            "009859fbfb9646f5ac722fadb0ed1595",
            "08c6acfb71e1446ba9d991f8f10c0b9c",
            "73fa8ed1368247b6ba2d3af60331f064",
            "7f149db5120d4d54b3c89bfb9f69ba18",
            "b059cf4647144f5088887cd718aa1e3c",
            "4f6906566f524b599cda64f506a4f39c",
            "4e5e7bc818624d318de71c156d5920de",
            "20c36ecc8db44652a2fee65560b01178",
            "30a8ec80057440c2bfb439408b622502",
            "2c20872f1e3f4ae0b79994f4e672734f",
            "aab813425e9445ce971b5c03a82f26b2",
            "623dc9dbf9b642d4b04a7c1cd40eed48",
            "e15f156bfbd04ff7b733d6bf4607f141",
            "2f6955036ffe49ccb41954037103975c",
            "8098a4f0d3354af893f4d4e52a3bbe4a",
            "8b5df271c5b04b86aaa5b4a2a2d4829c",
            "85fd72cd29424679a69332f61be049fa",
            "e12cb99f3b7642c4bdee13e442a0e66c",
            "c7335fe41af14f7fad20c83a6738f0ae",
            "49f1911d93f24ccfa7adab2a703a23dc",
            "a2ccff6455ad4eb8984f35d16ae9e721",
            "e5ef6c3da5f548a1a99fe676ac487fc8",
            "c9d713c37eb44c08b4a41a8ad7960de7",
            "6be6e563d9e544ff98e314fef000a59f",
            "f82f4af98be74f99b86285de1c889b78",
            "ba009a573ffc47b7bd7f0be4ac1a29ce",
            "54849126d52e4608b580fb2948bc2292",
            "2c5f58d7ac284164b49c399626d7a40e",
            "9a976c84ad5e425ca226384720cbda6d",
            "86d504f16a2b4c09a3d4fb11953ccec3",
            "38d72aca5a1a4c01badcdec3903dba6a",
            "c33533ced44a4308b82641104591576c",
            "64fd11511c8545fd9c93eec2494a4c2b",
            "1533b16a48f549b4a249a144fb9c5579",
            "841117e042884553aaa490cdfaadf816",
            "48fb145aa4a24128bb36402524f82cf1",
            "c179297419b24e3789d5f06772a0d999",
            "8a03d22ba44d4aaca9e6cfd4efea95c6",
            "4427de778bf94a8885af8dc83adaaae2",
            "c89355fa4146414b9311cebfb8475118",
            "6ef27f44cf3642448eea10e10bec054c",
            "9ea937e074c849899d7f64c95c2ec32a",
            "88f2be06bd0a402d88a195bab3e79768",
            "e12ce2442ce542e88a89d17d97905936",
            "2464f8c82fac44a3bd9f2a16d8e1c3f2",
            "11f8e8ef68b347919208ce66d2825c02",
            "abfb514e39f84a689a7284251be8f690",
            "e95344892e7b4e1c9154af341c7ebd7a",
            "c752a1fe95d540c8aeab9aa67aad5376",
            "00f4c3d81d1e4dcc8c639d19ab1e2085",
            "7b581c027f3f41789d558a252f95d3c4",
            "d6e454c487664eefb049488d1148b3f5",
            "2aad29fd3d8245ddb4aeb6eb1c00495c",
            "cdc91b7b36f24bf8b5c76823c5b71de5",
            "e3b664aeb6134d4186fdc0517f9447ef",
            "5bc4db6e8f2e48edbeaaf40550c505d6",
            "c0dd57cf445d4848b375f2c882c23955",
            "47c899a8d10b422ba5fdcc576c1118f5",
            "53e8ce96d7804790a219f11d253245fe",
            "8a08544e68c84fb4adb0d892cffd5516",
            "7e547628e1a64ca5b7a0ded939dd81dd",
            "b84943bee4d04308a0d14c2fa6b17f95",
            "afbf8acd934144d99b1b849ee9f36c25",
            "aa7c4c5f612148f9ab275317cbcad47f",
            "0486af33d5a644f8b50c3f04d3f49539",
            "fed082a9915c4c6ab6c2a87d3d23504d",
            "7eb77a12000846c497a5fe39252e2c73",
            "408bb16c363e457ab1f1925bca173fda",
            "e7b4b03638ec436e970dfcaf1346cfc0",
            "c8f503f78e1f4d62b1a974243f3da2ea",
            "1453addb78d14b68ad73f41e734a8a9d",
            "8b55abe8358349ac82971283b813df6d",
            "8fc74d1224504738af3c235b66a7710a",
            "db8708b3852748d691c08ef39c0bd9e1",
            "d16082c4f5da40e395caf9d2c45ce52f",
            "051976a3fe7c451e8fa2547adc9fdd8d",
            "37f34f80640c4961a48388db26fa301b",
            "0317b383e8714b63a0a127af01252ea0",
            "1046f90acac449989d416677572674f4",
            "f5773c2eccaf475695d6b8a246578cd3",
            "a18f354374fd4a3b987b191a5bcfd88f",
            "571fb777eb6e406aaf4e4972b4db23ab",
            "abb4eea6ee46495ca096c2e8060b5842",
            "48747224fc964f75b2c57989a73c6689",
            "47ac1fda890f44d19635917add631e4e",
            "a7d70a2d11f74892a4180b054b4af9f0",
            "8b52da2961734d76bff0c6f8e878df4f",
            "33c1a9b98a09412690317d9fb113fd0f",
            "1eb081ca1e8f4a289a83a2c72c21b2f8",
            "a18d806163db43eb9af29bf1ea2ef5ac",
            "50d42131829245f5bf5711a2bfc1634d",
            "80ed79c7c4994152bd2348f6ae8b636e",
            "0c3d05c9489b4db6a69f71af88a9cdf4",
            "9732ff44812f4e6ea02fd8f585c8034e",
            "5250705202d1421ea3459f4daab64ef3",
            "4a9b27a9aa604f6089f741bcb8cf75da",
            "90d7e4af813d42ad9bc864078f4e2302",
            "b78e09b86c1040bc946403487f030c6b",
            "26f10eba595543b0bba4668b8875b1e4",
            "e927b4f8a3af4ef5a8c5dcb22f0433c4",
            "ca8479a3b85d4339a08b31b375f5669e",
            "fe671f241d6a46f48532314ebe050eec",
            "62226fd0eeca43859fd553038c75dd6a",
            "3d44bccb13ec4bbf8813056387253c86",
            "49b235ca9d354c3e94a3008ff9f71399",
            "957375484b7247228361df163bee9f57",
            "5d4c3b810c574247a1d467effcb4cc6b",
            "38bf8d1cb6074570a8a2d753f333f273",
            "eece455105514296995f797c6cae5131",
            "63c97a24f6bc4af18e8ff6bc1ac321c6",
            "fe29bb5d8ba043d8aaf08d29915f5529",
            "8150d2035b944b1eab27b333619da82f",
            "7b3f186f0de94c269006108bc032590a",
            "c4278a72511e49feba1f724ff48d0649",
            "2bf2c2822d5d4fbabda271e811654553",
            "b022cf5237e9400ea0303a9a86bcf3d8",
            "56c243a6efff4fe3b8cdbb70772f0ba9",
            "b832fb20d9b44b7593a010acce2d53b7",
            "492ccf71738c4e6cac82b59c6cbd2f04",
            "71e856a08cac42c2a6117c29d931d25b",
            "293b35bfe38f4a4e87c07d1b5ccd2340",
            "fc8bf72547f444709f0637be09bd0c01",
            "626e392e751941f59d3b38bf5269473e",
            "4073e072d87942709166dd34b5a8c0ba",
            "a064d320dcd24949a5625fb05c052d0d",
            "bdc8c4867f264575b66b6259df269b49",
            "f112bd26daf04559987eb12b2b365c08",
            "5cb8890e57d44f95970ade8726cea430",
            "e69e08ce150e45148b5ceb9e6f4e51da",
            "5039a05e902140c38b08d293cf70f170",
            "e50dc0baa9bb4b7f89d48cb69e7416f8",
            "7b86d283bd5e4ea1a4cd05a5a8373d9a",
            "b2763eab422943c591d32cab433382b4",
            "43af646f623341c1bdc10f9682b395bc",
            "d82a278870b041daad49f34d3e6b276e",
            "5c63e97b5c814310aaee0524f833b17f",
            "a2e097abf21144b3bbed089d90022eae",
            "b8cd94bcb9ce43f6ad8948b1d2ecaac4",
            "becc4d171a4b44b8942bc2564c69af4a",
            "cd609aa3e22b4595bb47b56aad959d66",
            "5b88887936f346568630e6aa1fdc7e0a",
            "db7628f840b846f8b0d9607b65272924",
            "2fe0449d20d2485b918a841634815e36",
            "b612f3a2aaf94a028a6208e2225f90fc",
            "3c29437fd289424fbbd998b5881abf36",
            "f183528f69fb482791e770dd32919846",
            "b1c940b672424593a4492b2b9bcafa15",
            "cc025be56bfd4d108062cfceaf877abf",
            "c0945e9e803c43529b810629faaf53dc",
            "42f31b9aff8a41d8907fbd4af7ecf175",
            "a7a6591bbcca4f9bb677cbca8fc10b83",
            "b98d661cfb4c4ef18db328e2dc3c0efb",
            "b3ee5157eeed40b3a5a093a6449d6de6",
            "6edb3c427890431e9541cab215b961a9",
            "7ad425048c4d4069a3662193d39e79c7",
            "bc871a094aad420f9e4102864e400f30",
            "98ab6995c6ef4d6abf0c4797fb76aab9",
            "4cfe6dc7cbd74669965045b63080ab7e",
            "6e425b72ab714477b72b7f3dec0e4bda",
            "6817d702b83d46b9a0054a7fdcbb60cd",
            "199a65fd58b3453ab6259848df3b37d7",
            "8e5d93d4b77949b3ab61ffc27ef2586d",
            "a0d773b6fcf64bb1a1e86d21ecebddb8",
            "8e545de289a3444281ddcc8689201f1d",
            "58fc2dc88cb641bd8c9064b3a72e9342",
            "308b858d5170484b9fea7e53befecf7a",
            "8df5b31eeccf4d79a83864379fc0328b",
            "a35618ce061e4ec9bd082d06d69ad2ff",
            "c816451fee3745a6bd0e71d61cf182b4",
            "f34fed3c89844aed8297938fa1e043f8",
            "1e6d7f6104c44d41b15d6e54df7cf0fd",
            "06ce81cd5b3b4680883451299e9a7e1d",
            "e0bbf6106c784e45a023a04cd4ddabd4",
            "83db9005b8ab4be7a5dbf5b7e46e0b5e",
            "6a8444cd33d440509acb9601dd9fb4e6",
            "d9b871644fc24e27ac62620f3fe842d1",
            "d913d083b3d54d46a0eee788d9d5d9db",
            "df7072ffb7b94204baa9e7ffcf55caf1",
            "53f209bcfc204ed4be35dc4c1182cec1",
            "1d804a86f9fe4b1b962ee666be37bb55",
            "4c1ccd48020c4ca3bac56bd4830227a2",
            "b393186d19254970b0ffae1344ee91b1"
          ]
        },
        "outputId": "f62b7505-6d5c-4576-b47f-fd3ff7cc2eb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### r1q15b – 8bit\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8f41c378554487d9dbcb780d36c149f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13c1a071033e468e813713aad1a74792"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d3316c8da0140e79615c369ab9bb146"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05fe33da804e49ac8f9f3eeab666fdd8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e21b9a53ab8e46a0bf471fcebe9b6f32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e5e7bc818624d318de71c156d5920de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e12cb99f3b7642c4bdee13e442a0e66c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a976c84ad5e425ca226384720cbda6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c89355fa4146414b9311cebfb8475118"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b581c027f3f41789d558a252f95d3c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b84943bee4d04308a0d14c2fa6b17f95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fc74d1224504738af3c235b66a7710a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48747224fc964f75b2c57989a73c6689"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00000-of-00003.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5250705202d1421ea3459f4daab64ef3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00001-of-00003.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "957375484b7247228361df163bee9f57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00002-of-00003.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56c243a6efff4fe3b8cdbb70772f0ba9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/validation-00000-of-00001.parquet:   0%|          | 0.00/474k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cb8890e57d44f95970ade8726cea430"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/test-00000-of-00001.parquet:   0%|          | 0.00/509k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "becc4d171a4b44b8942bc2564c69af4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42f31b9aff8a41d8907fbd4af7ecf175"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "199a65fd58b3453ab6259848df3b37d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06ce81cd5b3b4680883451299e9a7e1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### r1q15b – 4bit\n",
            "\n",
            "Ergebnisse:\n",
            "                                     model_id   alias variant precision  \\\n",
            "0  deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  r1q15b    8bit      8bit   \n",
            "1  deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  r1q15b    4bit      4bit   \n",
            "\n",
            "       time_s  energy_kwh    co2_kg  kg_per_kwh  tokens_out         ppl  \\\n",
            "0  197.950969    0.005993  0.002821    0.470783          96  202.727943   \n",
            "1   79.516219    0.002485  0.001170    0.470783          96  213.635419   \n",
            "\n",
            "        bleu    ram_GB  vram_alloc_GB  vram_reserved_GB  \\\n",
            "0  11.736827  4.578312       2.140763          2.242188   \n",
            "1  11.105658  5.511726       1.519278          1.562500   \n",
            "\n",
            "                                     notes  \n",
            "0  GPU=NVIDIA A100-SXM4-80GB, VRAM=79.3 GB  \n",
            "1  GPU=NVIDIA A100-SXM4-80GB, VRAM=79.3 GB  \n",
            "\n",
            "Per-Phase:\n",
            "   phase      time_s  energy_kwh    co2_kg   alias variant  kg_per_kwh\n",
            "0   gen   13.278282    0.000404  0.000190  r1q15b    8bit    0.470783\n",
            "1   ppl   14.834761    0.000444  0.000209  r1q15b    8bit    0.470783\n",
            "2  bleu  169.837926    0.005144  0.002422  r1q15b    8bit    0.470783\n",
            "3   gen    6.103974    0.000191  0.000090  r1q15b    4bit    0.470783\n",
            "4   ppl    5.825160    0.000180  0.000085  r1q15b    4bit    0.470783\n",
            "5  bleu   67.587085    0.002113  0.000995  r1q15b    4bit    0.470783\n",
            "Gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/deepseek_quantisierung/deepseek_quant_results.csv\n"
          ]
        }
      ]
    }
  ]
}