{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ino54/MA_GreenAI-Practical-Experiments/blob/main/deepseek_frame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thrMsQ4cxw-g",
        "outputId": "3a5711fd-843a-40ec-ac78-023c48a386d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "# ---------- Requirements ----------\n",
        "%%writefile requirements.txt\n",
        "transformers>=4.44\n",
        "accelerate>=0.33\n",
        "bitsandbytes\n",
        "datasets>=2.20\n",
        "evaluate>=0.4\n",
        "sacrebleu>=2.4\n",
        "codecarbon>=2.5,<3\n",
        "pynvml>=12,<13\n",
        "psutil\n",
        "numpy==2.0.2\n",
        "pandas==2.2.2\n",
        "huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U -r requirements.txt --no-warn-conflicts\n",
        "!pip uninstall -y -q google-genai firebase-admin || true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9H1EiiGx-Rr",
        "outputId": "ba1a6b1c-b352-40b3-b7db-20eb4dab381c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.6/517.6 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.2/291.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Hugging Face Login via Colab-Secret ----------\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(hf_token); print(\"Hugging Face Login erfolgreich!\")\n",
        "else:\n",
        "    print(\"WARNUNG: Kein HF_TOKEN gefunden\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUbDaDDIx-OC",
        "outputId": "a9706fa1-404c-48b3-c660-7f03ebbd25de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face Login erfolgreich!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Login, Drive, Ordner ----------\n",
        "import os, shutil, time, pathlib, platform, gc, re, math, warnings, inspect\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from google.colab import drive\n",
        "MOUNTPOINT=\"/content/drive\"\n",
        "already=os.path.isdir(os.path.join(MOUNTPOINT,\"MyDrive\"))\n",
        "if not already and os.path.isdir(MOUNTPOINT) and os.listdir(MOUNTPOINT):\n",
        "    backup=f\"/content/drive_stale_{int(time.time())}\"\n",
        "    shutil.move(MOUNTPOINT, backup); os.makedirs(MOUNTPOINT, exist_ok=True)\n",
        "drive.mount(MOUNTPOINT, force_remount=(not already))\n",
        "\n",
        "work_dir=\"/content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/deepseek_frameworks\"\n",
        "pathlib.Path(work_dir).mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(work_dir); project_dir=work_dir\n",
        "print(\"Arbeitsordner:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVhLS2Lmx-Hd",
        "outputId": "d0059879-b9c7-4876-afa5-a9bc26864e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Arbeitsordner: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/deepseek_frameworks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Imports & Device ----------\n",
        "import numpy as np, pandas as pd, psutil, torch\n",
        "from contextlib import nullcontext\n",
        "from types import SimpleNamespace\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
        "                          BitsAndBytesConfig, GenerationConfig, set_seed)\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "import os as _os\n",
        "_os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True\"\n",
        "\n",
        "set_seed(42)\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device==\"cuda\":\n",
        "    gpu_name=torch.cuda.get_device_name(0)\n",
        "    vram_total_gb=torch.cuda.get_device_properties(0).total_memory/(1024**3)\n",
        "    torch.backends.cuda.matmul.allow_tf32=True\n",
        "    torch.backends.cudnn.benchmark=True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except: pass\n",
        "else:\n",
        "    gpu_name=\"CPU\"; vram_total_gb=0.0\n",
        "print(f\"Device: {device} | GPU: {gpu_name} | VRAM={vram_total_gb:.1f} GB | Torch {torch.__version__} | Py {platform.python_version()}\")\n",
        "\n",
        "RESULT_BASENAME=\"deepseek_framew_4bit\"\n",
        "ALIAS=\"r1q15b\"\n",
        "MODEL_ID=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "ATTN_IMPL=\"sdpa\"  # Qwen/DeepSeek: SDPA performant\n",
        "USE_BF16=(device==\"cuda\" and torch.cuda.get_device_capability(0)[0] >= 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQoovyWSyFvK",
        "outputId": "c8eb0e99-cada-45df-d4fc-7ff6448706b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | GPU: NVIDIA A100-SXM4-40GB | VRAM=39.6 GB | Torch 2.8.0+cu126 | Py 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Eval-Konfiguration ----------\n",
        "EVAL = {\n",
        "    \"max_new_tokens\": 32,\n",
        "    \"ppl\":  {\"name\":\"wikitext\",\"config\":\"wikitext-2-raw-v1\",\"split\":\"test[:1%]\"},\n",
        "    \"bleu\": {\"name\":\"wmt14\",\"config\":\"de-en\",\"split\":\"test[:32]\"},\n",
        "}\n",
        "PROMPTS = [\n",
        "    \"Schreibe einen kurzen Absatz über nachhaltige KI.\",\n",
        "    \"Erkläre in einfachen Worten, was Quantisierung in neuronalen Netzen ist.\",\n",
        "    \"Nenne drei Vorteile von Mixture-of-Experts-Modellen.\"\n",
        "]\n",
        "bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "MAX_LEN_CAP = 128 # verringern auf 192/128 falls VRAM knapp\n",
        "GENCFG = GenerationConfig(do_sample=False, temperature=None, top_p=None, top_k=None, num_beams=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a9247aefa8544cc7af82cb55a661fdfd",
            "b83fb1c39aff4446b16d78ff4208efbc",
            "f5a1778a25bd4b4e930c554a792dd360",
            "c9189820889d45de82b8b6bc74123ff2",
            "520c57017c9e4003a7d6c306f7aa3fe5",
            "8146e851229c44fbbd6ed7a91d60f054",
            "b9ba55d62d9345fd807486f6823f4761",
            "bbe68de545544e38a071d89f486eb7cb",
            "fd410e56048f4b4994cb9657ee88b26f",
            "931cfe837f1747cf9311936166230dd3",
            "e077909a2d59476c8cbb8cc893f224f4"
          ]
        },
        "id": "s49heBoZyFpk",
        "outputId": "a05cfea2-631b-4218-a22e-8714f46d5941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9247aefa8544cc7af82cb55a661fdfd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- CodeCarbon-Helfer ----------\n",
        "def _cleanup_cc_locks():\n",
        "    for p in [\"/tmp/.codecarbon.lock\",\n",
        "              _os.path.expanduser(\"~/.codecarbon/codecarbon.lock\"),\n",
        "              \"/content/.codecarbon/codecarbon.lock\"]:\n",
        "        try:\n",
        "            if _os.path.exists(p): _os.remove(p)\n",
        "        except: pass\n",
        "\n",
        "_os.environ[\"CODECARBON_CACHE_DIR\"]=f\"/content/.cc_cache_framew_{int(time.time())}\"\n",
        "\n",
        "def _cc_supported_kwargs():\n",
        "    base=dict(log_level=\"error\", output_dir=\".\", measure_power_secs=1, tracking_mode=\"process\")\n",
        "    try:\n",
        "        params=inspect.signature(EmissionsTracker.__init__).parameters\n",
        "        if \"allow_multiple_runs\" in params: base[\"allow_multiple_runs\"]=True\n",
        "        if \"cloud_provider\" in params:      base[\"cloud_provider\"]=\"google\"\n",
        "        if \"cloud_region\" in params:        base[\"cloud_region\"]=\"europe-west10\"\n",
        "        if \"country_iso_code\" in params:    base[\"country_iso_code\"]=\"DEU\"\n",
        "    except: pass\n",
        "    return base\n",
        "def make_trk(name,out):\n",
        "    _cleanup_cc_locks()\n",
        "    return EmissionsTracker(project_name=name, output_file=out, **_cc_supported_kwargs())\n",
        "def start(tr):\n",
        "    try: tr.start(); return True\n",
        "    except:\n",
        "        _cleanup_cc_locks()\n",
        "        try: tr.start(); return True\n",
        "        except: return False\n",
        "def stop(tr, st):\n",
        "    if not st: return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "    try: return tr.stop()\n",
        "    except: return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "def unpack(em):\n",
        "    if hasattr(em,\"energy_consumed\") and hasattr(em,\"emissions\"):\n",
        "        try: return float(em.energy_consumed), float(em.emissions)\n",
        "        except: return 0.0,0.0\n",
        "    if isinstance(em, dict):\n",
        "        e=em.get(\"energy_consumed\",0.0); c=em.get(\"emissions\", em.get(\"emissions_kg\",0.0))\n",
        "        try: return float(e), float(c)\n",
        "        except: return 0.0,0.0\n",
        "    try: return 0.0, float(em)\n",
        "    except: return 0.0,0.0\n",
        "def read_energy(path):\n",
        "    try:\n",
        "        if not _os.path.exists(path): return 0.0\n",
        "        import pandas as _pd\n",
        "        df=_pd.read_csv(path)\n",
        "        for c in [\"energy_consumed\",\"energy_consumed_kwh\",\"energy_consumed (kWh)\",\"energy (kWh)\"]:\n",
        "            if c in df.columns: return float(df[c].iloc[-1])\n",
        "        for c in df.columns:\n",
        "            if \"energy\" in c.lower() and \"kwh\" in c.lower(): return float(df[c].iloc[-1])\n",
        "    except: pass\n",
        "    return 0.0\n",
        "def measure(phase, fn, prefix):\n",
        "    logfile=_os.path.join(project_dir, f\"{prefix}_{phase}.csv\")\n",
        "    tr=make_trk(f\"{prefix}_{phase}\", logfile)\n",
        "    import time as _t\n",
        "    st=start(tr); t0=_t.time(); res=fn(); t1=_t.time()\n",
        "    em=stop(tr, st); ekwh,co2=unpack(em)\n",
        "    if ekwh==0.0:\n",
        "        ek=read_energy(logfile)\n",
        "        if ek: ekwh=ek\n",
        "    return {\"phase\":phase,\"time_s\":t1-t0,\"energy_kwh\":ekwh,\"co2_kg\":co2}, res"
      ],
      "metadata": {
        "id": "tpYU8VJ0ydXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Utilities ----------\n",
        "def safe_max_len(tok, model, fallback=2048, upper=100000):\n",
        "    cand=getattr(tok,\"model_max_length\",None)\n",
        "    if isinstance(cand,int) and 0<cand<upper: return min(cand, fallback)\n",
        "    cand=getattr(getattr(model,\"config\",None),\"max_position_embeddings\",None)\n",
        "    if isinstance(cand,int) and 0<cand<upper: return min(cand, fallback)\n",
        "    return fallback\n",
        "\n",
        "def autocast_ctx():\n",
        "    if device!=\"cuda\": return nullcontext()\n",
        "    return torch.autocast(device_type=\"cuda\", dtype=(torch.bfloat16 if USE_BF16 else torch.float16))\n",
        "\n",
        "def _pin_and_move(batch, device):\n",
        "    out={}\n",
        "    for k,v in batch.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            try:\n",
        "                if v.device.type==\"cpu\": v=v.pin_memory()\n",
        "            except: pass\n",
        "            out[k]=v.to(device, non_blocking=True)\n",
        "        else:\n",
        "            out[k]=v\n",
        "    return out\n",
        "\n",
        "def get_model_device(model):\n",
        "    try: return next(model.parameters()).device\n",
        "    except StopIteration: return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def capture_memory():\n",
        "    ram=psutil.Process().memory_info().rss\n",
        "    valloc=torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "    vres =torch.cuda.memory_reserved()  if torch.cuda.is_available() else 0\n",
        "    return ram, valloc, vres\n",
        "\n",
        "def bytes_to_gb(b): return float(b)/(1024**3)"
      ],
      "metadata": {
        "id": "VMo4j0wiydS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- HF-Backend: 4-bit (NF4) ----------\n",
        "def load_hf_4bit(model_id:str):\n",
        "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    if tok.pad_token_id is None: tok.pad_token = tok.eos_token\n",
        "    tok.padding_side=\"left\"\n",
        "\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    bnb4 = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=(torch.bfloat16 if USE_BF16 else torch.float16),\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id, device_map=\"auto\", quantization_config=bnb4, attn_implementation=ATTN_IMPL\n",
        "    ).eval()\n",
        "    return tok, model, \"hf_4bit_sdpa\""
      ],
      "metadata": {
        "id": "sEUJam4YydPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Eval: Generation / PPL / BLEU ----------\n",
        "def simple_generate(model, tok, prompts, max_new_tokens=32):\n",
        "    dev=get_model_device(model)\n",
        "    ml=min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    enc=tok(prompts, return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "    if dev.type==\"cuda\": enc=_pin_and_move(enc, dev)\n",
        "    room=ml-enc[\"input_ids\"].shape[1]; cur_new=max(1, min(max_new_tokens, int(room)))\n",
        "    with torch.inference_mode(), autocast_ctx():\n",
        "        out=model.generate(**enc, max_new_tokens=cur_new, generation_config=GENCFG, pad_token_id=tok.eos_token_id)\n",
        "\n",
        "    texts, total_gen_tokens=[], 0\n",
        "    max_in=int(enc[\"input_ids\"].shape[1]); pad_id=tok.pad_token_id; eos_id=tok.eos_token_id\n",
        "    for i in range(out.size(0)):\n",
        "        seq=out[i]; gen_slice=seq[max_in:]; gen_i=0\n",
        "        for t in gen_slice.tolist():\n",
        "            if t==eos_id: gen_i+=1; break\n",
        "            if (pad_id is not None) and (t==pad_id): break\n",
        "            gen_i+=1\n",
        "        total_gen_tokens+=gen_i\n",
        "        texts.append(tok.decode(seq, skip_special_tokens=True))\n",
        "    return texts, total_gen_tokens\n",
        "\n",
        "import torch.nn.functional as F\n",
        "def eval_perplexity(model, tok, ds_cfg):\n",
        "    dev=get_model_device(model)\n",
        "    ds=load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    ml=min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    losses=[]\n",
        "    with torch.inference_mode(), autocast_ctx():\n",
        "        for t in ds[\"text\"]:\n",
        "            if not isinstance(t,str) or len(t.strip())<4: continue\n",
        "            enc=tok(t, return_tensors=\"pt\", truncation=True, max_length=ml)\n",
        "            if dev.type==\"cuda\": enc=_pin_and_move(enc, dev)\n",
        "            out=model(enc[\"input_ids\"], labels=enc[\"input_ids\"]); loss=out.loss\n",
        "            losses.append(float(loss.detach().cpu()))\n",
        "    return math.exp(np.mean(losses)) if losses else None\n",
        "\n",
        "def eval_bleu_llm(model, tok, ds_cfg, max_new_tokens=32, batch_size=8):\n",
        "    dev=get_model_device(model)\n",
        "    ds=load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    ml=min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    preds, refs=[], []\n",
        "    with torch.inference_mode(), autocast_ctx():\n",
        "        batch_prompts, batch_refs=[], []\n",
        "        for ex in ds:\n",
        "            de,en=ex[\"translation\"][\"de\"], ex[\"translation\"][\"en\"]\n",
        "            prompt=f\"Translate to English:\\nGerman: {de}\\nEnglish:\"\n",
        "            batch_prompts.append(prompt); batch_refs.append(en)\n",
        "            if len(batch_prompts)>=batch_size:\n",
        "                enc=tok(batch_prompts, return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "                if dev.type==\"cuda\": enc=_pin_and_move(enc, dev)\n",
        "                room=ml-enc[\"input_ids\"].shape[1]; cur_new=max(1, min(max_new_tokens, int(room)))\n",
        "                out=model.generate(**enc, max_new_tokens=cur_new, generation_config=GENCFG, pad_token_id=tok.eos_token_id)\n",
        "                for i in range(out.size(0)):\n",
        "                    gen=tok.decode(out[i], skip_special_tokens=True)\n",
        "                    hyp=gen.split(\"English:\")[-1].strip().split(\"\\n\")[0].strip() or gen.strip()\n",
        "                    preds.append(hyp); refs.append([batch_refs[i]])\n",
        "                batch_prompts, batch_refs=[], []\n",
        "        if batch_prompts:\n",
        "            enc=tok(batch_prompts, return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "            if dev.type==\"cuda\": enc=_pin_and_move(enc, dev)\n",
        "            room=ml-enc[\"input_ids\"].shape[1]; cur_new=max(1, min(max_new_tokens, int(room)))\n",
        "            out=model.generate(**enc, max_new_tokens=cur_new, generation_config=GENCFG, pad_token_id=tok.eos_token_id)\n",
        "            for i in range(out.size(0)):\n",
        "                gen=tok.decode(out[i], skip_special_tokens=True)\n",
        "                hyp=gen.split(\"English:\")[-1].strip().split(\"\\n\")[0].strip() or gen.strip()\n",
        "                preds.append(hyp); refs.append([batch_prompts and batch_refs[i]])\n",
        "    return float(bleu_metric.compute(predictions=preds, references=refs)[\"score\"])"
      ],
      "metadata": {
        "id": "KTfSjAnbyoNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Runner (HF 4-bit) ----------\n",
        "def run_backend(model_id:str, alias:str):\n",
        "    print(f\"\\n### Starte Framework (HF 4-bit): {alias} ({model_id})\")\n",
        "\n",
        "    def _do_load():\n",
        "        tok, model, note = load_hf_4bit(model_id)\n",
        "        dev=get_model_device(model); ml=min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "        wenc=tok([\"Warmup token 1\",\"Warmup token 2\"], return_tensors=\"pt\",\n",
        "                 truncation=True, max_length=ml, padding=True)\n",
        "        if dev.type==\"cuda\": wenc=_pin_and_move(wenc, dev)\n",
        "        with torch.inference_mode(), autocast_ctx():\n",
        "            _=model.generate(**wenc, max_new_tokens=1, generation_config=GENCFG, pad_token_id=tok.eos_token_id)\n",
        "        return tok, model, note\n",
        "\n",
        "    m_warm, (tok, model, backend_note)=measure(\"warmup\", _do_load, f\"{RESULT_BASENAME}_{alias}\")\n",
        "\n",
        "    m_gen, (samples, n_tok)=measure(\"gen\",\n",
        "        lambda: simple_generate(model, tok, PROMPTS, EVAL[\"max_new_tokens\"]),\n",
        "        f\"{RESULT_BASENAME}_{alias}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    m_ppl, ppl=measure(\"ppl\",\n",
        "        lambda: eval_perplexity(model, tok, EVAL[\"ppl\"]),\n",
        "        f\"{RESULT_BASENAME}_{alias}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    m_bleu, bleu=measure(\"bleu\",\n",
        "        lambda: eval_bleu_llm(model, tok, EVAL[\"bleu\"], EVAL[\"max_new_tokens\"], batch_size=8),\n",
        "        f\"{RESULT_BASENAME}_{alias}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    total_time=m_warm[\"time_s\"]+m_gen[\"time_s\"]+m_ppl[\"time_s\"]+m_bleu[\"time_s\"]\n",
        "    total_energy=m_warm[\"energy_kwh\"]+m_gen[\"energy_kwh\"]+m_ppl[\"energy_kwh\"]+m_bleu[\"energy_kwh\"]\n",
        "    total_co2=m_warm[\"co2_kg\"]+m_gen[\"co2_kg\"]+m_ppl[\"co2_kg\"]+m_bleu[\"co2_kg\"]\n",
        "\n",
        "    ram,valloc,vres=capture_memory()\n",
        "    per_phase=[m_warm,m_gen,m_ppl,m_bleu]\n",
        "    for p in per_phase:\n",
        "        p[\"alias\"]=alias; p[\"model_id\"]=model_id; p[\"backend\"]=backend_note\n",
        "\n",
        "    row=dict(\n",
        "        model_id=model_id, alias=alias, backend=backend_note,\n",
        "        precision=\"int4_nf4_sdpa\",\n",
        "        time_s=total_time, energy_kwh=total_energy, co2_kg=total_co2,\n",
        "        kg_per_kwh=(total_co2/total_energy) if total_energy else None,\n",
        "        tokens_out=int(n_tok), ppl=ppl, bleu=bleu,\n",
        "        ram_GB=bytes_to_gb(ram), vram_alloc_GB=bytes_to_gb(valloc), vram_reserved_GB=bytes_to_gb(vres),\n",
        "        notes=f\"MAX_LEN_CAP={MAX_LEN_CAP}; bf16_autocast={USE_BF16}\"\n",
        "    )\n",
        "\n",
        "    out_samples=os.path.join(project_dir, f\"{RESULT_BASENAME}_samples_{alias}.txt\")\n",
        "    with open(out_samples,\"w\",encoding=\"utf-8\") as f:\n",
        "        for i,txt in enumerate(samples,1):\n",
        "            f.write(f\"--- Beispiel {i} ({alias}, {backend_note}) ---\\n{txt}\\n\\n\")\n",
        "    print(\"Beispiele gespeichert:\", out_samples)\n",
        "\n",
        "    return row, per_phase"
      ],
      "metadata": {
        "id": "HJRfUNJayoJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Ausführen & Speichern ----------\n",
        "row, phases = run_backend(MODEL_ID, ALIAS)\n",
        "\n",
        "df  = pd.DataFrame([row]).sort_values([\"backend\"]).reset_index(drop=True)\n",
        "dfp = pd.DataFrame(phases)\n",
        "dfp[\"wh_total\"]=dfp[\"energy_kwh\"]*1000.0\n",
        "dfp[\"phase\"]=pd.Categorical(dfp[\"phase\"], categories=[\"warmup\",\"gen\",\"ppl\",\"bleu\"], ordered=True)\n",
        "\n",
        "out_csv = os.path.join(project_dir, f\"{RESULT_BASENAME}_results.csv\")\n",
        "df.to_csv(out_csv, index=False)\n",
        "out_phase_csv = os.path.join(project_dir, f\"{RESULT_BASENAME}_per_phase.csv\")\n",
        "dfp.to_csv(out_phase_csv, index=False)\n",
        "\n",
        "print(\"\\nErgebnisse (gesamt):\")\n",
        "print(df); print(\"Gespeichert (gesamt):\", out_csv)\n",
        "\n",
        "print(\"\\nPer-Phase Übersicht:\")\n",
        "print(dfp[[\"backend\",\"phase\",\"time_s\",\"energy_kwh\",\"co2_kg\"]].sort_values([\"backend\",\"phase\"]))\n",
        "print(\"Gespeichert (per Phase):\", out_phase_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "52abe515cfc04115949c9acaf8eed9e2",
            "474ae6d33e1c4754833920faca2d9faf",
            "e22552095e454ad4bb79133fac91e49d",
            "eff8a254a2cb48c5b7f7112c60944c80",
            "610cb5f529e849d781e7e366f884bb6f",
            "2af977b5432e48a0b2e54eedd55dc63e",
            "58c05ee42f98400cbad1d1e8457e1bfc",
            "a85d002f2d1f4324b58c2923547fcfa1",
            "60ff9a88d7e047b88f42ceead33313d3",
            "534fd51030a241138b694f4120c0dcc4",
            "0481e7fdef1f444e9c78659c4e3d3056",
            "14721d0796674bad9d584e084cd6dafb",
            "0964addbe960416a95d4a45a41e31a0b",
            "48e6ddbe23b5405b89f3c9a46d95386d",
            "406f9cfe47c6457ca86f7d21a664ab33",
            "58798c97d42d44b283f1a0f86f684a89",
            "d98089c4d58744de99df7e9f2b24c179",
            "2e3b576d7efd487f986b7f31a2d879d0",
            "ece009ae6ee2497b8168f5b14313bbc7",
            "b3d1901f78d14e7ea824fecd047cfd7e",
            "ef32996ec0e643e8af9864d7017166d7",
            "9692def9e51f44ec8f58217ba929872d",
            "8a0a8c37b239417ab7596e366f8e6748",
            "65ceb185ea55438fa2392f01563f6ec1",
            "edc87e7f0dd8448ba97d9d209c1e9f63",
            "936abced0f284884a5110a35cad49ec2",
            "febf558539c14bdcbc8434960c421199",
            "0c02a90bc246403d98753bb956d0ccc2",
            "5126f982e5a44901882554cb708c2b5e",
            "342b430df1124eea84afdb61d977ccfc",
            "e509f19691df4bb79278d72b8989cf8f",
            "572795dbd1e94dc89d0228e59d1d22df",
            "1856d8ba16754ed89c37bb57a4c41296",
            "3c4ba53976934c5b8912c978f1335d61",
            "d40bf389bd654b4bb54f698a3784f0c1",
            "386bdfdb2c5b4ef4bb62cc3ac49eca41",
            "d8ad58303b444be0a5e2f49f705d5d67",
            "48814f1ec8e449a296004b541f908db9",
            "c73f52361b4b482fb496ccf4383ca393",
            "1ac5cd662f284c76a6a596b58fcf6d13",
            "5e1159c728d84112b114bf0057c82e86",
            "f8fba0c93159477492f78330f1d5a22f",
            "4e266a6f436c4eba94d52b330bf85d08",
            "5083046ac3024d8e91496f0b9322d416",
            "b4e15988937843a4ba4f0683da07ef0e",
            "bc3956301ddd4c138a4a5adf7a1574c7",
            "cf7ba44156024d4fab979fdc94b943f8",
            "899d02522ebc483ca05e4cce7e16a620",
            "dd2f8fe68290498191e4e220b941b5d2",
            "26db7530371d41cdbeab1080cbefdaaf",
            "36c66cb52274484d8874673866eb9da8",
            "7810f6428fd641198a1ee74c64146e79",
            "7e319a1f28634069a0b22988e2239d36",
            "e2225747da8545ecb3fa6792f49f5d4d",
            "3ab121ba2fe644c1bdc9d28fa8a4a750",
            "40289e1842d54abe93f266ef157d6fff",
            "dfe14c145a8a4f0e99ae4bc472b8d30d",
            "b1f60a7cd60947d2bc8b4b35d0df96b5",
            "bbc48b787bc640f7978ed344097c5cc7",
            "b2c9899316e74144bc35402b6c2707e7",
            "81ad632c88594fb59840c2aa14150066",
            "fa92a5add0994caf8b14d19d0531378e",
            "7b5b1298f6474d67a4d4960a42ec3f46",
            "13f55c68ef63470bbe02ada1a4bd0275",
            "ad3ccfeca9414b6e971bed554e05cab8",
            "e88dd4d796414632bc66661b16509ac0",
            "84ea133ff26c476ab9813b8f88d4656b",
            "8e57247748a04b6da69362782f953ce7",
            "6c0e61d587094aaf8e29ff6760c40411",
            "1f0a4247ca4145f1991f43bd9847ef2d",
            "de12fcec96b74ce1b215553fef88da91",
            "fcbbde4859fd47babd497e1576d1bd7d",
            "63525f36e2db48089e982ce02e31e4b8",
            "7bfc62b901784bb0b7600aa66541a5a4",
            "0ae5cab8a82c43c5a4013a12d2ecd0f4",
            "c1d5077dc4494bc1bddc0c5476eed645",
            "0fdbc6063d7e47458f3653d328d6755f",
            "1e3c0ab887a544f5a6c85796549f1c41",
            "77ba803634714792a46132deeaeb5c8b",
            "3fb34c11077646e4857e39df45135e68",
            "a49f6153bdf145958e46d88de3215e27",
            "36e503b8d90d4546b69f9c92fa10ee09",
            "d172a1621be74299ad97769bdae39961",
            "07411ce0ec2b42f6af61adc8c6b5aa1e",
            "c743a7edefaf43c3b314af638f749acf",
            "18fd933fc2fb46739fced2efd6dd5415",
            "d6a2ec8e15a340978fae84bf158ee886",
            "f589a69b51ba49a8984e36d01f108cd9",
            "a1cef2db1c0744609831a4f58ac4740b",
            "0d1f5be91056420abc560d75076f4aee",
            "224c31cd1e2a4b46b75f12ab04843cd8",
            "5419cc865f40491ab308d07095150a62",
            "fbf1872d94f14801953d2506747d222c",
            "5fc49739386e4f81ade7d10a09c2744d",
            "e6f4640e47c647e592e4d30990e41342",
            "7c68ff9d013246cebd1e2bf573ef3788",
            "771a8582700f4f189e1c232c3f46fdc3",
            "760902a4c46b4375a9628ce2f88d9a43",
            "119f370d14b64d189f17d7e1d22a43a2",
            "88f79bff42f94ae09616821ec9ec904e",
            "3b6c287c30a94f9b8863f532ee529b5d",
            "e746328eeda74629b083d7c20c6b0c61",
            "f8e444bdc17347789b092d0557234e9b",
            "bac7b63efb354ed7a7d61296b96c73f3",
            "ffc1f7f7de5d4ba28a6757a3e424ae04",
            "a34b5980f74641179f97ddf744524c0d",
            "69531617cf2f4f12abaa87d3d5d1c26f",
            "c920ea7692be412ab358f24007b75d36",
            "8cc9be678808427ba995057ed69b96c3",
            "e0a2f148f4bd45b5ab38a6048dab4013",
            "929a4d4419a7401c8a4b6cb69714545d",
            "ae0480f14cf0456487e14eb8a105dffc",
            "5e7d00149c554518abd28cdf8d7d36f0",
            "17befd7ce28f4d688ed12dde3b2ad041",
            "535e707321e84002bfceffcbf33e3f10",
            "09cf8c2f08da46268a0c1854ccb8bb10",
            "ad645b34bedb4cf7b4ee09a4e77fd421",
            "4eeca38d57b34eecb978e1df20b0c9d4",
            "78491c46e4aa43b7af8fd50422c66c9e",
            "a4c975e6ca734aff8b5c8a0ea9f0a7bb",
            "946020db75dd422d81d99f8104cfa041",
            "64c40d8790ab43b9b4f8cbee1502be64",
            "a10d92c963094a5c9abeae971d3ae06e",
            "99b242823ff3438d9bf06df517c4ebc4",
            "9c123a2512274ac691b55cebd90c0d34",
            "adc0d27cd05a4d2799a571b2a154002a",
            "3b5ff4ee476d4c9abe979e91d055c75f",
            "44b69e6b15a04bc19ab5b2148a1e3f4b",
            "0fbfca11719847f69b7e497962756955",
            "674f5a7b0fb7496e8b4d4053d026f256",
            "4915727b8c294c988b62c11d031c83de",
            "111f0c4a30454b5b9bc5d8751b1d645c",
            "89b62fd45aeb4af685faf6e7c51e2d80",
            "806c3c05328d48c89aca345e690cd43d",
            "36c7803d34164bf5ba6383eec22c54ee",
            "b36ae83481724f64bb85abc70a820bbb",
            "6e2da2d1fc20424981a214ceab9d4d44",
            "991768137062475895105914e3aeb4cc",
            "2eae8cd246cd46c7bfeb4edc27e406e2",
            "09c42a6147a744859c1367df5e2485a5",
            "a73d72bef11249d39c5337947a6ea49d",
            "e8281de540c841549b04a76bc3d2ff9b",
            "c8556192053d46a4874afb39ae610dca",
            "387be2f2ef7345578e6ac88ec4856134",
            "10056ae4256d44828f0895a034a32074",
            "ba026701366d48aebdc72a664fd1d74e",
            "51e63cb794284248809a2c710db7afef",
            "2c4298f7f0f14fddbe7c1434c7e2d007",
            "7a289c0838d7491a811efc2f95d044a8",
            "21bf645c379d45fd97e955cc4eb64112",
            "0e605fb4aadd45b0ae37b4ee8e242966",
            "472afd8359544a73a293e980131770fe",
            "2f72986b4cbe4d04863d9808eacd8057",
            "53ad27f89d854b74a70d6626459b4c2f",
            "ae35698bfbed4937bb431931a6a52dfc",
            "be1b7075b25f4cf8ade53c5de52e9372",
            "c1941859271a4339923b3a580864b09b",
            "f267d2280ac045cda1b84ad0de688795",
            "6d1afd2e9c1241e2a864eb43d0037ae1",
            "a556db37bff441dead45b7b81e3b3d93",
            "8a08df4cea884601b3398aee7b4d2f52",
            "f814eafaa66845eb877fe9f7ea12e221",
            "8d4a2019fd714c819e785d5a2db439ad",
            "40a4da1887404ec1a68a99c89eb789b6",
            "8e384b14df8740f6a1c632ded2a6e223",
            "36058fc8b4de43718fe354cfbfcc5e63",
            "347dae14b7fd4f10b2521f9b818dace3",
            "a0485c4c06ca4a2abbe010a5ca16927e",
            "e5e9534ec8b94a63964e8e121115ba71",
            "f779638f7e9b45ada5468e0b05338cf0",
            "aa2004848c184dd1b3f3ee15d6756957",
            "da29328f02684aeab6fa7c41af85a7a7",
            "3b8a8a172bbf43168eae0aaa45b30211",
            "715522df89e3420fa02c5e1d1149cd5d",
            "394869c77ea34d3d8814094f836badf8",
            "faaa1281703e425391757e291293b514",
            "486311fe9fe04251bf44b1744eb8b895",
            "1c57db7e70fe43cd9332b401c1f07328",
            "af482dc755e0449889110c76cc7dea60",
            "3a83a60150954428948a240a0d35aee8",
            "359ee02103654628a5f3bda1a42f300e",
            "58b4a79f1d0f48d7abcf658cd9c23aff",
            "25c623b341f34ae1abc81de08927304e",
            "d7ee08c39e5d4abaae1bb64ad0f731b9",
            "86f938d74f03455fb06665c1e1d6d9bb",
            "b48b30e0438d4fffb7d0c0233107eaf0",
            "c4f24ebcd1b545e28b14501ad0e9e0a7",
            "50bc33737ebd48bb9ee9831e71498da4",
            "0c91535c30e142ff8647a6e23170cbde",
            "228ecefddb63422ca8ece90efbd3242f",
            "7bb203e6a8a34f83a39fb0e83237e6f1",
            "b183d079619844b6945ac74a1a4b4867",
            "2db2731c21634366aec3bf17a29dd0bb",
            "ada6bed6ed144afeb2b6b120dde72015",
            "8bf7a8e3d3164bc8b49140096e54ccb5",
            "323b8b47c02b446c9f1b880cdbf3e865",
            "a07a9e73a3254dab81702e31eed2a129",
            "d1d9a2ad57a0417d82c5f35d67c7c19c",
            "a6ac6a88785d46669bd3b8c48ee5ee75",
            "2312e2c0d4c84a67b33ca0eb35e5afcc",
            "eac9ac631cd34f71981ebff64b169125",
            "57bf54c2b1914345bedd6e3cb75451c4",
            "a8bb485787f94dbfbac5da449f5739ca",
            "e44fc5b0b94d494e9964ba2b31e3b730",
            "ea65e947a1de483e860e02b963aa940d",
            "731e4afd1ccd4c11bb3887d9b7e20040",
            "2c344465edbe4d7c821908903fa1e34d",
            "d9baa836ad644d7486c121840074702c",
            "57db5a57c4fb4bfea77c3e1ccaf63041",
            "6a236b6047684afba74fc4ed778bb1d4",
            "b60d600c1f5245daa9b026e91fffe5ba",
            "7bdb84a83a6740d8a33e690e5f7b8238",
            "cc6251d7727848fca0eb3058c81e7bb7",
            "bb32e3e4a30c4089ba4e2708a631c527",
            "3d0f1569eb1d4426bf3a6595e45fce3f",
            "52c05ec2951240038577c32094a89376",
            "016e6f954ac74520a163c7cf40aa4202",
            "94cd25b1774c42cd9953545d816f5750",
            "8354152cc1eb48afbc45fb813765ff7f",
            "10c7f971cf134121a2dacc03d9da4e21",
            "59580eb72f4e4046803b2a990f98dbb5",
            "b9b82ab0a3f442efa1e2d95588e30786",
            "e1b4069f252343059d666a0eb275bba2",
            "dd6b48390d7c4168a2da784f41c8ee03",
            "c0b27652efd54dd6981af97d85ff9697",
            "0f46fc2ac9794c669988df82a7a5fd18",
            "337dbed8ca964af38f74960303fb7693",
            "83e85125f82a455b83006f7275e9e99a",
            "a35afe0313e74736a7fb3842d4e568a6",
            "2bd8d9a3518f45a6b435690f97a193d0",
            "e28e55115a66497cb6596f5e6b9828d3"
          ]
        },
        "id": "rwzDfl5FyoF_",
        "outputId": "f5445e41-e679-4543-9da7-e6ea571cdd8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 11:21:27] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Starte Framework (HF 4-bit): r1q15b (deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52abe515cfc04115949c9acaf8eed9e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14721d0796674bad9d584e084cd6dafb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a0a8c37b239417ab7596e366f8e6748"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c4ba53976934c5b8912c978f1335d61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4e15988937843a4ba4f0683da07ef0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40289e1842d54abe93f266ef157d6fff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84ea133ff26c476ab9813b8f88d4656b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e3c0ab887a544f5a6c85796549f1c41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1cef2db1c0744609831a4f58ac4740b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88f79bff42f94ae09616821ec9ec904e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "929a4d4419a7401c8a4b6cb69714545d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64c40d8790ab43b9b4f8cbee1502be64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89b62fd45aeb4af685faf6e7c51e2d80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00000-of-00003.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "387be2f2ef7345578e6ac88ec4856134"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00001-of-00003.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae35698bfbed4937bb431931a6a52dfc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00002-of-00003.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36058fc8b4de43718fe354cfbfcc5e63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/validation-00000-of-00001.parquet:   0%|          | 0.00/474k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "486311fe9fe04251bf44b1744eb8b895"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/test-00000-of-00001.parquet:   0%|          | 0.00/509k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50bc33737ebd48bb9ee9831e71498da4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6ac6a88785d46669bd3b8c48ee5ee75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a236b6047684afba74fc4ed778bb1d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59580eb72f4e4046803b2a990f98dbb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beispiele gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/deepseek_frameworks/deepseek_framew_4bit_samples_r1q15b.txt\n",
            "\n",
            "Ergebnisse (gesamt):\n",
            "                                    model_id   alias       backend  \\\n",
            "0  deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  r1q15b  hf_4bit_sdpa   \n",
            "\n",
            "       precision     time_s  energy_kwh    co2_kg  kg_per_kwh  tokens_out  \\\n",
            "0  int4_nf4_sdpa  67.500892    0.001906  0.000863    0.452621          96   \n",
            "\n",
            "          ppl      bleu    ram_GB  vram_alloc_GB  vram_reserved_GB  \\\n",
            "0  238.563029  11.30959  4.244473       1.517489          1.583984   \n",
            "\n",
            "                                 notes  \n",
            "0  MAX_LEN_CAP=128; bf16_autocast=True  \n",
            "Gespeichert (gesamt): /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/deepseek_frameworks/deepseek_framew_4bit_results.csv\n",
            "\n",
            "Per-Phase Übersicht:\n",
            "        backend   phase     time_s  energy_kwh    co2_kg\n",
            "0  hf_4bit_sdpa  warmup  30.314054    0.000776  0.000351\n",
            "1  hf_4bit_sdpa     gen   3.138607    0.000102  0.000046\n",
            "2  hf_4bit_sdpa     ppl   5.025981    0.000149  0.000068\n",
            "3  hf_4bit_sdpa    bleu  29.022249    0.000879  0.000398\n",
            "Gespeichert (per Phase): /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/deepseek_frameworks/deepseek_framew_4bit_per_phase.csv\n"
          ]
        }
      ]
    }
  ]
}