{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ino54/MA_GreenAI-Practical-Experiments/blob/main/deepseek_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtHXCTvdFjbx",
        "outputId": "9c72f0f3-9e7d-4b5d-a8cf-8864cc5ebc08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "# --- Requirements ---\n",
        "%%writefile requirements.txt\n",
        "transformers>=4.44\n",
        "accelerate>=0.33\n",
        "bitsandbytes\n",
        "datasets>=2.20\n",
        "evaluate>=0.4\n",
        "sacrebleu>=2.4\n",
        "codecarbon>=2.5,<3\n",
        "psutil\n",
        "pynvml>=12,<13\n",
        "numpy==2.0.2\n",
        "pandas==2.2.2\n",
        "huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U -r requirements.txt --no-warn-conflicts\n",
        "!pip uninstall -y -q google-genai firebase-admin || true\n",
        "!pip show fief-client || echo \"fief-client nicht installiert\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chQTnAxoFwZr",
        "outputId": "5a4358a6-5411-4e9a-e5f6-4b769bb6954f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.6/517.6 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.2/291.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hName: fief-client\n",
            "Version: 0.20.0\n",
            "Summary: Fief Client for Python\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: François Voron <contact@fief.dev>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: httpx, jwcrypto\n",
            "Required-by: codecarbon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Google Drive mounten ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Projektordner setzen ---\n",
        "import os, pathlib, re, platform\n",
        "project_path = \"/content/drive/MyDrive/LLM-Effizienz/4_2_Baseline\"\n",
        "pathlib.Path(project_path).mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(project_path)\n",
        "print(\"Arbeitsordner:\", os.getcwd())\n",
        "\n",
        "# --- Hugging Face Login via Colab-Secret ---\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(hf_token)\n",
        "    print(\"Hugging Face Login erfolgreich!\")\n",
        "else:\n",
        "    print(\"WARNUNG: Kein HF_TOKEN.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmcUuyyKFwWW",
        "outputId": "e719c8a6-9c5a-41e2-c536-0d8b1888dea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Arbeitsordner: /content/drive/MyDrive/LLM-Effizienz/4_2_Baseline\n",
            "Hugging Face Login erfolgreich!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports & Setup ---\n",
        "import warnings; warnings.filterwarnings(\"ignore\")\n",
        "import time, math, gc, inspect\n",
        "from dataclasses import dataclass, asdict\n",
        "from contextlib import nullcontext\n",
        "from typing import Optional\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import psutil, numpy as np, pandas as pd, torch\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
        "                          set_seed, GenerationConfig)\n",
        "from codecarbon import EmissionsTracker, __version__ as cc_ver\n",
        "\n",
        "print(\"CodeCarbon-Version:\", cc_ver)\n",
        "\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_total_gb = torch.cuda.get_device_properties(0).total_memory/(1024**3)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.set_grad_enabled(False)\n",
        "else:\n",
        "    gpu_name = \"CPU\"; vram_total_gb = 0.0\n",
        "\n",
        "print(f\"Device: {device} | GPU: {gpu_name} | VRAM: {vram_total_gb:.1f} GB | Torch {torch.__version__} | Python {platform.python_version()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p56X6Fp8FwSt",
        "outputId": "af72098f-8e78-4e72-bf13-e69fc0256384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CodeCarbon-Version: 2.8.4\n",
            "Device: cuda | GPU: NVIDIA A100-SXM4-40GB | VRAM: 39.6 GB | Torch 2.8.0+cu126 | Python 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Standort-Konfiguration (Berlin) + CodeCarbon-Helper ==========\n",
        "USE_GCP_REGION = True\n",
        "COUNTRY_ISO_CODE = \"DEU\"\n",
        "CLOUD_PROVIDER   = \"google\"\n",
        "CLOUD_REGION     = \"europe-west10\"\n",
        "\n",
        "import os as _os, time as _time\n",
        "\n",
        "def tracker_kwargs_base():\n",
        "    base = dict(log_level=\"error\", output_dir=\".\")\n",
        "    try:\n",
        "        sig = inspect.signature(EmissionsTracker.__init__)\n",
        "        if \"measure_power_secs\" in sig.parameters: base[\"measure_power_secs\"] = 1\n",
        "        if \"tracking_mode\" in sig.parameters:      base[\"tracking_mode\"] = \"process\"\n",
        "        if USE_GCP_REGION:\n",
        "            if \"cloud_provider\" in sig.parameters: base[\"cloud_provider\"] = CLOUD_PROVIDER\n",
        "            if \"cloud_region\"   in sig.parameters: base[\"cloud_region\"]   = CLOUD_REGION\n",
        "            if \"country_iso_code\" in sig.parameters: base[\"country_iso_code\"] = COUNTRY_ISO_CODE\n",
        "        else:\n",
        "            if \"country_iso_code\" in sig.parameters: base[\"country_iso_code\"] = COUNTRY_ISO_CODE\n",
        "            if \"cloud_provider\" in sig.parameters:   base[\"cloud_provider\"] = None\n",
        "            if \"cloud_region\"   in sig.parameters:   base[\"cloud_region\"]   = None\n",
        "    except Exception:\n",
        "        pass\n",
        "    return base\n",
        "\n",
        "def make_tracker_named(project_name: str, output_file: str):\n",
        "    cache_dir = f\"/content/.codecarbon_cache_{project_name}_{int(_time.time())}\"\n",
        "    _os.environ[\"CODECARBON_CACHE_DIR\"] = cache_dir\n",
        "    for d in (_os.path.expanduser(\"~/.codecarbon\"), \"/content/.codecarbon\"):\n",
        "        lock_file = _os.path.join(d, \"codecarbon.lock\")\n",
        "        if os.path.exists(lock_file):\n",
        "            try: os.remove(lock_file)\n",
        "            except: pass\n",
        "    return EmissionsTracker(project_name=project_name, output_file=output_file, **tracker_kwargs_base())\n",
        "\n",
        "def safe_start(tracker):\n",
        "    try:\n",
        "        tracker.start(); return True\n",
        "    except Exception as e:\n",
        "        print(f\"[CodeCarbon] Start fehlgeschlagen: {e} → Fallback 0/0.\"); return False\n",
        "\n",
        "def safe_stop(tracker, started: bool):\n",
        "    if not started: return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "    try:\n",
        "        return tracker.stop()\n",
        "    except Exception as e:\n",
        "        print(f\"[CodeCarbon] Stop fehlgeschlagen: {e} → Fallback 0/0.\")\n",
        "        return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "\n",
        "def unpack_emissions(em):\n",
        "    if em is None: return 0.0, 0.0\n",
        "    if hasattr(em,\"energy_consumed\") and hasattr(em,\"emissions\"):\n",
        "        try: return float(em.energy_consumed), float(em.emissions)\n",
        "        except: pass\n",
        "    if isinstance(em, dict):\n",
        "        e = em.get(\"energy_consumed\", 0.0); c = em.get(\"emissions\", em.get(\"emissions_kg\", 0.0))\n",
        "        try: return float(e), float(c)\n",
        "        except: return 0.0, 0.0\n",
        "    try: return 0.0, float(em)\n",
        "    except: return 0.0, 0.0\n",
        "\n",
        "def read_energy_from_log(path: str) -> float:\n",
        "    try:\n",
        "        if not os.path.exists(path): return 0.0\n",
        "        df = pd.read_csv(path)\n",
        "        for c in [\"energy_consumed\",\"energy_consumed_kwh\",\"energy_consumed (kWh)\",\"energy (kWh)\"]:\n",
        "            if c in df.columns: return float(df[c].iloc[-1])\n",
        "        for c in df.columns:\n",
        "            n = c.lower()\n",
        "            if \"energy\" in n and \"kwh\" in n: return float(df[c].iloc[-1])\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def measure_phase(phase_name: str, fn, log_prefix: str):\n",
        "    logfile = f\"{log_prefix}_{phase_name}.csv\"\n",
        "    tracker = make_tracker_named(project_name=f\"{log_prefix}_{phase_name}\", output_file=logfile)\n",
        "    started = safe_start(tracker)\n",
        "    t0 = time.time(); result = fn(); t1 = time.time()\n",
        "    em_raw = safe_stop(tracker, started)\n",
        "    energy_kwh, co2_kg = unpack_emissions(em_raw)\n",
        "    if not energy_kwh or energy_kwh == 0.0:\n",
        "        energy_from_csv = read_energy_from_log(logfile)\n",
        "        if energy_from_csv: energy_kwh = energy_from_csv\n",
        "    return {\"phase\": phase_name, \"time_s\": t1 - t0, \"energy_kwh\": energy_kwh, \"co2_kg\": co2_kg}, result"
      ],
      "metadata": {
        "id": "woXD_HteGDdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluation-Config ---\n",
        "MODEL_ID   = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "ALIAS_LONG = \"DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "ALIAS      = \"r1q15b\"\n",
        "\n",
        "EVAL = {\n",
        "    \"max_new_tokens\": 32,\n",
        "    \"ppl_dataset\":  {\"name\":\"wikitext\",\"config\":\"wikitext-2-raw-v1\",\"split\":\"test[:1%]\"},\n",
        "    \"bleu_dataset\": {\"name\":\"wmt14\",\"config\":\"de-en\",\"split\":\"test[:32]\"},\n",
        "}\n",
        "import re\n",
        "def parse_subset_count(split_str: str, default=32):\n",
        "    m = re.search(r\":\\s*(\\d+)\\s*\\]$\", split_str or \"\"); return int(m.group(1)) if m else default\n",
        "BLEU_N = parse_subset_count(EVAL[\"bleu_dataset\"][\"split\"], default=32)\n",
        "\n",
        "PROMPTS = [\n",
        "    \"Schreibe einen kurzen Absatz über nachhaltige KI.\",\n",
        "    \"Erkläre in einfachen Worten, was Quantisierung in neuronalen Netzen ist.\",\n",
        "    \"Nenne drei Vorteile von Mixture-of-Experts-Modellen.\"\n",
        "]\n",
        "\n",
        "def autocast_ctx():\n",
        "    return torch.autocast(device_type=\"cuda\", dtype=torch.float16) if device==\"cuda\" else nullcontext()\n",
        "\n",
        "def capture_memory():\n",
        "    ram = psutil.Process().memory_info().rss\n",
        "    valloc = torch.cuda.memory_allocated() if device==\"cuda\" else 0\n",
        "    vres  = torch.cuda.memory_reserved()  if device==\"cuda\" else 0\n",
        "    return ram, valloc, vres\n",
        "\n",
        "def bytes_to_gb(b): return float(b)/(1024**3)\n",
        "bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "@dataclass\n",
        "class BaselineResult:\n",
        "    model_id: str\n",
        "    alias: str\n",
        "    precision: str\n",
        "    time_s: float\n",
        "    energy_kwh: float\n",
        "    co2_kg: float\n",
        "    tokens_out: int\n",
        "    ram_GB: float\n",
        "    vram_alloc_GB: float\n",
        "    vram_reserved_GB: float\n",
        "    ppl: Optional[float] = None\n",
        "    bleu: Optional[float] = None\n",
        "    notes: str = \"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "24d669d1dcc8460a8b819853bf9193fc",
            "ef03b9fb8eda4b52a0dbdd6ba748cd23",
            "68e8082a25a64c2bbcb9fca896215a28",
            "edc36ea9d4a44171b9f7d46106fbb5f0",
            "340fce8fef6240829d952d0df0346ea4",
            "aec076ad93254042b84bcb2fa9944135",
            "300f74e04bcc48edba134b174e3193e0",
            "3a0776ba64f34a26b109f1a9aa934db9",
            "c2734fe56d4742d8900ae0bf2c2b0a30",
            "816796bdefd6490a88ec8626625d3791",
            "2b1ea1ef1ad44d9dab6e922ba627221c"
          ]
        },
        "id": "zK9kv90pGDY7",
        "outputId": "0bdef8d0-dfed-438c-a39a-2e71d13195a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24d669d1dcc8460a8b819853bf9193fc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Utility: safe_max_len + stabile GenerationConfig (keine temperature/top_p-Warnung) ---\n",
        "def safe_max_len(tok, model, fallback=2048, upper=100000):\n",
        "    cand = getattr(tok, \"model_max_length\", None)\n",
        "    if isinstance(cand, int) and 0 < cand < upper: return cand\n",
        "    cand = getattr(getattr(model, \"config\", None), \"max_position_embeddings\", None)\n",
        "    if isinstance(cand, int) and 0 < cand < upper: return cand\n",
        "    return fallback\n",
        "\n",
        "GC_GREEDY = GenerationConfig(\n",
        "    do_sample=False,\n",
        "    temperature=None,\n",
        "    top_p=None,\n",
        "    top_k=None,\n",
        "    num_beams=1,\n",
        ")"
      ],
      "metadata": {
        "id": "FUmJrk8vGDT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Modell-Lader (erst FP16, bei OOM dann INT8) ---\n",
        "def load_model(model_id: str):\n",
        "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    if tok.pad_token_id is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    tok.padding_side = \"left\"\n",
        "\n",
        "    try:\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache(); gc.collect()\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id, device_map=\"auto\",\n",
        "            torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n",
        "            attn_implementation=\"sdpa\",\n",
        "        )\n",
        "        prec = \"fp16\" if device==\"cuda\" else \"fp32\"\n",
        "        return tok, model, prec\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" not in str(e).lower(): raise\n",
        "        print(f\"[Info] OOM bei FP16 für {model_id}. Fallback auf INT8…\")\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "    bnb = BitsAndBytesConfig(load_in_8bit=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id, device_map=\"auto\", quantization_config=bnb,\n",
        "        torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n",
        "        attn_implementation=\"sdpa\",\n",
        "    )\n",
        "    return tok, model, \"int8\"\n",
        "\n",
        "def warmup(model, tok, max_len):\n",
        "    with torch.no_grad(), autocast_ctx():\n",
        "        dummy = tok(\"Hello\", return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
        "        _ = model.generate(**dummy, max_new_tokens=1, generation_config=GC_GREEDY, pad_token_id=tok.eos_token_id)"
      ],
      "metadata": {
        "id": "YmpQpNdyGdGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Generation / PPL / BLEU (analog zu BLOOM, aber mit GC_GREEDY) ---\n",
        "def simple_generate(model, tok, prompts, max_new_tokens=32):\n",
        "    model.eval(); total_gen_tokens, texts = 0, []\n",
        "    max_len = safe_max_len(tok, model)\n",
        "    for p in prompts:\n",
        "        enc = tok(p, return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
        "        room = max_len - enc[\"input_ids\"].shape[1]\n",
        "        cur_new = max(1, min(max_new_tokens, int(room)))\n",
        "        with torch.no_grad(), autocast_ctx():\n",
        "            out = model.generate(\n",
        "                **enc,\n",
        "                max_new_tokens=cur_new,\n",
        "                generation_config=GC_GREEDY,\n",
        "                pad_token_id=tok.eos_token_id\n",
        "            )\n",
        "        gen_len = out.shape[1] - enc[\"input_ids\"].shape[1]\n",
        "        total_gen_tokens += int(gen_len)\n",
        "        texts.append(tok.decode(out[0], skip_special_tokens=True))\n",
        "    return texts, total_gen_tokens\n",
        "\n",
        "def eval_perplexity(model, tok, ds_cfg):\n",
        "    ds = load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    max_len = safe_max_len(tok, model); losses = []\n",
        "    with torch.no_grad():\n",
        "        for t in ds[\"text\"]:\n",
        "            if not isinstance(t, str) or len(t.strip()) < 4: continue\n",
        "            enc = tok(t, return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
        "            with autocast_ctx(): out = model(enc[\"input_ids\"], labels=enc[\"input_ids\"])\n",
        "            losses.append(float(out.loss.detach().cpu()))\n",
        "    return math.exp(np.mean(losses)) if losses else None\n",
        "\n",
        "def eval_bleu_llm(model, tok, ds_cfg, max_new_tokens=32):\n",
        "    ds = load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    max_len = safe_max_len(tok, model); preds, refs = [], []\n",
        "    with torch.no_grad():\n",
        "        for ex in ds:\n",
        "            de, en = ex[\"translation\"][\"de\"], ex[\"translation\"][\"en\"]\n",
        "            prompt = f\"Translate to English:\\nGerman: {de}\\nEnglish:\"\n",
        "            inputs = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
        "            room = max_len - inputs[\"input_ids\"].shape[1]\n",
        "            cur_new = max(1, min(max_new_tokens, int(room)))\n",
        "            with autocast_ctx():\n",
        "                out = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=cur_new,\n",
        "                    generation_config=GC_GREEDY,\n",
        "                    pad_token_id=tok.eos_token_id\n",
        "                )\n",
        "            gen = tok.decode(out[0], skip_special_tokens=True)\n",
        "            seg = gen.split(\"English:\")[-1].strip()\n",
        "            hyp = seg.split(\"\\n\")[0].strip() or gen.strip()\n",
        "            preds.append(hyp); refs.append([en])\n",
        "    return float(bleu_metric.compute(predictions=preds, references=refs)[\"score\"])"
      ],
      "metadata": {
        "id": "OGbsb6_FGdB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Baseline-Lauf (pro Phase) ---\n",
        "def run_baseline(model_id: str, alias_long: str, alias_short: str):\n",
        "    tok, model, prec = load_model(model_id)\n",
        "    max_len = safe_max_len(tok, model)\n",
        "    warmup(model, tok, max_len)  # Warm-up außerhalb der Messung\n",
        "    log_prefix = f\"deepseek_{alias_short}\"\n",
        "\n",
        "    def _do_gen(): return simple_generate(model, tok, PROMPTS, EVAL[\"max_new_tokens\"])\n",
        "    gen_metrics, (examples, tokens_out) = measure_phase(\"gen\", _do_gen, log_prefix)\n",
        "\n",
        "    def _do_ppl(): return eval_perplexity(model, tok, EVAL[\"ppl_dataset\"])\n",
        "    ppl_metrics, ppl = measure_phase(\"ppl\", _do_ppl, log_prefix)\n",
        "\n",
        "    if device == \"cuda\": torch.cuda.empty_cache()\n",
        "    def _do_bleu(): return eval_bleu_llm(model, tok, EVAL[\"bleu_dataset\"], EVAL[\"max_new_tokens\"])\n",
        "    bleu_metrics, bleu = measure_phase(\"bleu\", _do_bleu, log_prefix)\n",
        "\n",
        "    total_time   = gen_metrics[\"time_s\"] + ppl_metrics[\"time_s\"] + bleu_metrics[\"time_s\"]\n",
        "    total_energy = gen_metrics[\"energy_kwh\"] + ppl_metrics[\"energy_kwh\"] + bleu_metrics[\"energy_kwh\"]\n",
        "    total_co2    = gen_metrics[\"co2_kg\"] + ppl_metrics[\"co2_kg\"] + bleu_metrics[\"co2_kg\"]\n",
        "\n",
        "    ram = psutil.Process().memory_info().rss\n",
        "    valloc = torch.cuda.memory_allocated() if device==\"cuda\" else 0\n",
        "    vres  = torch.cuda.memory_reserved()  if device==\"cuda\" else 0\n",
        "\n",
        "    per_phase_df = pd.DataFrame([gen_metrics, ppl_metrics, bleu_metrics])\n",
        "    per_phase_df[\"alias\"]     = alias_short\n",
        "    per_phase_df[\"model_id\"]  = model_id\n",
        "    per_phase_df[\"precision\"] = prec\n",
        "    per_phase_df[\"tokens_out\"]= [tokens_out, None, None]\n",
        "    per_phase_df[\"ppl\"]       = [None, ppl, None]\n",
        "    per_phase_df[\"bleu\"]      = [None, None, bleu]\n",
        "\n",
        "    # Abgeleitete Kennzahlen\n",
        "    per_phase_df[\"wh_total\"] = per_phase_df[\"energy_kwh\"] * 1000.0\n",
        "    per_phase_df[\"tokens_s\"] = None\n",
        "    per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"tokens_s\"] = (\n",
        "        per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"tokens_out\"]\n",
        "        / per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"time_s\"]\n",
        "    )\n",
        "    per_phase_df[\"wh_per_token\"] = None\n",
        "    per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"wh_per_token\"] = (\n",
        "        per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"wh_total\"]\n",
        "        / per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"tokens_out\"]\n",
        "    )\n",
        "    per_phase_df[\"s_per_example\"] = None\n",
        "    per_phase_df.loc[per_phase_df[\"phase\"]==\"bleu\", \"s_per_example\"] = (\n",
        "        per_phase_df.loc[per_phase_df[\"phase\"]==\"bleu\", \"time_s\"] / float(BLEU_N)\n",
        "    )\n",
        "    per_phase_df[\"wh_per_example\"] = None\n",
        "    per_phase_df.loc[per_phase_df[\"phase\"]==\"bleu\", \"wh_per_example\"] = (\n",
        "        per_phase_df.loc[per_phase_df[\"phase\"]==\"bleu\", \"wh_total\"] / float(BLEU_N)\n",
        "    )\n",
        "    per_phase_df[\"kg_per_kwh\"] = (per_phase_df[\"co2_kg\"] / per_phase_df[\"energy_kwh\"]).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    print(f\"\\nPer-Phase ({alias_short}) — Standort:\",\n",
        "          f\"GCP {CLOUD_REGION}\" if USE_GCP_REGION else f\"Ländermix {COUNTRY_ISO_CODE}\")\n",
        "    print(per_phase_df[[\n",
        "        \"phase\",\"time_s\",\"energy_kwh\",\"co2_kg\",\"kg_per_kwh\",\n",
        "        \"tokens_out\",\"ppl\",\"bleu\",\"tokens_s\",\"wh_per_token\",\"s_per_example\",\"wh_per_example\"\n",
        "    ]])\n",
        "\n",
        "    res = BaselineResult(\n",
        "        model_id=model_id, alias=alias_short, precision=prec,\n",
        "        time_s=total_time, energy_kwh=total_energy, co2_kg=total_co2,\n",
        "        tokens_out=int(tokens_out),\n",
        "        ram_GB=bytes_to_gb(ram), vram_alloc_GB=bytes_to_gb(valloc), vram_reserved_GB=bytes_to_gb(vres),\n",
        "        ppl=ppl, bleu=bleu, notes=f\"GPU={gpu_name}, VRAM={vram_total_gb:.1f} GB\"\n",
        "    )\n",
        "    return res, examples, per_phase_df"
      ],
      "metadata": {
        "id": "NbkzXLpFGtRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Ausführen & Speichern ---\n",
        "print(f\"\\n### Starte Baseline (per Phase): {ALIAS_LONG}\")\n",
        "res, examples, phase_df = run_baseline(MODEL_ID, ALIAS_LONG, ALIAS)\n",
        "\n",
        "df = pd.DataFrame([asdict(res)])\n",
        "df[\"kg_per_kwh\"] = (df[\"co2_kg\"] / df[\"energy_kwh\"]).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "print(\"\\nGesamt (summiert über Phasen):\")\n",
        "print(df[[\"model_id\",\"alias\",\"precision\",\"time_s\",\"energy_kwh\",\"co2_kg\",\"kg_per_kwh\",\n",
        "          \"tokens_out\",\"ppl\",\"bleu\",\"ram_GB\",\"vram_alloc_GB\",\"vram_reserved_GB\",\"notes\"]])\n",
        "\n",
        "out_dir = project_path\n",
        "df.to_csv(os.path.join(out_dir, \"baseline_deepseek_results.csv\"), index=False)\n",
        "phase_df.to_csv(os.path.join(out_dir, \"baseline_deepseek_per_phase.csv\"), index=False)\n",
        "print(\"Gespeichert (gesamt):\", os.path.join(out_dir, \"baseline_deepseek_results.csv\"))\n",
        "print(\"Gespeichert (per Phase):\", os.path.join(out_dir, \"baseline_deepseek_per_phase.csv\"))\n",
        "\n",
        "samples_path = os.path.join(out_dir, f\"baseline_samples_{ALIAS}.txt\")\n",
        "with open(samples_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, txt in enumerate(examples, 1):\n",
        "        f.write(f\"--- Beispiel {i} ({ALIAS}) ---\\n{txt}\\n\\n\")\n",
        "print(\"Beispiele gespeichert:\", samples_path)\n",
        "\n",
        "print(\"\\nEmissions-Logs (pro Phase):\")\n",
        "print(f\" - deepseek_{ALIAS}_gen.csv\")\n",
        "print(f\" - deepseek_{ALIAS}_ppl.csv\")\n",
        "print(f\" - deepseek_{ALIAS}_bleu.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2311fff45582400cb90bf8761a4e5163",
            "d865200e3dbf439d9e1cf9e92723aa50",
            "eadee630f4ca42bbbb1627172ce95bd7",
            "1fca46f5565145379abfd8be86c53391",
            "d13dc16655ca4aaa8d1a83866e155cf7",
            "34dd178f708247839dbe4e1f9acb9014",
            "127e8d82e6ad4decad6fc8aea1972363",
            "b5f8cd11238b4fa3aa1f8235d8e9542f",
            "c2f92948aefa49a59c01e299855d0149",
            "2235153316e143ddb67d6b6910e43ef8",
            "3f9b96830caf4533a6b593bfedffcccc",
            "6eb32c6fb4034d55bf83441213abe9f2",
            "88852bbd92e34995a03d5859daafb248",
            "a629ad68eafe49e189404d1e42ea7a3b",
            "427fe0c3dec74916833fd84089b284a5",
            "2de8d908598c4f7f8aa16c6eddbccadf",
            "f449f614876c4c1898f3104d8a798253",
            "35103e49e05946d99e555b50dcf23a6a",
            "50c41d4a0c854d109556cf7de902983a",
            "be1b54da6b01485d910f8e57e04637b4",
            "6333abbb0ab340f1a04c742fa6e0b7e4",
            "5c197b4189e5400db58be36d9708fd69",
            "d1bd76c63eac434b829e17f5e3efd18c",
            "9e1a1b230a254b278cecd4a58b3eebf0",
            "ddc1386b8caf4b9bae5686365a712ea4",
            "351e39bf0ddb4c74a98b60540551bee5",
            "b8ba59673a804d908a43e42526cd5aaa",
            "992dcbbb90654008a793f45368b62b64",
            "66487724ad2f44d19a71d969ab97b1c8",
            "0d089ff7d90b4c0283a1cb6b8ea64a9e",
            "3879432762934025bf12448001685d45",
            "0af19a9f78c14509ab71724532fb7503",
            "8ff5f03b95a04aa9bbad8fad5ba002a6",
            "c3ca5508eb9642c68f650b7b0779be2c",
            "737a33a1a9b4406eb6a3e28848b9a1dd",
            "b662759835d348c1a0cd3286207f0fc5",
            "d4239002b0d541f09805e69a59ec977c",
            "fab64ad1bf644dca8f1543192758b07d",
            "945284e6ba7542879d09400724b290cc",
            "34f8fd751ab04083ba1b589714dda053",
            "f2553350508643c19f96aaeeac3ea4c2",
            "71a402692d864ae8b2ffd9bb570dc07d",
            "9d1da68cd99d476b9f3ac798ad86649d",
            "129539a7983b4506b5caa3f66a511e74",
            "9ade4716c32849f6bc69e194ae0abaab",
            "acea40f7b599432e817b91fb6c91a1aa",
            "e4560db15c214bc2bb0a9426caf0f66b",
            "90c8c4241a2d4cb9af3e2ac46f4281b3",
            "23d3eca4d64548f1b3ee959793e9f0d9",
            "2aa2760b402641f7acc6439e75560001",
            "a27fe964e21d4461ad621896157db1b9",
            "70ab1ef6a1784e8f90f6528b0a6db43c",
            "9e976a89f0f84110b40c415e890cfd29",
            "9bcdbbb3db55459088fea8d97b0bbdfb",
            "64c4e91693ac43c9b0ba808ac1ff42cc",
            "7a2fbb696c3d4af6b1c025238386a9d3",
            "1f580017f0bf431aa3221033e464d20f",
            "20c0ec1c5a93464f8df0d38238023570",
            "1c207c455066492081012c3dd555eb54",
            "ba705f9d60934d6cab69f2f02a1e2fe0",
            "784424950bc84fa68369c1c4ec9119a9",
            "2669f1e4a0b3401b8f4c8e048977fa03",
            "481445210f644a58a4395da2c0f2e947",
            "db178355768e44f0831868779b28057c",
            "ee45a461ac5645a1b88de4f9bbed1dee",
            "46032d42845e4f24ac44dc16e7b4735a",
            "9a8df7a26f2f4a17bf65a244717ca89e",
            "3aea55a48e524d7b9a9abd85bf73c638",
            "58a51095f62e4f3980c672089467cfac",
            "ba3599b3444b490c9c4c426bf9041db5",
            "851e61fb0a634c1d8cf6c5afa3f602ff",
            "d16d7d64b32444078a8fb81095450f62",
            "a0683e130bf1401ab11d29d7fe208e1d",
            "ded07497456a464b8952643043e5697d",
            "ae517a4825164c889470d101052e40c9",
            "6338dc6f4bfc462d88df5e1e3fbc0fa2",
            "2ed9dc7c72a94cd2b810532043919b95",
            "1f1980f3fb974a0d811198c29e842ef1",
            "50a994c842ce4a8b9edcf6ba431eb2fe",
            "77633fbfe4a048bfb6d8c4889e9f7d60",
            "92ea06b7d8da41908b25c733eb33e154",
            "d487191cbd9b4da4849b51da471817ae",
            "2e645bb0315747f59726350f3df8e7a2",
            "74855190fb7940d5a454efe68b6e0f57",
            "54ecd319036f470bbed583d1e2f38af3",
            "484a7e2959f34f9284501172abd55b0d",
            "28f95244496a48078f2f89db8c180d93",
            "c63c39fe77ed4ff1a12be1cf633d79af",
            "43eb58827cf247cb86e3f49333d76b05",
            "ea5a7e7cbe7944bc96ec072965b9e038",
            "c71adeee778d40b8ab4058303d97b638",
            "fd84bef7ac1e4c7b9c524f6f7969e05b",
            "22e93c6c529e40c3b58ad1c9dc228556",
            "627e6997a3e749af9156730bfcb3a00c",
            "a4841ee4f5e74680aeddd370d4cea40a",
            "2961f78f476444e4991df5626f99e1a7",
            "0484ed3557bd463295eddb20e2e564c0",
            "8a430d7c13e046b6870aa37d380b291e",
            "12403548c36840eda06c68d066838c0a",
            "a04285be79b54e15a909df3271b1e8b7",
            "e0f83cb8268f4896b0ad2c8510745368",
            "b56b1245363645298600847299e1b973",
            "78e7649b05dc4f73b694fe28694a7ba6",
            "8ee6c37fc0b547839f85007bd6a1b1a9",
            "1a1dac009ffc4d8a9c0ee3dbc965b851",
            "759392c618074945bc553385386f38c2",
            "abadd3e39cf0410a9b561bb4c7eb7234",
            "386608e2b3fa49ae8e8780dbf9b98530",
            "40c9e4140c4343b884cf1a2b160b10a6",
            "496fed400bd04fc28cb282b250e432f9",
            "a2215e073b7e450daa4628944227ab3b",
            "aa8408141d244477854481f10c469c3e",
            "17514a1263954c0aa7b967df979e28c3",
            "7ec9f4b9ea7345afaccc9ab7e220abed",
            "053640544e3d43dd9312fc37ac7568e1",
            "719d795cb94842189e87ccde42257545",
            "df0f942bf1e34becb2d6cab490eac30b",
            "9e2ec454381748f09122659950266fde",
            "42c2870416d34c4d953a09633ad39228",
            "ffec9a63d68f49e691463925e40063a2",
            "12c05b9a174f4f2086d03f8c7148f930",
            "845acc553bab4198acb8093ce3de33d6",
            "81f6bf22648a4152bc745c234d396767",
            "356db21c6e8149f8b0fadc09f81be8b3",
            "721dbb2dd9c8444f926cf6cb6c30d0b0",
            "341ecc7bd56b4959866a0f6360150108",
            "2380213f42084342b16496803b9b5007",
            "bf9680f4ebc548abb89d8674bede57bc",
            "e04aac5d750b41199b3874eaf3312120",
            "e29c2ffe92f445139d6907de3b9c2044",
            "8a716127e3c64eebbeb00121e1619401",
            "74da0786c9ec48b3bb372b47a4c6fc75",
            "d71f95ec7a994c268e06289ec815ec9c",
            "7acf60dc7f0f4b14989db27e9ef556f9",
            "f0802cf5318d46cf8b2b2bf4f6c27ca8",
            "0a77d9e7c59442daaf3084510d0888a0",
            "dc23f70bceea4f0784377d38d14048db",
            "3f6d4044aace4ad28580b7bb445f913f",
            "3ed0892f0ef0455ea782e1fae5ec7f24",
            "4be817843c91491d8efb2eef63455800",
            "8a95b40d87c0415897f1691c28b9c9e2",
            "e26ea55ee4594fb0ace4939c85d4e1ba",
            "ac681e34e0294b2e818764971ed5f9eb",
            "f6b1d0d8044649ffb9e57172699cd9df",
            "4d46e8964b35445298dd2bf9ed483460",
            "68c8ab2d7e8b4d0ebce233a14c7b36af",
            "69bf9903b3f04365ab74e8009924ebf4",
            "7286d3b38d8543df9480f4de4bfd3efd",
            "e91aa3626d0947c9bb338c4916804fb2",
            "feaab0a59101459283226fed86f39cff",
            "32c93ce42259465e80fcd8e82cf95d64",
            "763759d9521042eb9114dcd7ae3d650a",
            "1d410c5e2f324a6ab6a8036a82e50acf",
            "a24ffb27c00e47ea8e85e322312c547a",
            "9d3f107960444cd88870b5bd9c99fddf",
            "32e52b1e0c0b46128b2144ed4c5ada45",
            "36782f2b2a6d45e79c9f0ccf60196286",
            "8000991c9893412fbe749f09ef2581fa",
            "ab0d20a26825415b8be114455158de8a",
            "d67876ec79e44b7ca0ce3d2ce1af6ad6",
            "bbe33af470824639bb073a8a4b78657a",
            "4d8a5e5424ab4bf6af319132bdfacea6",
            "2c8c167d3f924df5bfe4d5c68cf47ef4",
            "27f5c437d7f44eb4b379302af305691c",
            "ebd9d124a91346c2a620f3b3f48d6f2d",
            "413d29aae56e498795d5300fc2ed542b",
            "f6da58e2c9a14d3488015ec78bab4644",
            "25c001700636400c8c28587ae5314184",
            "70e01454358c43ed9d44545031a8f45d",
            "519b40586e33411ebc1557aaedd3df18",
            "da82b3a04d9f4896bb3fa10bdf066f6a",
            "1266267f205d487f85a8cb25a9393e2d",
            "2ac74b1a56d048cf90282796677339fc",
            "95597f519d6f4260a7ad3b7932d7da57",
            "0bb738661d9c46da8470d89b72f907a3",
            "821902f0b29543508300bbfc654e3b70",
            "9019377f7bb244848c098e804d3747f1",
            "dcd3160b17f148fa9b05edf8671ae35c",
            "28b43175cc6147eda8494dd4f87bd76a",
            "110bc136277f4fd5a1ac87f37e098317",
            "05365dc872cb48e0922a2a14d05b2bb7",
            "2165ef4bd2d34cd0b516eec04bb15e5d",
            "4945aa3a736f42dea9bd98e70d5a6b64",
            "c2f10811f22e4f85ae3092a90c9f832f",
            "31be0d9a6fec46c390efe552f51b1dde",
            "26dfd8124c2d44c3a3203ff93c18e3e4",
            "39ab3b0fab8346d58f23f368ed318102",
            "cd3cce5974f542219672d57e9a642ab4",
            "668418e8079d4a0e857497de200d7527",
            "1674172cfa6d42c7bdd7a6155b298e5b",
            "db857d3286604572bfc64138b6186712",
            "b6aee02b90bc42f490ee6c901b9081ac",
            "5e3071bd5b0e4fda8267a03827713eac",
            "00e6a7537bde441093e9909ce088467d",
            "43260b0f0d3e4c17b7eab6997be7cace",
            "5522bb3027964da7b31532b21811f359",
            "5b95c58b7ef542f4ae07a7a3e283e0a3",
            "a489a09b94dd41b2820741ac26a95c69",
            "8c094776826c46df93d2377cb22a6b1c",
            "b8b007d40bd94ac7aaaf6dd76715a711",
            "7d47d49d324d4b79a6c1e97295e46b66",
            "59ea720ffac04fa198b2f896886f486c",
            "b2266f116b6241f8b2152864a076a19a",
            "6042719c2b5041619b9ab1b09b0c8b6f",
            "38b26f0ac2c247bf8261f52e3524e101",
            "fdf59f2fc7b8475c935f6017d1e07dfe",
            "f536b333e7f9497b93316a880755109c",
            "a314da92f19540bf8fcbd6d59cf2f84f",
            "5d080de45045411691d5e4cfa8516c93",
            "f56c8403155646248cdb1d9ca50658a7",
            "6007b80b89584044a55e4ddb54dd233e",
            "ea0cdf13d80d462693d0718aee29d0bb",
            "e34d6b64e4d644728757eccdceb7d855",
            "06dab4435ad943f99de31973d9621d4d",
            "5c1d0259d02f4ad78917bb000e639e55",
            "c39401e3cd2c46f29c717c854b3ee885",
            "6122944165da4ec9ba409c4aa0a10c38",
            "ffb540777a4447a88d47c5cb045b7670",
            "17fbe70804814148a908e282e5ee10f1",
            "470471fc40ae4465aaddf0f544ac9861",
            "bcc02341eb214a448cddf77bea82912b",
            "4b4be368a1ec44a7b833e94b33fbc09d",
            "d7d3c290753d45a5b879a75222218984",
            "63e4f61f713f4a0d84b53a1af2033af0",
            "a89e6777a21d4f939dc6fa1a3e001bba",
            "b8914ff5037c4060a6982a22e3be7b00",
            "e1adc8f915c949cba029e3c9120e6c24",
            "07e952f375d1434b835836c401add352",
            "28b62d75db314f838d6ad40b0c7a4b83",
            "a67482543e124c45a7f3c2ad82480ed2",
            "7648bdf1c3634585ac077c64901a0d4b"
          ]
        },
        "id": "iY6kATbMG2gS",
        "outputId": "35cd464b-ac38-456e-ecbb-b10fa1d47629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Starte Baseline (per Phase): DeepSeek-R1-Distill-Qwen-1.5B\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2311fff45582400cb90bf8761a4e5163"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6eb32c6fb4034d55bf83441213abe9f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1bd76c63eac434b829e17f5e3efd18c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3ca5508eb9642c68f650b7b0779be2c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ade4716c32849f6bc69e194ae0abaab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a2fbb696c3d4af6b1c025238386a9d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a8df7a26f2f4a17bf65a244717ca89e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f1980f3fb974a0d811198c29e842ef1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43eb58827cf247cb86e3f49333d76b05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a04285be79b54e15a909df3271b1e8b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2215e073b7e450daa4628944227ab3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "845acc553bab4198acb8093ce3de33d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d71f95ec7a994c268e06289ec815ec9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00000-of-00003.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6b1d0d8044649ffb9e57172699cd9df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00001-of-00003.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d3f107960444cd88870b5bd9c99fddf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00002-of-00003.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "413d29aae56e498795d5300fc2ed542b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/validation-00000-of-00001.parquet:   0%|          | 0.00/474k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9019377f7bb244848c098e804d3747f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/test-00000-of-00001.parquet:   0%|          | 0.00/509k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd3cce5974f542219672d57e9a642ab4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c094776826c46df93d2377cb22a6b1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f56c8403155646248cdb1d9ca50658a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bcc02341eb214a448cddf77bea82912b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-Phase (r1q15b) — Standort: GCP europe-west10\n",
            "  phase     time_s  energy_kwh    co2_kg  kg_per_kwh  tokens_out        ppl  \\\n",
            "0   gen   3.172588    0.000094  0.000043    0.452621        96.0        NaN   \n",
            "1   ppl   3.948225    0.000108  0.000049    0.452621         NaN  212.92872   \n",
            "2  bleu  43.889363    0.001268  0.000574    0.452621         NaN        NaN   \n",
            "\n",
            "        bleu   tokens_s wh_per_token s_per_example wh_per_example  \n",
            "0        NaN  30.259207     0.000983          None           None  \n",
            "1        NaN       None         None          None           None  \n",
            "2  12.974741       None         None      1.371543       0.039618  \n",
            "\n",
            "Gesamt (summiert über Phasen):\n",
            "                                    model_id   alias precision     time_s  \\\n",
            "0  deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  r1q15b      fp16  51.010176   \n",
            "\n",
            "   energy_kwh    co2_kg  kg_per_kwh  tokens_out        ppl       bleu  \\\n",
            "0     0.00147  0.000665    0.452621          96  212.92872  12.974741   \n",
            "\n",
            "     ram_GB  vram_alloc_GB  vram_reserved_GB  \\\n",
            "0  2.716827       3.318996          3.425781   \n",
            "\n",
            "                                     notes  \n",
            "0  GPU=NVIDIA A100-SXM4-40GB, VRAM=39.6 GB  \n",
            "Gespeichert (gesamt): /content/drive/MyDrive/LLM-Effizienz/4_2_Baseline/baseline_deepseek_results.csv\n",
            "Gespeichert (per Phase): /content/drive/MyDrive/LLM-Effizienz/4_2_Baseline/baseline_deepseek_per_phase.csv\n",
            "Beispiele gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_2_Baseline/baseline_samples_r1q15b.txt\n",
            "\n",
            "Emissions-Logs (pro Phase):\n",
            " - deepseek_r1q15b_gen.csv\n",
            " - deepseek_r1q15b_ppl.csv\n",
            " - deepseek_r1q15b_bleu.csv\n"
          ]
        }
      ]
    }
  ]
}