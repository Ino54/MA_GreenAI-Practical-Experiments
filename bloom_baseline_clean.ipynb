{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1758481872864,
     "user": {
      "displayName": "Kossiwa Ines Deh",
      "userId": "09014473696885298182"
     },
     "user_tz": -120
    },
    "id": "2xg9gJ0BoN2h",
    "outputId": "96c94d26-cd1b-42a6-bb25-9ca3ee24da4a"
   },
   "outputs": [],
   "source": [
    " # --- Requirements schreiben & installieren\n",
    "%%writefile requirements.txt\n",
    "transformers\n",
    "accelerate\n",
    "bitsandbytes\n",
    "datasets\n",
    "evaluate\n",
    "sacrebleu\n",
    "codecarbon>=2.5,<3\n",
    "pynvml>=11.5.0\n",
    "psutil\n",
    "numpy\n",
    "pandas\n",
    "huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16575,
     "status": "ok",
     "timestamp": 1758481935595,
     "user": {
      "displayName": "Kossiwa Ines Deh",
      "userId": "09014473696885298182"
     },
     "user_tz": -120
    },
    "id": "qRbPt8ETsBcz",
    "outputId": "053d3d02-37fc-45e8-b8ec-bbe064059dc4"
   },
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "!pip uninstall -y -q google-genai firebase-admin || true\n",
    "!pip show fief-client || echo \"fief-client nicht installiert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35334,
     "status": "ok",
     "timestamp": 1758482011411,
     "user": {
      "displayName": "Kossiwa Ines Deh",
      "userId": "09014473696885298182"
     },
     "user_tz": -120
    },
    "id": "ewpdq_acsS0D",
    "outputId": "f09ab70f-90c7-4314-b6b5-48a5325e2844"
   },
   "outputs": [],
   "source": [
    "# --- Google Drive mounten ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# --- Projektordner setzen ---\n",
    "import os, pathlib, re\n",
    "project_path = \"/content/drive/MyDrive/LLM-Effizienz/4_2_Baseline\"\n",
    "pathlib.Path(project_path).mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(project_path)\n",
    "print(\"Arbeitsordner:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4291,
     "status": "ok",
     "timestamp": 1758482084621,
     "user": {
      "displayName": "Kossiwa Ines Deh",
      "userId": "09014473696885298182"
     },
     "user_tz": -120
    },
    "id": "Tg2rtIsEsl-D",
    "outputId": "27f4d299-2dde-439b-c9e9-5e9b84b7cabc"
   },
   "outputs": [],
   "source": [
    "# --- Hugging Face Login via Colab-Secret\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = userdata.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(hf_token)\n",
    "    print(\"Hugging Face Login erfolgreich!\")\n",
    "else:\n",
    "    print(\"WARNUNG: Kein HF_TOKEN gefunden – öffentliche Modelle meist trotzdem ladbar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19045,
     "status": "ok",
     "timestamp": 1758482129372,
     "user": {
      "displayName": "Kossiwa Ines Deh",
      "userId": "09014473696885298182"
     },
     "user_tz": -120
    },
    "id": "KphQXbTfsz68",
    "outputId": "956fe028-4ad2-4e77-b3db-bea0c22972d9"
   },
   "outputs": [],
   "source": [
    "# --- Imports & Setup ---\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import time, math, gc, platform, inspect\n",
    "from dataclasses import dataclass, asdict\n",
    "from contextlib import nullcontext\n",
    "from typing import Optional, Tuple\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
    "from codecarbon import EmissionsTracker, __version__ as cc_ver\n",
    "\n",
    "print(\"CodeCarbon-Version:\", cc_ver)\n",
    "\n",
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_total_gb = torch.cuda.get_device_properties(0).total_memory/(1024**3)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.set_grad_enabled(False)\n",
    "else:\n",
    "    gpu_name = \"CPU\"\n",
    "    vram_total_gb = 0.0\n",
    "\n",
    "print(f\"Device: {device} | GPU: {gpu_name} | VRAM: {vram_total_gb:.1f} GB | Torch {torch.__version__} | Python {platform.python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758482198972,
     "user": {
      "displayName": "Kossiwa Ines Deh",
      "userId": "09014473696885298182"
     },
     "user_tz": -120
    },
    "id": "vca3D67ctB0-"
   },
   "outputs": [],
   "source": [
    "# ========== Standort-Konfiguration ==========\n",
    "USE_GCP_REGION = True  # True => google/europe-west10 (Berlin), False => deutscher Strommix (DEU)\n",
    "\n",
    "COUNTRY_ISO_CODE = \"DEU\"          # Ländermix Deutschland\n",
    "CLOUD_PROVIDER   = \"google\"       # Cloud-Anbieter\n",
    "CLOUD_REGION     = \"europe-west10\"  # GCP Berlin\n",
    "# Referenz: GCP Frankfurt=europe-west3, Niederlande=europe-west4, Berlin=europe-west10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1758482268107,
     "user": {
      "displayName": "Kossiwa Ines Deh",
      "userId": "09014473696885298182"
     },
     "user_tz": -120
    },
    "id": "cZzayuxXtPpw"
   },
   "outputs": [],
   "source": [
    "# -------- CodeCarbon Helfer & Fallbacks --------\n",
    "import os as _os, time as _time\n",
    "\n",
    "def tracker_kwargs_base():\n",
    "    \"\"\"\n",
    "    Gemeinsame Tracker-Parameter:\n",
    "    - feines Power-Sampling (1s)\n",
    "    - 'process' Tracking (nur aktueller Prozess)\n",
    "    - Standortwahl:\n",
    "        * USE_GCP_REGION=True  -> cloud_provider/region (hier: Google Berlin)\n",
    "        * USE_GCP_REGION=False -> country_iso_code=DEU\n",
    "    Hinweis: Falls beides gesetzt ist, priorisiert CodeCarbon i. d. R. die Cloud-Region.\n",
    "    \"\"\"\n",
    "    base = dict(log_level=\"error\", output_dir=\".\")\n",
    "    try:\n",
    "        sig = inspect.signature(EmissionsTracker.__init__)\n",
    "        if \"measure_power_secs\" in sig.parameters:\n",
    "            base[\"measure_power_secs\"] = 1\n",
    "        if \"tracking_mode\" in sig.parameters:\n",
    "            base[\"tracking_mode\"] = \"process\"\n",
    "\n",
    "        if USE_GCP_REGION:\n",
    "            # Cloud-Region explizit setzen (Berlin)\n",
    "            if \"cloud_provider\" in sig.parameters:\n",
    "                base[\"cloud_provider\"] = CLOUD_PROVIDER\n",
    "            if \"cloud_region\" in sig.parameters:\n",
    "                base[\"cloud_region\"] = CLOUD_REGION\n",
    "            # Optional: country zusätzlich setzen (wird typischerweise ignoriert, schadet aber nicht)\n",
    "            if \"country_iso_code\" in sig.parameters:\n",
    "                base[\"country_iso_code\"] = COUNTRY_ISO_CODE\n",
    "        else:\n",
    "            # Deutscher Strommix (ohne Cloud-Autodetektion)\n",
    "            if \"country_iso_code\" in sig.parameters:\n",
    "                base[\"country_iso_code\"] = COUNTRY_ISO_CODE\n",
    "            if \"cloud_provider\" in sig.parameters:\n",
    "                base[\"cloud_provider\"] = None\n",
    "            if \"cloud_region\" in sig.parameters:\n",
    "                base[\"cloud_region\"] = None\n",
    "    except Exception:\n",
    "        pass\n",
    "    return base\n",
    "\n",
    "def make_tracker_named(project_name: str, output_file: str):\n",
    "    # Eigener Cache pro Run (vermeidet Lock-Konflikte)\n",
    "    cache_dir = f\"/content/.codecarbon_cache_{project_name}_{int(_time.time())}\"\n",
    "    _os.environ[\"CODECARBON_CACHE_DIR\"] = cache_dir\n",
    "    # evtl. alten Lock entfernen (best effort)\n",
    "    for d in (_os.path.expanduser(\"~/.codecarbon\"), \"/content/.codecarbon\"):\n",
    "        lock_file = _os.path.join(d, \"codecarbon.lock\")\n",
    "        if os.path.exists(lock_file):\n",
    "            try: os.remove(lock_file)\n",
    "            except: pass\n",
    "    return EmissionsTracker(project_name=project_name, output_file=output_file, **tracker_kwargs_base())\n",
    "\n",
    "def safe_start(tracker):\n",
    "    try:\n",
    "        tracker.start(); return True\n",
    "    except Exception as e:\n",
    "        print(f\"[CodeCarbon] Start fehlgeschlagen: {e} → Fallback 0/0.\"); return False\n",
    "\n",
    "def safe_stop(tracker, started: bool):\n",
    "    if not started:  # nichts gemessen\n",
    "        return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
    "    try:\n",
    "        return tracker.stop()\n",
    "    except Exception as e:\n",
    "        print(f\"[CodeCarbon] Stop fehlgeschlagen: {e} → Fallback 0/0.\")\n",
    "        return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
    "\n",
    "def unpack_emissions(em):\n",
    "    # normalisiert verschiedene Rückgabeformen (Objekt/dict/float/None)\n",
    "    if em is None:\n",
    "        return 0.0, 0.0\n",
    "    if hasattr(em, \"energy_consumed\") and hasattr(em, \"emissions\"):\n",
    "        try: return float(em.energy_consumed), float(em.emissions)\n",
    "        except Exception: pass\n",
    "    if isinstance(em, dict):\n",
    "        e = em.get(\"energy_consumed\", 0.0); c = em.get(\"emissions\", em.get(\"emissions_kg\", 0.0))\n",
    "        try: return float(e), float(c)\n",
    "        except Exception: return 0.0, 0.0\n",
    "    try:\n",
    "        return 0.0, float(em)  # nur CO2\n",
    "    except Exception:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "def read_energy_from_log(path: str) -> float:\n",
    "    # CSV-Fallback: Energie (kWh) aus CodeCarbon-Log\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            return 0.0\n",
    "        df = pd.read_csv(path)\n",
    "        for c in [\"energy_consumed\",\"energy_consumed_kwh\",\"energy_consumed (kWh)\",\"energy (kWh)\"]:\n",
    "            if c in df.columns:\n",
    "                return float(df[c].iloc[-1])\n",
    "        for c in df.columns:\n",
    "            n = c.lower()\n",
    "            if \"energy\" in n and \"kwh\" in n:\n",
    "                return float(df[c].iloc[-1])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return 0.0\n",
    "\n",
    "def measure_phase(phase_name: str, fn, log_prefix: str):\n",
    "    # Startet eigenen Tracker pro Phase, misst Zeit & Energie, nutzt CSV-Fallback.\n",
    "    logfile = f\"{log_prefix}_{phase_name}.csv\"\n",
    "    tracker = make_tracker_named(project_name=f\"{log_prefix}_{phase_name}\", output_file=logfile)\n",
    "    started = safe_start(tracker)\n",
    "    t0 = time.time()\n",
    "    result = fn()\n",
    "    t1 = time.time()\n",
    "    em_raw = safe_stop(tracker, started)\n",
    "    energy_kwh, co2_kg = unpack_emissions(em_raw)\n",
    "    if not energy_kwh or energy_kwh == 0.0:\n",
    "        energy_from_csv = read_energy_from_log(logfile)\n",
    "        if energy_from_csv:\n",
    "            energy_kwh = energy_from_csv\n",
    "    return {\"phase\": phase_name, \"time_s\": t1 - t0, \"energy_kwh\": energy_kwh, \"co2_kg\": co2_kg}, result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d9717aa7a8644fcda5e9ede55e0a0ba4",
      "8d886314208e44fb8446c75d8e398bf1",
      "a85ff58c00294389a6a15d14fe10db0e",
      "112107affffa4563b80607525d209640",
      "7a71b47d8a8343c6b9fe2ca48f5aca4c",
      "63053d41691349b586bf4c2ad3ba32b4",
      "4ab985c4f28648f1b3827737865e3e50",
      "0ee2ecc1a8a440738f097d8e927cfeca",
      "3edfe21876394234b96daa4fc5d4b58d",
      "0af5eaa1c71c426c9e0821e026904c98",
      "aef31095d6b348fc8a8d21a0f68ce5bc"
     ]
    },
    "executionInfo": {
     "elapsed": 3043,
     "status": "ok",
     "timestamp": 1758482334516,
     "user": {
      "displayName": "Kossiwa Ines Deh",
      "userId": "09014473696885298182"
     },
     "user_tz": -120
    },
    "id": "QgAozQJltg9z",
    "outputId": "1f1c6994-1b0f-4edf-9578-a4b6b70ac839"
   },
   "outputs": [],
   "source": [
    "# --- Evaluation-Config ---\n",
    "MODELS = [\n",
    "    (\"bigscience/bloom-560m\", \"bloom560m\", \"b560\"),\n",
    "    (\"bigscience/bloom-3b\",   \"bloom3b\",   \"b3b\"),\n",
    "]\n",
    "EVAL = {\n",
    "    \"max_new_tokens\": 32,\n",
    "    \"ppl_dataset\":  {\"name\":\"wikitext\",\"config\":\"wikitext-2-raw-v1\",\"split\":\"test[:1%]\"},\n",
    "    \"bleu_dataset\": {\"name\":\"wmt14\",\"config\":\"de-en\",\"split\":\"test[:32]\"},\n",
    "}\n",
    "def parse_subset_count(split_str: str, default=32):\n",
    "    m = re.search(r\":\\s*(\\d+)\\s*\\]$\", split_str or \"\"); return int(m.group(1)) if m else default\n",
    "BLEU_N = parse_subset_count(EVAL[\"bleu_dataset\"][\"split\"], default=32)\n",
    "\n",
    "PROMPTS = [\n",
    "    \"Schreibe einen kurzen Absatz über nachhaltige KI.\",\n",
    "    \"Erkläre in einfachen Worten, was Quantisierung in neuronalen Netzen ist.\",\n",
    "    \"Nenne drei Vorteile von Mixture-of-Experts-Modellen.\"\n",
    "]\n",
    "\n",
    "def autocast_ctx():\n",
    "    return torch.autocast(device_type=\"cuda\", dtype=torch.float16) if device==\"cuda\" else nullcontext()\n",
    "\n",
    "def capture_memory():\n",
    "    ram = psutil.Process().memory_info().rss\n",
    "    valloc = torch.cuda.memory_allocated() if device==\"cuda\" else 0\n",
    "    vres  = torch.cuda.memory_reserved()  if device==\"cuda\" else 0\n",
    "    return ram, valloc, vres\n",
    "\n",
    "def bytes_to_gb(b): return float(b)/(1024**3)\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "@dataclass\n",
    "class BaselineResult:\n",
    "    model_id: str\n",
    "    alias: str\n",
    "    precision: str\n",
    "    time_s: float\n",
    "    energy_kwh: float\n",
    "    co2_kg: float\n",
    "    tokens_out: int\n",
    "    ram_GB: float\n",
    "    vram_alloc_GB: float\n",
    "    vram_reserved_GB: float\n",
    "    ppl: Optional[float] = None\n",
    "    bleu: Optional[float] = None\n",
    "    notes: str = \"\"\n",
    "\n",
    "def safe_max_len(tok, model, fallback=2048, upper=100000):\n",
    "    cand = getattr(tok, \"model_max_length\", None)\n",
    "    if isinstance(cand, int) and 0 < cand < upper: return cand\n",
    "    cand = getattr(getattr(model, \"config\", None), \"max_position_embeddings\", None)\n",
    "    if isinstance(cand, int) and 0 < cand < upper: return cand\n",
    "    return fallback\n",
    "\n",
    "def load_model(model_id: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    tok.padding_side = \"left\"; tok.pad_token = tok.eos_token\n",
    "    try:\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, device_map=\"auto\",\n",
    "            torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n",
    "        )\n",
    "        return tok, model, \"fp16\"\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" not in str(e).lower(): raise\n",
    "        print(f\"[Info] OOM bei FP16 für {model_id}. Fallback auf INT8…\")\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "    bnb = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, device_map=\"auto\", quantization_config=bnb,\n",
    "        torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n",
    "    )\n",
    "    return tok, model, \"int8\"\n",
    "\n",
    "def warmup(model, tok, max_len):\n",
    "    with torch.no_grad(), autocast_ctx():\n",
    "        dummy = tok(\"Hello\", return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
    "        _ = model.generate(**dummy, max_new_tokens=1, do_sample=False, pad_token_id=tok.eos_token_id)\n",
    "\n",
    "def simple_generate(model, tok, prompts, max_new_tokens=32):\n",
    "    model.eval(); total_gen_tokens, texts = 0, []\n",
    "    max_len = safe_max_len(tok, model)\n",
    "    for p in prompts:\n",
    "        enc = tok(p, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "        input_ids = enc[\"input_ids\"].to(model.device)\n",
    "        attn = enc.get(\"attention_mask\", None);\n",
    "        if attn is not None: attn = attn.to(model.device)\n",
    "        room = max_len - input_ids.shape[1]\n",
    "        cur_new = max(1, min(max_new_tokens, int(room)))\n",
    "        with torch.no_grad(), autocast_ctx():\n",
    "            out_ids = model.generate(\n",
    "                input_ids=input_ids, attention_mask=attn,\n",
    "                max_new_tokens=cur_new, do_sample=False,\n",
    "                pad_token_id=tok.eos_token_id\n",
    "            )\n",
    "        gen_len = out_ids.shape[1] - input_ids.shape[1]\n",
    "        total_gen_tokens += int(gen_len)\n",
    "        texts.append(tok.decode(out_ids[0], skip_special_tokens=True))\n",
    "    return texts, total_gen_tokens\n",
    "\n",
    "def eval_perplexity(model, tok, ds_cfg):\n",
    "    ds = load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
    "    max_len = safe_max_len(tok, model); losses = []\n",
    "    with torch.no_grad():\n",
    "        for t in ds[\"text\"]:\n",
    "            if not isinstance(t, str) or len(t.strip()) < 4: continue\n",
    "            enc = tok(t, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "            ids = enc[\"input_ids\"].to(model.device)\n",
    "            with autocast_ctx(): out = model(ids, labels=ids)\n",
    "            losses.append(float(out.loss.detach().cpu()))\n",
    "    return math.exp(np.mean(losses)) if losses else None\n",
    "\n",
    "def eval_bleu_llm(model, tok, ds_cfg, max_new_tokens=32):\n",
    "    ds = load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
    "    max_len = safe_max_len(tok, model); preds, refs = [], []\n",
    "    with torch.no_grad():\n",
    "        for ex in ds:\n",
    "            de, en = ex[\"translation\"][\"de\"], ex[\"translation\"][\"en\"]\n",
    "            prompt = f\"Translate to English:\\nGerman: {de}\\nEnglish:\"\n",
    "            inputs = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
    "            room = max_len - inputs[\"input_ids\"].shape[1]\n",
    "            cur_new = max(1, min(max_new_tokens, int(room)))\n",
    "            with autocast_ctx():\n",
    "                out = model.generate(**inputs, max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
    "            gen = tok.decode(out[0], skip_special_tokens=True)\n",
    "            seg = gen.split(\"English:\")[-1].strip()\n",
    "            hyp = seg.split(\"\\n\")[0].strip() or gen.strip()\n",
    "            preds.append(hyp); refs.append([en])\n",
    "    return float(bleu_metric.compute(predictions=preds, references=refs)[\"score\"])\n",
    "\n",
    "# --- Baseline-Lauf (per Phase) ---\n",
    "def run_baseline(model_id: str, alias_long: str, alias_short: str):\n",
    "    tok, model, prec = load_model(model_id)\n",
    "    max_len = safe_max_len(tok, model)\n",
    "    warmup(model, tok, max_len)  # Warm-up außerhalb der Messung\n",
    "    log_prefix = f\"bloom_{alias_short}\"\n",
    "\n",
    "    def _do_gen(): return simple_generate(model, tok, PROMPTS, EVAL[\"max_new_tokens\"])\n",
    "    gen_metrics, (examples, tokens_out) = measure_phase(\"gen\", _do_gen, log_prefix)\n",
    "\n",
    "    def _do_ppl(): return eval_perplexity(model, tok, EVAL[\"ppl_dataset\"])\n",
    "    ppl_metrics, ppl = measure_phase(\"ppl\", _do_ppl, log_prefix)\n",
    "\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    def _do_bleu(): return eval_bleu_llm(model, tok, EVAL[\"bleu_dataset\"], EVAL[\"max_new_tokens\"])\n",
    "    bleu_metrics, bleu = measure_phase(\"bleu\", _do_bleu, log_prefix)\n",
    "\n",
    "    total_time   = gen_metrics[\"time_s\"] + ppl_metrics[\"time_s\"] + bleu_metrics[\"time_s\"]\n",
    "    total_energy = gen_metrics[\"energy_kwh\"] + ppl_metrics[\"energy_kwh\"] + bleu_metrics[\"energy_kwh\"]\n",
    "    total_co2    = gen_metrics[\"co2_kg\"] + ppl_metrics[\"co2_kg\"] + bleu_metrics[\"co2_kg\"]\n",
    "\n",
    "    ram, valloc, vres = capture_memory()\n",
    "\n",
    "    per_phase_df = pd.DataFrame([gen_metrics, ppl_metrics, bleu_metrics])\n",
    "    per_phase_df[\"alias\"] = alias_short\n",
    "    per_phase_df[\"model_id\"] = model_id\n",
    "    per_phase_df[\"precision\"] = prec\n",
    "    per_phase_df[\"tokens_out\"] = [tokens_out, None, None]\n",
    "    per_phase_df[\"ppl\"] = [None, ppl, None]\n",
    "    per_phase_df[\"bleu\"] = [None, None, bleu]\n",
    "\n",
    "    # Abgeleitete Kennzahlen (vektorisiert, robust)\n",
    "    per_phase_df[\"wh_total\"] = per_phase_df[\"energy_kwh\"] * 1000.0\n",
    "    per_phase_df[\"tokens_s\"] = None\n",
    "    per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"tokens_s\"] = (\n",
    "        per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"tokens_out\"]\n",
    "        / per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"time_s\"]\n",
    "    )\n",
    "    per_phase_df[\"wh_per_token\"] = None\n",
    "    per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"wh_per_token\"] = (\n",
    "        per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"wh_total\"]\n",
    "        / per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"tokens_out\"]\n",
    "    )\n",
    "    per_phase_df[\"s_per_example\"] = None\n",
    "    per_phase_df.loc[per_phase_df[\"phase\"]==\"bleu\", \"s_per_example\"] = (\n",
    "        per_phase_df.loc[per_phase_df[\"phase\"]==\"bleu\", \"time_s\"] / float(BLEU_N)\n",
    "    )\n",
    "    per_phase_df[\"wh_per_example\"] = None\n",
    "    per_phase_df.loc[per_phase_df[\"phase\"]==\"bleu\", \"wh_per_example\"] = (\n",
    "        per_phase_df.loc[per_phase_df[\"phase\"]==\"bleu\", \"wh_total\"] / float(BLEU_N)\n",
    "    )\n",
    "    # Emissionsfaktor (kg CO2 pro kWh) je Phase\n",
    "    per_phase_df[\"kg_per_kwh\"] = (per_phase_df[\"co2_kg\"] / per_phase_df[\"energy_kwh\"]).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    print(f\"\\nPer-Phase ({alias_short}) — Standort:\",\n",
    "          f\"GCP {CLOUD_REGION}\" if USE_GCP_REGION else f\"Ländermix {COUNTRY_ISO_CODE}\")\n",
    "    print(per_phase_df[[\n",
    "        \"phase\",\"time_s\",\"energy_kwh\",\"co2_kg\",\"kg_per_kwh\",\n",
    "        \"tokens_out\",\"ppl\",\"bleu\",\"tokens_s\",\"wh_per_token\",\"s_per_example\",\"wh_per_example\"\n",
    "    ]])\n",
    "\n",
    "    res = BaselineResult(\n",
    "        model_id=model_id, alias=alias_short, precision=prec,\n",
    "        time_s=total_time, energy_kwh=total_energy, co2_kg=total_co2,\n",
    "        tokens_out=int(tokens_out),\n",
    "        ram_GB=bytes_to_gb(ram), vram_alloc_GB=bytes_to_gb(valloc), vram_reserved_GB=bytes_to_gb(vres),\n",
    "        ppl=ppl, bleu=bleu, notes=f\"GPU={gpu_name}, VRAM={vram_total_gb:.1f} GB\"\n",
    "    )\n",
    "    return res, examples, per_phase_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9b7ef25e2c5f4ba895bf459bf4cba5b8",
      "68183847118a4227ad589147b0ba2782",
      "21468e17bfbb4289b999cfcd25796657",
      "0f9bea05550d481ab928233178d838c9",
      "544231f213004aacb383eb3afe58831a",
      "06a18a8dda504c45964a9636dbf5ad97",
      "f5b44b3a2845487aa4533cc489adc18d",
      "93bd0db72c864f9db084cb059cb6e81f",
      "5d34de70207c43d794d47a90670a8ec0",
      "ac2fe065fa8a4af1bd48aa3b600c34ff",
      "29a8beec31b54963abf07d4a23eb724c",
      "420b0c38c11546d6ac0d46c4a0ef6fa1",
      "375f769ba5ab4ad2a30db1eca7bb09aa",
      "1b5228ad3b4b4fb69cf37f6ff3505e33",
      "77df51e1418f4d0eae0f2c1d95b397e1",
      "94a76c9b02df445d8a1494dccf7c0cc1",
      "166849ad4f054b1c8120550af563580b",
      "f62af897932a467d99209f3a6220d090",
      "ce72b2f3fbb74d76960c7161b2758241",
      "c815e32883244fb8acfdc0d697714ac8",
      "710ca6ef52a346ca9efb1d34ce67171f",
      "313a87fd49e447f397672dcb56ef1afa",
      "28b44c64a251467c8153d369f1dc3dd1",
      "38c05331d4624131a2b273d5d7ab3d72",
      "77a8070a2c514c069efc696456351faa",
      "23f2b560e95f4575917023ba5b015511",
      "6b3f5f31017949dea0fb7cd674c3434b",
      "bb9f5ec7c0ed4aa282e915c4a53043d9",
      "6d8e877822f6428da5553213bccd4330",
      "76c69b71a365433d88413032c6789702",
      "2eaaddfb0a1e4a1f88ba1763dedd2abc",
      "e22beee1ef524dab9b24acd62e0b5f26",
      "21b3c7dad1fa4d2eb2f4ed85b6cf6cb0",
      "0cb68f8e7a8c43f5bd1916969ecafa74",
      "1f2b65b490524bec876b5b9cbc04f8bb",
      "fde9c939da554dec990aee5dd3ed3ecf",
      "a044a7db319e44ab84f0f6c0b8ba724b",
      "4102a80be96d41a98d09809168bc00d9",
      "0a6774076c764b2aa0aa6b2df3754a3f",
      "7240f33d67074f8f9306b28ac3112262",
      "c3a185df4ac44537bd297414a100df95",
      "37229b6e46af4d22a553d8f21ba82d3d",
      "cfdb1f2643364a1680a31198f10df73a",
      "48bc19c247974b4c9c79b7d330eb7db7",
      "be629c327cc14a53aee7d212a7899535",
      "90b48bc04aa240adbb56c0533b6ac544",
      "49a4041d21cf4da8bf45ba2e97515942",
      "786e2c4ffb694e93aa3728aa0754c245",
      "4a04713804164810b77ddce6216225da",
      "9be7be16306c4f2b964af1c5b754bdf6",
      "54d1866d338447719581c7e90e855591",
      "e9127ad0e88e44e989d520d6f6d6c877",
      "4e4dce88f1324f779a4ab7f4d26f6b84",
      "3a15cde7251f49b89a4f6581dc45cf2b",
      "85f43fc94abc4b9c82b2742bccba9b7c",
      "1e721a55dcb746bf8810d6fcc699fbf9",
      "807de171b9774b9a98288eae01e0e8c4",
      "2980a4b19719425587ab56bec70433fe",
      "6b4eaf0ffaed4ffe9b243554e2110226",
      "28acb7c1e8954f3ab566fbe97981c0be",
      "d00fc1d8df2249cab537d2ac6b3d6932",
      "5057401a2abc4e01aa8cc511ab0da89f",
      "9488823decf541f1bcdf5a6ee9b05b4c",
      "8803a4e2bda74ab387939e0be21ac0f8",
      "4a5b702b01e542c59573a1f3941852bd",
      "4826919b69884389ac6c4a280b7acfb8",
      "c058b81813604e848597dcbed353a351",
      "497a2a0a35cb4d1380738792ff718925",
      "111d5dc764164c4d98f81a743835cd09",
      "40f8e3abba78490cb9638c2258825e72",
      "751a21aa315b4dc294c275d6e461fbf9",
      "063cc8e4b7b1429d96df177c2429c87f",
      "4537bc8b7bce4235a8747c442d697577",
      "762113f91f444e818653341973f1503d",
      "6b1391180b89429bbf2955b071920ee4",
      "023794f9220942aabe0bde1aee3a2367",
      "48dff386d379482081ff720dcec55b5b",
      "91758daf15ed43da9f00be1976fe5074",
      "500e7b051e5745f98e5d10884bbf79ab",
      "3d1f1d6112224e8fafb22672878fbf8c",
      "98485e5a27d14bffa38e34d777961ca3",
      "68a80f94959f478e944e0e28eb886d82",
      "4a52e779a1e04e95b3d860c09616fad8",
      "a90c2f622f224a668eb25b8939acc2fd",
      "95daa5cdf8a741f4b3ccafbee7989c88",
      "edd21722bb6d41f196bad1faa05ab092",
      "18c769abcf844b14a1ae45f10e6bdac2",
      "751e33ae04034be1bf4175c0508c9d02",
      "80aa7a478fe84ca3b27f294b37e4f80d",
      "a1ae2c8e90ac4152bdc3969b0017a17a",
      "f45ddc588be44a31b2179241d48f7745",
      "fe29c908e62d48ef8a0130f7d5fe02d9",
      "a69d934662364b42afc1c1c49dd9a1f7",
      "48faa3de60ad44c7b5e47f528d85678e",
      "3b34fdef119847b1ab035f36bcad4ebb",
      "f6bb7d476f634d89a67e8227f8627b63",
      "2539f350ebba4042bc7e0130b8861e64",
      "05d597541de0440a958100491c72e612",
      "44526d0e330c427d94f2e9b5b5c5ac84",
      "9979bf5d9e5c4f1794a532a34efe3336",
      "f99521f1d8fb45d29a0dd06d2d96ed57",
      "5b42a01461154eec8d34934d0a48bae0",
      "4137ee8591594626b6f75b9723f49f5f",
      "1036cfbde7cb4fffb21c80d098ff904c",
      "684c11e9db9a4812aee5eafe023824f6",
      "0d4bbc45de8b4e59a9ef21c0f6e0edd5",
      "01cac586fb544703aa2e2f8169a437b4",
      "c61aa7743c254753bf4db11e524ad814",
      "8011c9325c524eab81758c1ab2a8d18e",
      "5c325fe8d4a54d6dad8f038b308d14d6",
      "8683c2bd7d794b97a284cac837e480d0",
      "77e018ed309a4456b74e64e6a650ea99",
      "49b26eb3e5404da98cc53437e0f8f892",
      "e2ee0ef5950a489b92f55c3d06377a8e",
      "1863ee9c01cb4867abeed63560cd8068",
      "ce3cc5e35a494a97b53054e03cc744cc",
      "2c8a199971324527a71569090d656b85",
      "81353e7f49a84021ad8aa9b01f6959a8",
      "1b296837e6e441328db77e1f0f6d9394",
      "c299230cb86542ec9a223b60c0a243cd",
      "6b24154d3e8a41c5bba50ff43ff1e54c",
      "410cfef18d574f95b94cdb843119982a",
      "523fafcad07240adaaf960d4192edeb3",
      "094b9881e25a431eacf873942183843b",
      "73e9cb884c37413086b402015e7067f1",
      "94b1f0ba9fd2402dbb8511f2ef731439",
      "597b2ccb625b4eb481cc35ff824bc829",
      "57244f53edbf4c8e9ab082f48b76ffb0",
      "ea13bb94d5ee49049dac32a4bf5c99f7",
      "ae3999a7cb874a9292eb10e105eb2e5a",
      "00b476aa8fb7425cb81cfc177cb6bd2f",
      "0fbba5dcc924434ea1b73c70a3b8cd80",
      "0bd7bf8143254efd80d238ccdc9e5332",
      "30780a30cc4648d7a60b4236e5ae84c1",
      "490cc043f0fe4b92a2e1755ee63e4e2f",
      "7574553f9b76412cbaead4763bc0c3f7",
      "113c00d834ae4d038fca25061e6a25c4",
      "8ff0e5fbb96549fd8153b5961b2f33ba",
      "b09da111ca3841fe9355ac27be1531f7",
      "e7d0e5f3b5114047aac0c3c2529faf75",
      "c91a0a04f7654f349678e09918476a02",
      "8301e821c5c6420f970d4ac0f5c7af87",
      "00fa79130e7f4e6f95990e13c0cd4000",
      "c68e2ebecf2c40a388688a4bec1ee250",
      "68174221423e408bbb040da7b56867b3",
      "503f23d77293498fab9ea35cfb45e3d6",
      "254b329d906147c7816333b293c08fe3",
      "9b6f945bf4dd4379883fe0be34d50076",
      "40a08f7305414ff1865e590e6b80b2f5",
      "48f0953bdcf148cebb54383d7b4dfbd0",
      "4cbf9b390c184a31ae9631931652c5d8",
      "02cd493987bd4defa48f24e3a9dffc6b",
      "ef03c1bf017c4e21b51a4b8f6e181495",
      "39da4ef92c3945a487d1c656f0796dae",
      "5935cb454b23450483949f970cf87e80",
      "2aceccba484e4bf5b5a9178553b6a089",
      "326c51dbef55420a8e1977405925acfb",
      "49e09abf2185457d8d79f7552bfcb8ad",
      "68e6b1f83bd544e3967b993fcfd513cb",
      "d3baf21c85da4347ae5ba9cc950528e5",
      "58bea37954c043f2ba9bd228f80474b4",
      "76f1ab41d721472897a5555e6e0c6a79",
      "051c3acd027b4d648c959d670c6b8233",
      "308bb84e1b22422e800bc688c389b29e",
      "6df79f31f1b0421ca747bc3ebc549cc8",
      "4b703c78cf6b44cea70099946167c7b1",
      "cac3c5e49f974b17bc4c1e57022008dc",
      "c8c83ba4ae74434c861267e0f4fd5002",
      "6e36e66860f944248a14caf9d4b61e4d",
      "d8f2f639c79c4de1aae43c3d114a4134",
      "ab0cf647d938402899b994ae3a534746",
      "c3ab7e5eacd14f1b90f385e654df9047",
      "2c5891a0899541a5891d4f79ec0b2f98",
      "65a8289c53b6432eb5b8c6cfdd50e6dd",
      "d55757d32fbf48008f0f41a9634bd413",
      "5097782103554eadb60e683fec4fa7f4",
      "6427511776bd48fe959cccc1974a84a8",
      "2249a396f9ce4d3b88c73b7600b261d6",
      "66030f9be3254581b7a12a39661695e9",
      "c8f24dcde882416796b5b3b726fcf213",
      "4a95c9a43f5348efb0738438fd031f51",
      "c7aa90315f5e4317acb1d1dcc5ce277e",
      "55917c3a74ed4d699ffa5c58d8e130eb",
      "60069c6c6b20443b8dd35355c29e1b0e",
      "88be943c2bbb466ebc1257eb22d7b78a",
      "6f7a3611c27e4518b081570ae5fa44df",
      "322c76db52424045a422565a5d8513d7",
      "e2ba6adfd3114320b896a3fc1d758b2e",
      "ebbd649200cb46729d2d6b68352eeadf",
      "65f1c6b559c74fb9bdbb125d900ae595",
      "ad1bce52500c4d31af731ba94a7c5992",
      "e46d15837ae54eb5aab5242b24a852fd",
      "5a4344367238461ebccb34171ceedaa5",
      "d0637d3a4eb745dfbbcc4306c493c715",
      "d900e69562e2490a84832087ac031ade",
      "6aea5c502c214790a8e4c08b8f57a26c",
      "7721025fc96245d78bf711f8a6714421",
      "6abd49493faf40278cf05f7a8bddd4f5",
      "45b35c4b7ae140969612bbbeac6ff49f",
      "bbc66bed29db4836b50f229126b764e7",
      "7517593fde434ecdbddd36a038cbb986",
      "3bfbdd9583c24353aa5f56b885776a38",
      "e516d6bb26a242ce879f504a00ab5508",
      "7f79cb7c77bb4fcb9d4cb124ad35abb6",
      "58239e0d05224309bfe6731c5a1acc4d",
      "4925603817a946098239e27894151e2a",
      "c4255aef449b474bb41e8e099898836d",
      "5e0bc931c11f4ae1b17e4479ef0cb940",
      "4d644a50c1134af0a8ef6c0e8124802b",
      "d0c4f2e1e1a945e4ad023ac25646d8a0",
      "6f18d1c905574032a21a71b7fdb2a6b9",
      "d31e19a9492243f0991e07f1e226e5e6",
      "6f9757ed839245b2a0e4651d62dbf053",
      "7461599581884173b8c9f29af722a224",
      "30125485cee14365a60d5cc8279633cc",
      "488d37e12dff4bf599b2c21111c2f4de",
      "49907e8e9a1c431698a6e2804e0f58df",
      "bae77c156b3f45dc99726f482ff73bcd",
      "092e25c8a5d94e298851b4774a58de5f",
      "e20f18dfd6ba48389e9320e26f7e1b8c",
      "93ad56de6aa94d94903d6d67ff983325",
      "c764d303c24c4cd2a6ad8d35427d5422",
      "dbe9fe6c054241fdab28cca6bfb3895b",
      "13cb05f7743b45f98e4abf000c3b52d9",
      "77094ca8b222425f850572c7903a6f9e",
      "1ed0a91ebf9d429eb8d79ffed199eb74",
      "88d9c178c21f4c4bbd4baa559a3be066",
      "15010c9dede146d2a4db8c679035d250",
      "e03c5c368b004a9ea3ec8fe373acc548",
      "90ef5105a00a419d992a235759341e7c",
      "012aca5710174015bd71fcfcda6bec00",
      "fcdfd18b94b7431f8ccbd6610035aa56",
      "b147819440f44cdab2f0ff7c05d5ced0",
      "2d77706c0db840f2a86c74c506294d79",
      "3b19659782684c0faaf41f4df01aad59",
      "2b08fce22e1d4e838b71e2f488c93c5c",
      "aa8621b7861846569a2ba42fd0e272bd",
      "2c17ad94385b4f0ca9a503de68a79522",
      "86f03735ee5346fd9b2184709768099c",
      "9c7c184309e24a9f910e23c84bd29aa0",
      "dbd3fb11b71146b4997790b957a4946d",
      "f7bd0c8a3b8643bf901792c6cd2d46bb",
      "da51cdea0b424a39afa27ecac26764cf",
      "7d2e90916e15496d93a05cb6c9363ca9",
      "dd87739a43b04e969396d65eb3df63e8",
      "3ba067d53b5e46b0a6afbc5ac686fa12",
      "88d535243fc14ba99689a1a42aecdd9a",
      "28332f325578400ba9d50f008ec6e81d",
      "6e73afc0d579403b915f4077bb0bdeee",
      "d18677531817416b8e8224e79e14f3a7",
      "e1289f23a1bf4c2383c142b5e6eea621",
      "18048d28e929427a8e6ffe70974b9f35",
      "0baf999dfc894c48aad3994af8c02d1c",
      "42e4c7044699433b8620b940535d8f24",
      "1db5f0eb85754326acea6a0c929e2088",
      "af11c2a67ead4137a84c30d305b9bf45",
      "1b652ac65ff842d192802d5d831dc00d",
      "33586a68556c461687fafd439884d69c",
      "f9de88fa562146ffb218387ee2f6047c",
      "6573acc6c75d4217918e4ff73db0bf1c",
      "d0a9a43c2afe43959216a9bb922d4b7e",
      "3c5914dbb76b470f87ccf8b1f967aa8b",
      "859cbd90c00e41e1a8c54f981f601fe0",
      "7f6d7c166d7e41b6bbb196727b124c79",
      "179fe010f77b4f50a93334fccd9c01b5",
      "98fc1dda6cb84c2b89945eee5335fb28",
      "6d9732d1c795409c9dd710ada14c9e8c",
      "119614e87f8f49098f0e2c4d7cab9061",
      "7cdbbd6ec72f496f82f6b8074eba6f83",
      "b60b0f447d114a3b9c2b4554a236f08e",
      "f45e05cef0c544088b0b5ede98c0e61d",
      "8c16955f309b4532b6346e14dedffa2e",
      "86037876c5274ecd9837da88b94618cf",
      "8a7e2da9cc2d4c50a3b1b4e76f6c0ffc",
      "7e11623788d2441c8e66ffff4ec5c0b7",
      "91acb86677934be3b3927e65c956e9c6",
      "42d9cb3b198641edb968691c22cac2e3",
      "f6f7d4ce004c4432aa250bb7a6d55ec3",
      "2844c3c4b54b4d83a6e4de3e281cda60",
      "224bd0d3c50e412da13679001fadba22",
      "c522d50026b84747b582e1d8af4e7047",
      "098e8b49b7534744b4ef622dbcaa340c",
      "2c1a015734bb4f188835b33b486927ee",
      "8ef412ed4c2a417cb682a7f42edc9e12",
      "4475d34f63af4620a0ef2c93da68b4a6",
      "33721488dd5a49bb9877ca59e3594bef"
     ]
    },
    "executionInfo": {
     "elapsed": 173980,
     "status": "ok",
     "timestamp": 1758482567275,
     "user": {
      "displayName": "Kossiwa Ines Deh",
      "userId": "09014473696885298182"
     },
     "user_tz": -120
    },
    "id": "H0Z6dyCjtxRb",
    "outputId": "bf82f391-b9c0-438d-cc19-4c20a7b94bdb"
   },
   "outputs": [],
   "source": [
    "# --- Ausführen & Speichern ---\n",
    "all_results, all_samples, phase_tables = [], [], []\n",
    "for model_id, alias_long, alias_short in MODELS:\n",
    "    print(f\"\\n### Starte Baseline (per Phase): {alias_long}\")\n",
    "    res, ex, phase_df = run_baseline(model_id, alias_long, alias_short)\n",
    "    all_results.append(asdict(res)); all_samples.append((alias_short, ex)); phase_tables.append(phase_df)\n",
    "\n",
    "df = pd.DataFrame(all_results).sort_values(\"alias\").reset_index(drop=True)\n",
    "# Emissionsfaktor (kg/kWh) auf Gesamtebene\n",
    "df[\"kg_per_kwh\"] = (df[\"co2_kg\"] / df[\"energy_kwh\"]).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(\"\\nGesamt (summiert über Phasen):\")\n",
    "print(df[[\"model_id\",\"alias\",\"precision\",\"time_s\",\"energy_kwh\",\"co2_kg\",\"kg_per_kwh\",\n",
    "          \"tokens_out\",\"ppl\",\"bleu\",\"ram_GB\",\"vram_alloc_GB\",\"vram_reserved_GB\",\"notes\"]])\n",
    "\n",
    "df_phase = pd.concat(phase_tables, ignore_index=True)\n",
    "print(\"\\nPer-Phase Übersicht (mit abgeleiteten Kennzahlen):\")\n",
    "print(df_phase[[\"alias\",\"phase\",\"time_s\",\"energy_kwh\",\"co2_kg\",\"kg_per_kwh\",\n",
    "                \"tokens_out\",\"ppl\",\"bleu\",\"tokens_s\",\"wh_per_token\",\"s_per_example\",\"wh_per_example\"]])\n",
    "\n",
    "out_dir = project_path\n",
    "df.to_csv(os.path.join(out_dir, \"baseline_bloom_dual_results.csv\"), index=False)\n",
    "df_phase.to_csv(os.path.join(out_dir, \"baseline_bloom_dual_per_phase.csv\"), index=False)\n",
    "print(\"Gespeichert (gesamt):\", os.path.join(out_dir, \"baseline_bloom_dual_results.csv\"))\n",
    "print(\"Gespeichert (per Phase):\", os.path.join(out_dir, \"baseline_bloom_dual_per_phase.csv\"))\n",
    "\n",
    "for alias_short, ex in all_samples:\n",
    "    samples_path = os.path.join(out_dir, f\"baseline_samples_{alias_short}.txt\")\n",
    "    with open(samples_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, txt in enumerate(ex, 1):\n",
    "            f.write(f\"--- Beispiel {i} ({alias_short}) ---\\n{txt}\\n\\n\")\n",
    "    print(\"Beispiele gespeichert:\", samples_path)\n",
    "\n",
    "print(\"\\nEmissions-Logs (pro Phase):\")\n",
    "for _, _, alias_short in MODELS:\n",
    "    print(f\" - bloom_{alias_short}_gen.csv\")\n",
    "    print(f\" - bloom_{alias_short}_ppl.csv\")\n",
    "    print(f\" - bloom_{alias_short}_bleu.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPRYOPV20nQrW6BXILZwUuV",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
