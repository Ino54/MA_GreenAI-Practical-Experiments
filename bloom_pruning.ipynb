{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ino54/MA_GreenAI-Practical-Experiments/blob/main/bloom_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BxYMjHZFVy1",
        "outputId": "aa469c52-7ca3-4fc3-93bf-65f72eeea56a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "# ---------- Requirements ----------\n",
        "%%writefile requirements.txt\n",
        "transformers\n",
        "accelerate\n",
        "bitsandbytes\n",
        "datasets\n",
        "evaluate\n",
        "sacrebleu\n",
        "codecarbon>=2.5,<3\n",
        "pynvml>=11.5.0\n",
        "psutil\n",
        "numpy\n",
        "pandas\n",
        "huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -r requirements.txt\n",
        "# häufige Konflikte still entfernen\n",
        "!pip uninstall -y -q google-genai firebase-admin || true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c9gZzAsGC3l",
        "outputId": "9d4be090-8d8c-43c2-80d6-a3177d01c963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.6/517.6 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-genai 1.38.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Hugging Face Login via Colab-Secret ----------\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(hf_token); print(\"Hugging Face Login erfolgreich!\")\n",
        "else:\n",
        "    print(\"WARNUNG: Kein HF_TOKEN gefunden\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvgmgaRTGGg2",
        "outputId": "e4313d6d-5710-40b1-cb6a-03f0379bf913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face Login erfolgreich!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Drive mounten & Zielordner prüfen ----------\n",
        "import os, shutil, time, pathlib, platform, gc, re, math, warnings, inspect\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from google.colab import drive\n",
        "MOUNTPOINT = \"/content/drive\"\n",
        "already = os.path.isdir(os.path.join(MOUNTPOINT, \"MyDrive\"))\n",
        "if not already and os.path.isdir(MOUNTPOINT) and os.listdir(MOUNTPOINT):\n",
        "    backup = f\"/content/drive_stale_{int(time.time())}\"\n",
        "    shutil.move(MOUNTPOINT, backup)\n",
        "    os.makedirs(MOUNTPOINT, exist_ok=True)\n",
        "drive.mount(MOUNTPOINT, force_remount=(not already))\n",
        "\n",
        "work_dir = \"/content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_pruning\"\n",
        "if not os.path.isdir(work_dir):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Zielordner fehlt: {work_dir}\\n\"\n",
        "        \"Bitte diesen Ordner manuell in Google Drive anlegen und das Notebook erneut starten.\"\n",
        "    )\n",
        "os.chdir(work_dir)\n",
        "project_dir = work_dir\n",
        "print(\"Arbeitsordner:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PShQCVx_GRq4",
        "outputId": "79fcd410-8a64-49eb-f32a-73856955f1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Arbeitsordner: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_pruning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Imports & Device ----------\n",
        "import numpy as np, pandas as pd\n",
        "import torch, psutil\n",
        "from contextlib import nullcontext\n",
        "from types import SimpleNamespace\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed, BitsAndBytesConfig\n",
        "from codecarbon import EmissionsTracker\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn as nn\n",
        "\n",
        "# VRAM-Fragmentation entschärfen\n",
        "import os as _os\n",
        "_os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_total_gb = torch.cuda.get_device_properties(0).total_memory/(1024**3)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "else:\n",
        "    gpu_name = \"CPU\"; vram_total_gb = 0.0\n",
        "print(f\"Device: {device} | GPU: {gpu_name} | VRAM={vram_total_gb:.1f} GB | Torch {torch.__version__} | Py {platform.python_version()}\")\n",
        "\n",
        "RESULT_BASENAME = \"bloom_pruning\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyu-6c7rGeAc",
        "outputId": "2db59692-ad07-4ede-ad94-127cb30ba8f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | GPU: NVIDIA A100-SXM4-80GB | VRAM=79.3 GB | Torch 2.8.0+cu126 | Py 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- CodeCarbon-Helfer ----------\n",
        "def _cleanup_cc_locks():\n",
        "    for p in [\n",
        "        \"/tmp/.codecarbon.lock\",\n",
        "        _os.path.expanduser(\"~/.codecarbon/codecarbon.lock\"),\n",
        "        \"/content/.codecarbon/codecarbon.lock\",\n",
        "    ]:\n",
        "        try:\n",
        "            if _os.path.exists(p):\n",
        "                _os.remove(p)\n",
        "                print(f\"[CodeCarbon] Lock entfernt: {p}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "_os.environ[\"CODECARBON_CACHE_DIR\"] = f\"/content/.codecarbon_cache_prune_{int(time.time())}\"\n",
        "\n",
        "def _cc_supported_kwargs():\n",
        "    base = dict(log_level=\"error\", output_dir=\".\", measure_power_secs=1, tracking_mode=\"process\")\n",
        "    try:\n",
        "        params = inspect.signature(EmissionsTracker.__init__).parameters\n",
        "        if \"allow_multiple_runs\" in params: base[\"allow_multiple_runs\"] = True\n",
        "        if \"cloud_provider\" in params:      base[\"cloud_provider\"] = \"google\"\n",
        "        if \"cloud_region\" in params:        base[\"cloud_region\"]   = \"europe-west10\"\n",
        "        if \"country_iso_code\" in params:    base[\"country_iso_code\"] = \"DEU\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    return base\n",
        "\n",
        "def make_trk(name, out):\n",
        "    _cleanup_cc_locks()\n",
        "    return EmissionsTracker(project_name=name, output_file=out, **_cc_supported_kwargs())\n",
        "\n",
        "def start(tr):\n",
        "    try:\n",
        "        tr.start(); return True\n",
        "    except Exception:\n",
        "        _cleanup_cc_locks()\n",
        "        try:\n",
        "            tr.start(); return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "def stop(tr, st):\n",
        "    if not st:\n",
        "        return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "    try:\n",
        "        return tr.stop()\n",
        "    except Exception:\n",
        "        return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "\n",
        "def unpack(em):\n",
        "    if hasattr(em, \"energy_consumed\") and hasattr(em, \"emissions\"):\n",
        "        try: return float(em.energy_consumed), float(em.emissions)\n",
        "        except: return 0.0, 0.0\n",
        "    if isinstance(em, dict):\n",
        "        e = em.get(\"energy_consumed\", 0.0)\n",
        "        c = em.get(\"emissions\", em.get(\"emissions_kg\", 0.0))\n",
        "        try: return float(e), float(c)\n",
        "        except: return 0.0, 0.0\n",
        "    try: return 0.0, float(em)\n",
        "    except: return 0.0, 0.0\n",
        "\n",
        "def read_energy(path):\n",
        "    try:\n",
        "        if not _os.path.exists(path): return 0.0\n",
        "        df = pd.read_csv(path)\n",
        "        for c in [\"energy_consumed\",\"energy_consumed_kwh\",\"energy_consumed (kWh)\",\"energy (kWh)\"]:\n",
        "            if c in df.columns: return float(df[c].iloc[-1])\n",
        "        for c in df.columns:\n",
        "            if \"energy\" in c.lower() and \"kwh\" in c.lower():\n",
        "                return float(df[c].iloc[-1])\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def measure(phase, fn, prefix):\n",
        "    logfile = _os.path.join(project_dir, f\"{prefix}_{phase}.csv\")\n",
        "    tr = make_trk(f\"{prefix}_{phase}\", logfile)\n",
        "    import time as _t\n",
        "    st = start(tr); t0 = _t.time(); res = fn(); t1 = _t.time()\n",
        "    em = stop(tr, st); ekwh, co2 = unpack(em)\n",
        "    if ekwh == 0.0:\n",
        "        ek = read_energy(logfile)\n",
        "        if ek: ekwh = ek\n",
        "    return {\"phase\": phase, \"time_s\": t1-t0, \"energy_kwh\": ekwh, \"co2_kg\": co2}, res"
      ],
      "metadata": {
        "id": "zDBsFLaCGrQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Eval-Config ----------\n",
        "MODELS = [\n",
        "    (\"bigscience/bloom-560m\", \"bloom560m\", \"b560\"),\n",
        "    (\"bigscience/bloom-3b\",   \"bloom3b\",   \"b3b\"),\n",
        "]\n",
        "EVAL = {\n",
        "    \"max_new_tokens\": 32,\n",
        "    \"ppl\":  {\"name\":\"wikitext\",\"config\":\"wikitext-2-raw-v1\",\"split\":\"test[:1%]\"},\n",
        "    \"bleu\": {\"name\":\"wmt14\",\"config\":\"de-en\",\"split\":\"test[:32]\"},\n",
        "}\n",
        "PROMPTS = [\n",
        "    \"Schreibe einen kurzen Absatz über nachhaltige KI.\",\n",
        "    \"Erkläre in einfachen Worten, was Quantisierung in neuronalen Netzen ist.\",\n",
        "    \"Nenne drei Vorteile von Mixture-of-Experts-Modellen.\"\n",
        "]\n",
        "bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# Pruning-Parameter (20%)\n",
        "PRUNE_AMOUNT   = 0.20\n",
        "PRUNE_TARGETS  = (\"query_key_value\", \"dense_h_to_4h\", \"dense_4h_to_h\")\n",
        "\n",
        "# Eval-Schutz\n",
        "MAX_LEN_CAP = 256  # ggf. 192/128 für noch weniger VRAM\n",
        "\n",
        "def autocast_ctx():\n",
        "    return torch.autocast(device_type=\"cuda\", dtype=torch.float16) if torch.cuda.is_available() else nullcontext()\n",
        "\n",
        "def safe_max_len(tok, model, fallback=2048, upper=100000):\n",
        "    cand = getattr(tok, \"model_max_length\", None)\n",
        "    if isinstance(cand, int) and 0 < cand < upper: return cand\n",
        "    cand = getattr(getattr(model, \"config\", None), \"max_position_embeddings\", None)\n",
        "    if isinstance(cand, int) and 0 < cand < upper: return cand\n",
        "    return fallback\n",
        "\n",
        "def capture_memory():\n",
        "    ram = psutil.Process().memory_info().rss\n",
        "    valloc = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "    vres  = torch.cuda.memory_reserved()  if torch.cuda.is_available() else 0\n",
        "    return ram, valloc, vres\n",
        "\n",
        "def bytes_to_gb(b): return float(b)/(1024**3)\n",
        "\n",
        "def get_model_device(model):\n",
        "    try:\n",
        "        return next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------- CPU-Loader fürs Pruning ----------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "def load_for_pruning_cpu(model_id: str):\n",
        "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    tok.padding_side = \"left\"; tok.pad_token = tok.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map={\"\": \"cpu\"},   # alles auf CPU\n",
        "        dtype=torch.float32,\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "    model.eval()\n",
        "    return tok, model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ba7f723b4f014675b43da8e40f90e1e7",
            "a587ef943cca4adb9ee02c176e50682e",
            "9f9f124604c84db7b72ecbbd9ce6200d",
            "604a2426e64846339a8926719523218c",
            "e91f609b6c024a7a8aeddcbd2d271278",
            "9d5e96b3d9c642eda6fc03549ce6515b",
            "7d49dcd5f72f4dff914f44416855e431",
            "6e6e75a815b04653ac5e864f90402668",
            "66c364f274424eea9b04450edbf92878",
            "ed00d8ae299544a78ae21db26216cc9d",
            "cb52753b6b5b4a6193e0182f19dfea30"
          ]
        },
        "id": "u0Ut5YCDGxS1",
        "outputId": "bb8200df-7b64-47f1-dbbb-656848d8c3b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba7f723b4f014675b43da8e40f90e1e7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Globales Pruning mit OOM-Fallback (layer-weise) ----------\n",
        "def global_magnitude_prune(model, amount: float):\n",
        "    params = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear) and any(tag in name for tag in PRUNE_TARGETS):\n",
        "            params.append((module, \"weight\"))\n",
        "    if not params:\n",
        "        print(\"[Pruning] Keine passenden Linear-Layer gefunden.\"); return 0.0\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            prune.global_unstructured(params, pruning_method=prune.L1Unstructured, amount=amount)\n",
        "        fallback_used = False\n",
        "    except Exception as e:\n",
        "        print(\"[Pruning] Global OOM -> layer-weise:\", repr(e))\n",
        "        with torch.no_grad():\n",
        "            for module, name in params:\n",
        "                prune.l1_unstructured(module, name, amount=amount)\n",
        "        fallback_used = True\n",
        "\n",
        "    total, zeros = 0, 0\n",
        "    for module, _ in params:\n",
        "        mask = dict(module.named_buffers()).get(\"weight_mask\", None)\n",
        "        w    = dict(module.named_parameters()).get(\"weight\", None)\n",
        "        if mask is not None and w is not None:\n",
        "            zeros += int((w * mask == 0).sum().item())\n",
        "            total += w.numel()\n",
        "        try: prune.remove(module, \"weight\")\n",
        "        except: pass\n",
        "    sparsity = (zeros/total) if total else 0.0\n",
        "    print(f\"[Pruning] Modus: {'layer-weise' if fallback_used else 'global'} | Sparsity≈{sparsity:.3f}\")\n",
        "    return sparsity"
      ],
      "metadata": {
        "id": "QU4FrmyTGz0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Evaluation ----------\n",
        "def simple_generate(model, tok, prompts, max_new_tokens=32):\n",
        "    dev = get_model_device(model)\n",
        "    total_gen_tokens, texts = 0, []\n",
        "    ml = min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    for p in prompts:\n",
        "        enc = tok(p, return_tensors=\"pt\", truncation=True, max_length=ml).to(dev)\n",
        "        room = ml - enc[\"input_ids\"].shape[1]\n",
        "        cur_new = max(1, min(max_new_tokens, int(room)))\n",
        "        with torch.no_grad(), autocast_ctx():\n",
        "            out = model.generate(**enc, max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "        gen_len = int(out.shape[1] - enc[\"input_ids\"].shape[1])\n",
        "        total_gen_tokens += gen_len\n",
        "        texts.append(tok.decode(out[0], skip_special_tokens=True))\n",
        "    return texts, total_gen_tokens\n",
        "\n",
        "def eval_perplexity(model, tok, ds_cfg):\n",
        "    dev = get_model_device(model)\n",
        "    ds = load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    ml = min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for t in ds[\"text\"]:\n",
        "            if not isinstance(t, str) or len(t.strip()) < 4: continue\n",
        "            enc = tok(t, return_tensors=\"pt\", truncation=True, max_length=ml).to(dev)\n",
        "            with autocast_ctx(): out = model(enc[\"input_ids\"], labels=enc[\"input_ids\"])\n",
        "            losses.append(float(out.loss.detach().cpu()))\n",
        "    return math.exp(np.mean(losses)) if losses else None\n",
        "\n",
        "def eval_bleu_llm(model, tok, ds_cfg, max_new_tokens=32):\n",
        "    dev = get_model_device(model)\n",
        "    ds = load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    ml = min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    preds, refs = [], []\n",
        "    with torch.no_grad():\n",
        "        for ex in ds:\n",
        "            de, en = ex[\"translation\"][\"de\"], ex[\"translation\"][\"en\"]\n",
        "            prompt = f\"Translate to English:\\nGerman: {de}\\nEnglish:\"\n",
        "            enc = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=ml).to(dev)\n",
        "            room = ml - enc[\"input_ids\"].shape[1]\n",
        "            cur_new = max(1, min(max_new_tokens, int(room)))\n",
        "            with autocast_ctx():\n",
        "                out = model.generate(**enc, max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "            gen = tok.decode(out[0], skip_special_tokens=True)\n",
        "            hyp = gen.split(\"English:\")[-1].strip().split(\"\\n\")[0].strip() or gen.strip()\n",
        "            preds.append(hyp); refs.append([en])\n",
        "    return float(bleu_metric.compute(predictions=preds, references=refs)[\"score\"])"
      ],
      "metadata": {
        "id": "pXO6VqN6G9gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Run pro Modell ----------\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "def run_once(model_id: str, alias: str):\n",
        "    print(f\"\\n### Starte Pruning: {alias} ({model_id})\")\n",
        "\n",
        "    # 1) CPU laden & PRUNE (kein GPU-VRAM)\n",
        "    tok, model_cpu = load_for_pruning_cpu(model_id)\n",
        "\n",
        "    def _do_prune():\n",
        "        with torch.no_grad():\n",
        "            sp = global_magnitude_prune(model_cpu, PRUNE_AMOUNT)  # 20%\n",
        "        return sp\n",
        "    m_prune, sparsity = measure(\"prune\", _do_prune, f\"{RESULT_BASENAME}_{alias}\")\n",
        "\n",
        "    # Geprunte Gewichte speichern & CPU-Modell freigeben\n",
        "    tmp_dir = os.path.join(project_dir, f\"{RESULT_BASENAME}_pruned_ckpt_{alias}\")\n",
        "    model_cpu.save_pretrained(tmp_dir)\n",
        "    del model_cpu; gc.collect()\n",
        "\n",
        "    # 2) Für EVAL in 8-bit + CPU-Offload neu laden (GPU-sparend)\n",
        "    bnb8 = BitsAndBytesConfig(load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        tmp_dir, device_map=\"auto\", quantization_config=bnb8, low_cpu_mem_usage=True,\n",
        "    )\n",
        "    model.eval()\n",
        "\n",
        "    # 3) GEN\n",
        "    m_gen, (samples, n_tok) = measure(\"gen\",\n",
        "        lambda: simple_generate(model, tok, PROMPTS, EVAL[\"max_new_tokens\"]),\n",
        "        f\"{RESULT_BASENAME}_{alias}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    # 4) PPL\n",
        "    m_ppl, ppl = measure(\"ppl\",\n",
        "        lambda: eval_perplexity(model, tok, EVAL[\"ppl\"]),\n",
        "        f\"{RESULT_BASENAME}_{alias}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    # 5) BLEU\n",
        "    m_bleu, bleu = measure(\"bleu\",\n",
        "        lambda: eval_bleu_llm(model, tok, EVAL[\"bleu\"], EVAL[\"max_new_tokens\"]),\n",
        "        f\"{RESULT_BASENAME}_{alias}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    # Gesamt\n",
        "    total_time   = m_prune[\"time_s\"] + m_gen[\"time_s\"] + m_ppl[\"time_s\"] + m_bleu[\"time_s\"]\n",
        "    total_energy = m_prune[\"energy_kwh\"] + m_gen[\"energy_kwh\"] + m_ppl[\"energy_kwh\"] + m_bleu[\"energy_kwh\"]\n",
        "    total_co2    = m_prune[\"co2_kg\"]     + m_gen[\"co2_kg\"]     + m_ppl[\"co2_kg\"]     + m_bleu[\"co2_kg\"]\n",
        "\n",
        "    ram, valloc, vres = capture_memory()\n",
        "\n",
        "    per_phase = [m_prune, m_gen, m_ppl, m_bleu]\n",
        "    for p in per_phase:\n",
        "        p[\"alias\"] = alias\n",
        "        p[\"model_id\"] = model_id\n",
        "\n",
        "    row = dict(\n",
        "        model_id=model_id, alias=alias, precision=\"8bit_eval_after_cpu_prune\",\n",
        "        sparsity=round(float(sparsity), 4),\n",
        "        time_s=total_time, energy_kwh=total_energy, co2_kg=total_co2,\n",
        "        kg_per_kwh=(total_co2/total_energy) if total_energy else None,\n",
        "        tokens_out=int(n_tok), ppl=ppl, bleu=bleu,\n",
        "        ram_GB=bytes_to_gb(ram), vram_alloc_GB=bytes_to_gb(valloc), vram_reserved_GB=bytes_to_gb(vres),\n",
        "        notes=f\"Prune@CPU; Eval 8-bit offload | GPU={gpu_name}, VRAM={vram_total_gb:.1f} GB | prune={PRUNE_AMOUNT:.2f} | MAX_LEN_CAP={MAX_LEN_CAP}\"\n",
        "    )\n",
        "\n",
        "    # Samples-Datei\n",
        "    samples_path = os.path.join(project_dir, f\"{RESULT_BASENAME}_samples_{alias}.txt\")\n",
        "    with open(samples_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, txt in enumerate(samples, 1):\n",
        "            f.write(f\"--- Beispiel {i} ({alias}) ---\\n{txt}\\n\\n\")\n",
        "    print(\"Beispiele gespeichert:\", samples_path)\n",
        "\n",
        "    return row, per_phase"
      ],
      "metadata": {
        "id": "260ZMh2sG_Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Ausführen für beide Modelle & Speichern ----------\n",
        "all_rows, all_phases = [], []\n",
        "for mid, long_name, alias in MODELS:\n",
        "    row, phases = run_once(mid, alias)\n",
        "    all_rows.append(row)\n",
        "    all_phases.extend(phases)\n",
        "\n",
        "df  = pd.DataFrame(all_rows).sort_values(\"alias\").reset_index(drop=True)\n",
        "dfp = pd.DataFrame(all_phases)\n",
        "dfp[\"wh_total\"] = dfp[\"energy_kwh\"] * 1000.0\n",
        "\n",
        "out_csv = os.path.join(project_dir, f\"{RESULT_BASENAME}_results.csv\")\n",
        "df.to_csv(out_csv, index=False)\n",
        "\n",
        "out_phase_csv = os.path.join(project_dir, f\"{RESULT_BASENAME}_per_phase.csv\")\n",
        "dfp.to_csv(out_phase_csv, index=False)\n",
        "\n",
        "print(\"\\nErgebnisse (gesamt):\")\n",
        "print(df)\n",
        "print(\"Gespeichert (gesamt):\", out_csv)\n",
        "\n",
        "print(\"\\nPer-Phase Übersicht:\")\n",
        "print(dfp[[\"alias\",\"phase\",\"time_s\",\"energy_kwh\",\"co2_kg\"]])\n",
        "print(\"Gespeichert (per Phase):\", out_phase_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b23cf86613334321bcb1e412a8bce35c",
            "e269e9d0cd7347358b5a4f776bd01e61",
            "fb629e3ebc5b44be813924358b18a945",
            "7c33b79cf6c7471d9cc356b8e4ce63bf",
            "29023ca3f38b4a5aa76cbcf93c843c9f",
            "eaa23162c2404e6f9deaa4348c536e26",
            "1d6dc3f80eeb480f9a07377d4e014e49",
            "2b49ed913aae481986f5d2c6001852eb",
            "c35efce3eb78427a96ae2c02af1df520",
            "779bbd0788e64ceca18e988d5aaa3d2c",
            "00b4f3efa28e4716a48fb2cae936f021",
            "5d4803eca0d94c66b58eebe3e14851d3",
            "b1ae5b8a896f4284b8c763991de28ca3",
            "438d8b0af72441f696acf72c0abc3332",
            "4a0d233a2a9443a38044d2a6559bc4b3",
            "7204b6c50b2e40d497d9fc3df32bb47f",
            "09502be7e6174d07bf39d11fd7cc2489",
            "db96a62a263f4e49bb8f5b5797dd395d",
            "4e5a258d7f204b53986961e2cedde44c",
            "f616eda1d2b4428c8dfdd67e0b2ed25e",
            "e1e2ca5b852a4b1cbca5cd33e9275fb8",
            "4929334aae504c68883d08568d47a69a",
            "db2a4fd1ed1e4da38d92b8e6ffe081dd",
            "ba783b980002484e8764448cc46ed206",
            "29f62514fcce4a2da410d2bac4dafeeb",
            "9dcb135fbbf04d119dbf84111b4272dd",
            "fd039a8a97714b2fbcaa7af4374473bb",
            "793bdfd7424e42c5add017911bd5b9f8",
            "6adebb5016b24bccace425efbb2b40fc",
            "e2b78dfd421444eba2fbe2b7b0f5a0e1",
            "eefabc4732d64d64ac34e3783578f58b",
            "ba3959d0c38c4168af5af9a4b383d467",
            "62953f1cc9434f77b3dd3992a4bb082c",
            "a7a07bf2130c4812b84ec58c4e083144",
            "4d954c7b4dbf4a29b8bb25e91c2ceb41",
            "7c523cd0a2db4e49a8d345dfd370e277",
            "1f2ca387fa8549b095dd540e06d8eb5c",
            "b971e571bf214666a2f127a7f43686d2",
            "97ef228477b94bd49e39423d5056a121",
            "8748498681b14ae093e102ff0bd32add",
            "6566b36102974fd7bcb6f4f8c3ec0c73",
            "92f3f485b19c46f997b452f71fdb912a",
            "79a850f5b99d4514940394845e4240b3",
            "f938d67156ca41c496d06cf2b80d234c",
            "0972f6e09a4f412e9221780d0bafeb0e",
            "6b13da04b4224c24b560c8e14665723b",
            "734dbd187450406ca512ad5edf0ca777",
            "357b0e5935f54b7eb5057dffed4c1e4d",
            "95169aae92264c1ca294872022bc2d00",
            "e990418beda643a291f647c48c65788f",
            "ea455c85ac3c48108717bc166c0abb50",
            "55d96e67376e406da78001836bfa5df7",
            "e20dc8623e22433cbbb2241d11fc2d6d",
            "392fc300b2854f7486daee1973ad0633",
            "274c113a84924fb797b9abe84d50c4d0",
            "16e7b988d8d24bbaac71b6f0823d97d2",
            "e35aa393e1ac4344827b78a68c8ea461",
            "b950537a03f245deb17d9d4ea697dd5f",
            "2e949b7c948a4c1790b14389f533d8dc",
            "3e8f49aac82440ed90f96fd65b3705cc",
            "8efd82749d154b40b3b13ae11a574ca8",
            "6c49f2557d86407d8c1e16f40b688425",
            "643f3ccf90d64193bec4ee28fc030273",
            "dc137d301feb4793a05c02b592a43e83",
            "f19d040e1c3a40cc81a8ffca5ce4180b",
            "f595ec6cc4444502a1d15f64d770817f",
            "1c5079757f23490db21da87cb357df21",
            "deaf8448c5c045f2ae37161c0f2dc450",
            "0d1f09b49c3e4141948e875ce97cf53e",
            "213542cc379a42c9943274e64c515a3c",
            "4c43c4d4294746c399bd67a826284994",
            "8bfffe12c334428bb0b9cbfbc8b16411",
            "ab3995df639c48578f3865032d917dba",
            "a267b89f8a414a95a9a377ffb41c3e50",
            "c0ada7adee2048709e7d531a6960a3e5",
            "a4a5adf66cbe488aa4d2e0603b524000",
            "b1920cb1fba6465c8dd5d8ea6613a30b",
            "348492313c0d48d4b95d2666bffa0b5b",
            "009e0b6f477a4b35bf0d43c094d14918",
            "91c333c1d61648c59f42000f5f9ea358",
            "43f573bf54c249278caa37869e2000b3",
            "6954a9506caa42d99a9330d93fb51b98",
            "661bf632548246a5a5a400820d50bef0",
            "d3c1f101b3d34f9baf6469048291a12f",
            "7f2e0593539e4256b1ee4f6a73aa580c",
            "ecfafc686c8a41d0870fd559b5e110dc",
            "9270d28f8e1746ddbacaaa2d77ab8429",
            "48a9f50cbd354ee883c73c79af534bcb",
            "bf837f92948e46bdbd4ca70d36c548d9",
            "4b5d680b102f4fbb927a8698728d3d09",
            "22849a3d65594be6ab4d894bb8d3fac1",
            "460ad84125194aec91087e95fd8ec533",
            "8780336a950c4ac7905ba6bbab326765",
            "84ffb3f687e24792af7897a4355648a2",
            "0c81fe04023947ab8f4e65043319c107",
            "2bbf6e27aaa24609b675c9436578b769",
            "743962c341d8453cb16b7148ddf2c9d1",
            "9db62e58950e4645ac0c4bcc6558cf04",
            "b0fb985d79c247249298d0ec91ef129a",
            "cd53c4ce0e264900bfb21b1e6722a7b4",
            "171dbfabb7d541f1bea4cdebaf9a1806",
            "5866925c54eb4f3b9154cf754a44f6a3",
            "213c743bc3134eaf9c286b452c4eb25e",
            "591b31cb74e74b539301955b44d2ce29",
            "ce6ca3438cb64d6587cb52a029a481d7",
            "c3095d85122d4d45b76c70c7efc8109e",
            "96624a5f8af4444e849f83fcaa4d7f5d",
            "0d756824dd5e448bbaa7a772c58bb33a",
            "3d44d337e91741308f02d212664c09df",
            "b949e5f7468a41bcb4e78a12a2c02af9",
            "5941f9f7ac654ceebc3ed543ed2c2056",
            "e9a83716b4b74f96aca69d4d2e2b417b",
            "512e3a7630db4157882dae19449a56a9",
            "0d11c0fbfb574e639a82ef6b226763b6",
            "a21aefa43cc447079f2e5425e37735b4",
            "0f96d0f406c242039203fa3e0cf97d1b",
            "7280e043720542ed87339050bf30f9fa",
            "8ec8bf63733241658fb398653c31c98e",
            "aed675bced2e460bbfbbb506ca2f7dbf",
            "393e780997ee49639f04533a31ec42a1",
            "98ae7ec4d46d4b88a2d3f778619c4ad0",
            "9bdf54a8cd6742b895b0424fd522698b",
            "ccd0dc465dfa4171a2b08bebde94c7e8",
            "2df2e68dbe504550b63a59db454eb10d",
            "54dff617db8a4c788a73c27ce126a2bc",
            "8def73bef26449dab0e76ec2e9faeb6a",
            "f790a714508f40cb84d5d1d053c48503",
            "f9c3b74a8b0d46a3883469dc9261fc60",
            "824af1623eaa46a9a1bd8746de7dde2b",
            "30a789e368aa460b95d9c38371717eab",
            "40f8160875fe4f94bcc8729addfba5b1",
            "01832eb445be4dd3ae6ce294a96a52bd",
            "fd7de2c5bc054537957e619758c239a6",
            "a8ac96afa2884229820fc7145d501cef",
            "e59484cf3b504cf78098d0ad6afbb3a8",
            "e2663cc52dc44972b3b3ad7b27eaf560",
            "13f01defa28f4e38a2b7908ab49694c1",
            "e8b1c15c90174f1bb423b19a7465c5b8",
            "193572ddc6f94f8a8a6941bfc0abe08e",
            "b34e81167e734ae6b577a65f06df8c90",
            "20f7c85e4355419c81977792fab062ca",
            "42d945c3cb5e49258333e3e9cf2d5cf5",
            "ada9448d2efb4808be24e7b3cd6bea86",
            "cd3520f290bc47fcbe37d918218e342f",
            "01533236421e445f8a7fcab944c4618c",
            "33ad0588a3084504bfbd692fe6244d29",
            "3e671f19327642cf84502e905e0371e8",
            "9b9816034f004bc8bdd50e6870c409f0",
            "e59aed0da6594a2683836fea25d204cf",
            "9fbbea22a1534dc4ba58be7bae12f2ce",
            "d644edf5d38a48428ca214b6337d6bb1",
            "dda9b8c96f524adeabcda01b67e10a3f",
            "364cbb9693c24b5ba3a05f340d00a733",
            "1707c3b8bfa243479c455c6f8b7be0d3",
            "ed0827f2b97c4b83b9dc4d4e57f2b057",
            "38183e913f974103af653d050e98cefc",
            "1b357d2627c0497d861a286482a96abc",
            "7c4ecd82fab2473ea4e6b7d89c64d059",
            "185fe9a40dc046268c81346e1ec9e39e",
            "2f7c7eaccab64bf7b859633238fc0061",
            "ded2465fbf004070995916fc41821632",
            "96b22084efb44a3484e8ab3f70e8b660",
            "a1e2eed52d9b45c1829110214ace04bf",
            "fcaef521e7224498a2d6655f9989a628",
            "b63d9abdc31f43dfa8db778816a6790b",
            "d474589d23f1427e8dd58d4c4ef08d86",
            "b810baa8f97b4468b35f03d4db1cc91e",
            "463eb0b00f8d4c51aa034bed89bc0e34",
            "e5474ee4f6a34913b04c330c0b87a11a",
            "e044f8de9b8d4fbd8576e2deeca43114",
            "ccfa5e985a2b4c6287d20831652804d5",
            "33d9c85b4b0a46fe8baa93a448d0649e",
            "8a8654dcdc1f43d5b729eb26275cac68",
            "14780e5d674943119a468d4a600da2b6",
            "8b36658f203d49fcb48171084e8ebfb7",
            "0bedc9a18614474ebec36a5e089a544d",
            "76f5d2cad6e347ea8f659ced6716a1be",
            "42c775d4e4c744a1baeadf56e7b08e66",
            "0fc4d6bd06ca428d9d8c49950aacdeda",
            "c27298e9e39f446dbc4eed3871f33f6b",
            "4e108fbc8d614834a00d81a4ecc044d6",
            "dceef4f578ef49b685ad3b08ac2bbf1a",
            "a1015ad427de44648365e235d290b583",
            "40d0b638100b4a8095bf45465f7a325c",
            "fc56af2881064bd9a7c24ed143325ec4",
            "b73e6a62c1384791acb121224d199332",
            "87122befc8bf4655958b4c871edb8354",
            "c89af6f8f2a243a7bdcf34cacefa79e3",
            "3c57983fc066401b883a9dcec423ad23",
            "60d56543a4ab46349024ee5776c475cb",
            "3a8f87d1adac4213a82fa742b4f21ed2",
            "f2174a1e30af47e0be7def85762f087d",
            "aaf9bab7102e4f63821223791c7b0f1b",
            "7b95d7340f8f4caa988cef000d7ff9db",
            "d045b8acc0e84f3d93d27676774605f3",
            "1d6a55b698e1498b980dc521f88b50f6",
            "eb545aa028c04f1ab2c1e70aa7c973cf",
            "17c3ccca6e2842c28b7e7f996eb4d134",
            "3c6a9713e07a4d5ca8b3ffcfa8a162d7",
            "63f46b47640a44b59df6e50a66a86655",
            "7916389db1cd4af3b1ae458f45a11b85",
            "f9d2615b9af74d6284255b9536e9f13f",
            "1d5de97bb3894aaeb12029d5c18e5f26",
            "c1763c4adf2546d09fd3afbf19d451b5",
            "eb82898d36e24541a261291d55e7854a",
            "5873c625614144f9bcf2d9fa4fdfe629",
            "6fa8e587fd194998853b63705f2f9a9c",
            "35ac9184017d468fa5563963f125efb1",
            "a9748d7538694347867a47c74c11040d",
            "1811446280124daead7888f829e75ac3",
            "e9ed61814d22417191433dc3e74e360b",
            "8e0de2892b33430fb3d05da1a0be740a",
            "b6a86b13d77b40fcb36cdcc422c0e6f9",
            "5eef68f5da144772abb3411f351924d1",
            "4b25926914f1484fbec5ec8d23d8bf21",
            "e4510f5861a84ac38aaae81f26fe47c2",
            "0ede65a9b46549989f14ac5123cbcbdf",
            "b383f51438ae48c5a1b55055e011d635",
            "fe963f89f70c42a18370399e7616fa2c",
            "218eed9110c542c681b2f26370eb0bd2",
            "11fa7eaa9ff94d1982b40ba439ad5315",
            "9ad9d9d4576f423da487549081f84797",
            "a1479e80a36443bdb8d841e8bcd0b89e",
            "f91f7b3d91c44da3b2812a5dae8c5d69",
            "23f7b6f5bd8940d5871975d747267666",
            "c028d5fc98044c19ac88346e12e1b659",
            "e6689712d08c491ebfd8dbe717001aaa",
            "9653697c1840462999235cbedc84e2a0",
            "3595457f1d3345bf81cb3be5b77c4e6f",
            "4037f7dba5e9484b9729c585b03bcb9f",
            "d74f8064ac674da7aaac6994ff57def0",
            "ff18da8e142a4a0fb3edf0a1a0947857",
            "2106a5a5549542d3a8b113812f6f7574",
            "4ad97a1d9cc646e38e6dd481c3c0d0cb",
            "958f00c861aa49edb8d21b1e350209e9",
            "1e86e81cdf23434693d5b3b0cbd262fa",
            "d1a9c55dc8a14bbb875a1a42e24307cc",
            "377d5323de354d46b6dec6d11d675a40",
            "c08ddc806d164826b00847ad930b4127",
            "09e3a9144e0542fd89b649e1fde17580",
            "0573248fd2fc43608287e6d833154401",
            "cd50c11df9e64d36b9dcc22cf73908d3",
            "25d30960d01c4d25ac68a49b84871bf3",
            "2a1abbbd9c2e4f4c97006eaeff2df533",
            "9a6e25ac52f9480ab47c560b58b06121",
            "d9c788b75dab48a184e8c66f374fd5ad",
            "5ac0e864f42b49ce8b84ab8d00da4ee8",
            "5c3b1e547b82444f9071d233889dfa98",
            "df7b511eb4004174a7442c3e868a5366",
            "2fe0bb625db74295a2003d6c829e8eeb",
            "23462a5e82a341299eed4e640cc1716a",
            "b37207fa476a461cb1b19d52a32d092d",
            "5fa46d7650a549a8b267d84140fa68b7",
            "f7ce07903dab4f81af0cba9d75775f4a",
            "8c6fdb2bd2d74ea4b0fd9d74cbc88d6e",
            "9dfd01dbdc37400c8705ecf154d20a4e",
            "c2a5f2b8ee9646d09b4ed4307210cda1",
            "ef30126237f141e1b490ff23952a45f8",
            "ba53fbde5cd54e90bde25096b80bf545",
            "da86c007fe42496590b52338a52ad37a",
            "c0d15722de6443a9b677ed8f9cbcd087",
            "5728c2d6ef28407ca9a74606e4c07c2a",
            "04c31b8bb85f4b179f55020d3a0647a4",
            "e7f93fa223cc4e5db504aa6a45fc3fb1",
            "8b84c6149edf498bb1ae3ef2d5d828ab",
            "a2d92000bea34d1f83ec6e2bd55355af",
            "478ce5626d964ac7a746fda53fabec97",
            "fd2a9658616e4269a2f92dd40a1d14d0",
            "bf224e6ad2004b309052c4edc4a26eab",
            "cfae66baee264036b1fee033fcdaf2e8",
            "603d12d796514f86865531572d04889b",
            "83b6b2581aad48758096c0dc5ed9fa91",
            "31eadb645d2d4750bf5a7ffc5854e9fb",
            "f0cc179c70b64edf93d7ada8c58df086",
            "867b9b12f01c496fa162633cd9673265",
            "89fd3401399f4632bd5d867b667d8f66",
            "20daae9f2fa04c338c8be6556a7c24e8",
            "5c7f44b7b65e408299b2fd450e25bce2",
            "c349bb3e464e41aa942250d0d92dc168",
            "681cd3f74fc944189ea93489263a27d3",
            "bf9b9e119bd344af9d11a09d3f0dde24",
            "21baef647a78418c9248c0b44916cee3",
            "b008bfbaf130439ab02b98696878de1d",
            "c2aa30e826d24ecf8339ab241e714f4b",
            "37023bd8d824453baeda8a28cb5b84ae",
            "629d7954192643b79acbd6ed840cfcbb",
            "6a9a9108e7814dd0b5557c7e885d521f",
            "9b06610432114e91b767d3fcae5844a4",
            "84eae9e52c63441cb22a3176cba4e004",
            "a9e0e42346df4b91a5d6d8984cf22ca0",
            "f58a89b8a38a42b8b43675ccedc4ad67",
            "29aa1b389cd94a96bbefb6743d6bf903",
            "1ec85462e0554882b3cc5f7c4054a253",
            "c59da7757f804bed9b28f2c2ddef428c",
            "1c3a64f8bfe443e1861db08421d7c7f0",
            "597bde2b1d724ef7ab5503a2f5b5ddaa",
            "6a195d9651164d56adbe5a71603bc79c"
          ]
        },
        "id": "GemDyfILHFtF",
        "outputId": "1f490c38-15dc-420d-9e46-bbbf1d0f31d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Starte Pruning: b560 (bigscience/bloom-560m)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b23cf86613334321bcb1e412a8bce35c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d4803eca0d94c66b58eebe3e14851d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db2a4fd1ed1e4da38d92b8e6ffe081dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7a07bf2130c4812b84ec58c4e083144"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0972f6e09a4f412e9221780d0bafeb0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 01:14:05] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Pruning] Modus: global | Sparsity≈0.000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16e7b988d8d24bbaac71b6f0823d97d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c5079757f23490db21da87cb357df21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "348492313c0d48d4b95d2666bffa0b5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf837f92948e46bdbd4ca70d36c548d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd53c4ce0e264900bfb21b1e6722a7b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5941f9f7ac654ceebc3ed543ed2c2056"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bdf54a8cd6742b895b0424fd522698b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd7de2c5bc054537957e619758c239a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00000-of-00003.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd3520f290bc47fcbe37d918218e342f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00001-of-00003.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed0827f2b97c4b83b9dc4d4e57f2b057"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00002-of-00003.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d474589d23f1427e8dd58d4c4ef08d86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/validation-00000-of-00001.parquet:   0%|          | 0.00/474k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76f5d2cad6e347ea8f659ced6716a1be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/test-00000-of-00001.parquet:   0%|          | 0.00/509k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c89af6f8f2a243a7bdcf34cacefa79e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c6a9713e07a4d5ca8b3ffcfa8a162d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1811446280124daead7888f829e75ac3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11fa7eaa9ff94d1982b40ba439ad5315"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beispiele gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_pruning/bloom_pruning_samples_b560.txt\n",
            "\n",
            "### Starte Pruning: b3b (bigscience/bloom-3b)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff18da8e142a4a0fb3edf0a1a0947857"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25d30960d01c4d25ac68a49b84871bf3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7ce07903dab4f81af0cba9d75775f4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b84c6149edf498bb1ae3ef2d5d828ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.01G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89fd3401399f4632bd5d867b667d8f66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Pruning] Modus: global | Sparsity≈0.000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a9a9108e7814dd0b5557c7e885d521f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beispiele gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_pruning/bloom_pruning_samples_b3b.txt\n",
            "\n",
            "Ergebnisse (gesamt):\n",
            "                model_id alias                  precision  sparsity  \\\n",
            "0    bigscience/bloom-3b   b3b  8bit_eval_after_cpu_prune       0.0   \n",
            "1  bigscience/bloom-560m  b560  8bit_eval_after_cpu_prune       0.0   \n",
            "\n",
            "       time_s  energy_kwh    co2_kg  kg_per_kwh  tokens_out         ppl  \\\n",
            "0  262.666237    0.009168  0.002454    0.267622          96  234.286083   \n",
            "1  124.704923    0.003772  0.001009    0.267622          96  327.963333   \n",
            "\n",
            "        bleu    ram_GB  vram_alloc_GB  vram_reserved_GB  \\\n",
            "0  10.244657  3.723312       3.406985          5.693359   \n",
            "1   2.708001  2.647381       0.769470          1.029297   \n",
            "\n",
            "                                               notes  \n",
            "0  Prune@CPU; Eval 8-bit offload | GPU=NVIDIA A10...  \n",
            "1  Prune@CPU; Eval 8-bit offload | GPU=NVIDIA A10...  \n",
            "Gespeichert (gesamt): /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_pruning/bloom_pruning_results.csv\n",
            "\n",
            "Per-Phase Übersicht:\n",
            "  alias  phase      time_s  energy_kwh    co2_kg\n",
            "0  b560  prune   16.627166    0.000480  0.000129\n",
            "1  b560    gen    8.624089    0.000263  0.000070\n",
            "2  b560    ppl    7.695587    0.000231  0.000062\n",
            "3  b560   bleu   91.758082    0.002797  0.000749\n",
            "4   b3b  prune  132.351860    0.005024  0.001345\n",
            "5   b3b    gen    9.495952    0.000305  0.000082\n",
            "6   b3b    ppl    4.462935    0.000142  0.000038\n",
            "7   b3b   bleu  116.355489    0.003697  0.000989\n",
            "Gespeichert (per Phase): /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_pruning/bloom_pruning_per_phase.csv\n"
          ]
        }
      ]
    }
  ]
}