{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ino54/MA_GreenAI-Practical-Experiments/blob/main/bloom_kd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XerNOj3Z8kVM",
        "outputId": "b864f07a-f4f1-44d3-b952-8c90dfe5047f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "# Requirements\n",
        "%%writefile requirements.txt\n",
        "transformers>=4.41,<5\n",
        "accelerate>=0.30\n",
        "bitsandbytes>=0.43\n",
        "datasets>=2.19\n",
        "evaluate\n",
        "sacrebleu\n",
        "codecarbon>=2.5,<3\n",
        "pynvml>=11.5.0\n",
        "psutil\n",
        "numpy\n",
        "pandas\n",
        "huggingface_hub\n",
        "peft>=0.11\n",
        "sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -r requirements.txt\n",
        "!pip uninstall -y -q google-genai firebase-admin || true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJsY_SrB8y6U",
        "outputId": "4ce3cc52-e40a-4cbc-ca4d-8431f7871571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.6/517.6 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-genai 1.38.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Google Drive mounten ---\n",
        "import os, shutil, time, pathlib, inspect\n",
        "from google.colab import drive\n",
        "MOUNTPOINT=\"/content/drive\"\n",
        "already=os.path.isdir(os.path.join(MOUNTPOINT,\"MyDrive\"))\n",
        "if not already and os.path.isdir(MOUNTPOINT) and os.listdir(MOUNTPOINT):\n",
        "    backup=f\"/content/drive_stale_{int(time.time())}\"\n",
        "    shutil.move(MOUNTPOINT, backup)\n",
        "    os.makedirs(MOUNTPOINT, exist_ok=True)\n",
        "drive.mount(MOUNTPOINT, force_remount=(not already))\n",
        "\n",
        "# --- Zielordner (dein Pfad) ---\n",
        "work_dir=\"/content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_kd\"\n",
        "if not os.path.isdir(work_dir):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Zielordner fehlt: {work_dir}\\n\"\n",
        "        \"Bitte Ordner in Google Drive anlegen und Zelle erneut ausführen.\"\n",
        "    )\n",
        "os.chdir(work_dir)\n",
        "project_dir=work_dir\n",
        "print(\"Arbeitsordner:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLkr996j84vd",
        "outputId": "e7ab6dfc-2c11-4c55-c44b-50e1cab284eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Arbeitsordner: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_kd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hugging Face Login ---\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(hf_token); print(\"Hugging Face Login erfolgreich!\")\n",
        "else:\n",
        "    print(\"WARNUNG: Kein HF_TOKEN gefunden.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpkNAhgH8-xV",
        "outputId": "58597d46-f4a4-422d-976f-40c5edd6723c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face Login erfolgreich!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Imports & Setup ==================\n",
        "import re, math, gc, platform, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np, pandas as pd, torch, psutil\n",
        "from contextlib import nullcontext\n",
        "from types import SimpleNamespace\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig, set_seed,\n",
        "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        ")\n",
        "from codecarbon import EmissionsTracker\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "\n",
        "RESULT_BASENAME = \"bloom_kd\"\n",
        "set_seed(42)\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device==\"cuda\":\n",
        "  gpu_name=torch.cuda.get_device_name(0)\n",
        "  vram_total_gb=torch.cuda.get_device_properties(0).total_memory/(1024**3)\n",
        "  torch.backends.cuda.matmul.allow_tf32=True\n",
        "else:\n",
        "  gpu_name=\"CPU\"; vram_total_gb=0.0\n",
        "print(f\"Device: {device} | GPU: {gpu_name} | VRAM={vram_total_gb:.1f} GB | Torch {torch.__version__} | Py {platform.python_version()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adn_GAab9C38",
        "outputId": "ac4bc25b-de0b-4435-ed34-7b0c99047afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | GPU: NVIDIA A100-SXM4-40GB | VRAM=39.6 GB | Torch 2.8.0+cu126 | Py 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== CodeCarbon Helpers ==================\n",
        "import pandas as _pd\n",
        "def _cc_supported_kwargs():\n",
        "    base=dict(log_level=\"error\", output_dir=\".\", measure_power_secs=1, tracking_mode=\"process\")\n",
        "    try:\n",
        "        params=inspect.signature(EmissionsTracker.__init__).parameters\n",
        "        if \"cloud_provider\" in params: base[\"cloud_provider\"]=\"google\"\n",
        "        if \"cloud_region\"  in params: base[\"cloud_region\"]=\"europe-west10\"\n",
        "        if \"country_iso_code\" in params: base[\"country_iso_code\"]=\"DEU\"\n",
        "    except Exception: pass\n",
        "    return base\n",
        "def make_tracker(name,out): return EmissionsTracker(project_name=name, output_file=out, **_cc_supported_kwargs())\n",
        "def safe_start(tr):\n",
        "    try: tr.start(); return True\n",
        "    except Exception as e: print(\"[CodeCarbon] Start fehlgeschlagen:\", e); return False\n",
        "def safe_stop(tr, started):\n",
        "    if not started: return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "    try: return tr.stop()\n",
        "    except Exception as e: print(\"[CodeCarbon] Stop fehlgeschlagen:\", e); return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "def unpack(em):\n",
        "    if hasattr(em,\"energy_consumed\") and hasattr(em,\"emissions\"):\n",
        "        try: return float(em.energy_consumed), float(em.emissions)\n",
        "        except: pass\n",
        "    if isinstance(em, dict):\n",
        "        e=em.get(\"energy_consumed\",0.0); c=em.get(\"emissions\", em.get(\"emissions_kg\",0.0))\n",
        "        try: return float(e), float(c)\n",
        "        except: return 0.0,0.0\n",
        "    try: return 0.0, float(em)\n",
        "    except: return 0.0,0.0\n",
        "def read_energy_from_log(path):\n",
        "    try:\n",
        "        if not os.path.exists(path): return 0.0\n",
        "        df=_pd.read_csv(path)\n",
        "        for c in [\"energy_consumed\",\"energy_consumed_kwh\",\"energy_consumed (kWh)\",\"energy (kWh)\"]:\n",
        "            if c in df.columns: return float(df[c].iloc[-1])\n",
        "        for c in df.columns:\n",
        "            if \"energy\" in c.lower() and \"kwh\" in c.lower(): return float(df[c].iloc[-1])\n",
        "    except: pass\n",
        "    return 0.0\n",
        "def measure_phase(phase, fn, prefix):\n",
        "    logfile=os.path.join(project_dir, f\"{prefix}_{phase}.csv\")\n",
        "    tr=make_tracker(f\"{prefix}_{phase}\", logfile)\n",
        "    import time as _t\n",
        "    st=safe_start(tr); t0=_t.time(); res=fn(); t1=_t.time()\n",
        "    e=safe_stop(tr, st); ekwh, co2=unpack(e)\n",
        "    if ekwh==0.0:\n",
        "        ek=read_energy_from_log(logfile)\n",
        "        if ek: ekwh=ek\n",
        "    return {\"phase\":phase,\"time_s\":t1-t0,\"energy_kwh\":ekwh,\"co2_kg\":co2}, res"
      ],
      "metadata": {
        "id": "pkQBrz3N9GkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== KD/Eval Konfiguration ==================\n",
        "TEACHER_ID=\"bigscience/bloom-3b\"\n",
        "STUDENT_ID=\"bigscience/bloom-560m\"\n",
        "KD_CFG = dict(\n",
        "    teacher_8bit=True,\n",
        "    student_4bit=True,\n",
        "    use_lora=True,\n",
        "    temperature=2.5,\n",
        "    alpha=0.7,              # Endwert; Startwert unten\n",
        "    alpha_start=0.9,        # Startwert für Schedule\n",
        "    block_size=1024,        # ggf. auf 512 reduzieren falls VRAM knapp\n",
        "    epochs=3,\n",
        "    bsz=2,\n",
        "    grad_accum=8,\n",
        "    lr=1.5e-4,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.03,\n",
        "    max_grad_norm=0.3,\n",
        "    logging_steps=25,\n",
        "    save_steps=500,\n",
        "    train_split=\"train[:10%]\",\n",
        "    eval_split=\"test[:1%]\",\n",
        ")\n",
        "EVAL={\"max_new_tokens\":32,\n",
        "      \"ppl\":{\"name\":\"wikitext\",\"config\":\"wikitext-2-raw-v1\",\"split\":KD_CFG[\"eval_split\"]},\n",
        "      \"bleu\":{\"name\":\"wmt14\",\"config\":\"de-en\",\"split\":\"test[:16]\"}}\n",
        "PROMPTS=[\n",
        "  \"Schreibe einen kurzen Absatz über nachhaltige KI.\",\n",
        "  \"Erkläre in einfachen Worten, was Quantisierung in neuronalen Netzen ist.\",\n",
        "  \"Nenne drei Vorteile von Mixture-of-Experts-Modellen.\"\n",
        "]\n",
        "bleu_metric=evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0693832530c84324b0743fc0ef9c9d71",
            "05106ac1687d42eeb4aadc30931c83d7",
            "75f7bdecbcf447098fb2a344cf9db9af",
            "3e83a4e2ee244576941390957f7538a9",
            "3a453c48857a4a6a8545823958ddb4e4",
            "11ab026e05df4d18b59d2a53f7e03f44",
            "333ce6ffee4f4e7aa3d28979533f8c5b",
            "3f9ab0e6821e4ec894ec78bc54bef21e",
            "2ba3b1ac80f2468080565998b2f06778",
            "4a92d882db3e40768783e78cc919ea88",
            "839f9776a76f40feb8f55cbb9a73046b"
          ]
        },
        "id": "pIcdjZv09MiQ",
        "outputId": "907eb79d-a49c-4d5b-afcc-548a7bfd5ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0693832530c84324b0743fc0ef9c9d71"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Helper (Gen/PPL/BLEU) ==================\n",
        "def autocast_ctx():\n",
        "  return torch.autocast(device_type=\"cuda\", dtype=torch.float16) if device==\"cuda\" else nullcontext()\n",
        "def safe_max_len(tok, model, fallback=2048, upper=100000):\n",
        "  cand=getattr(tok,\"model_max_length\",None)\n",
        "  if isinstance(cand,int) and 0<cand<upper: return cand\n",
        "  cand=getattr(getattr(model,\"config\",None),\"max_position_embeddings\",None)\n",
        "  if isinstance(cand,int) and 0<cand<upper: return cand\n",
        "  return fallback\n",
        "def warmup(model, tok, max_len):\n",
        "  with torch.no_grad(), autocast_ctx():\n",
        "    x=tok(\"Hello\", return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
        "    _=model.generate(**x, max_new_tokens=1, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "\n",
        "def do_gen(model, tok, max_new_tokens):\n",
        "  total, texts=0, []\n",
        "  ml=safe_max_len(tok, model)\n",
        "  for p in PROMPTS:\n",
        "    enc=tok(p, return_tensors=\"pt\", truncation=True, max_length=ml).to(model.device)\n",
        "    room=ml-enc[\"input_ids\"].shape[1]\n",
        "    cur_new=max(1, min(max_new_tokens, int(room)))\n",
        "    with torch.no_grad(), autocast_ctx():\n",
        "      out=model.generate(**enc, max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "    total+=int(out.shape[1]-enc[\"input_ids\"].shape[1])\n",
        "    texts.append(tok.decode(out[0], skip_special_tokens=True))\n",
        "  return texts, total\n",
        "\n",
        "def do_ppl(model, tok, ds_cfg):\n",
        "  ds=load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "  ml=safe_max_len(tok, model); losses=[]\n",
        "  with torch.no_grad():\n",
        "    for t in ds[\"text\"]:\n",
        "      if not isinstance(t,str) or len(t.strip())<4: continue\n",
        "      enc=tok(t, return_tensors=\"pt\", truncation=True, max_length=ml).to(model.device)\n",
        "      with autocast_ctx():\n",
        "        out=model(enc[\"input_ids\"], labels=enc[\"input_ids\"])\n",
        "      losses.append(float(out.loss.detach().cpu()))\n",
        "  return math.exp(np.mean(losses)) if losses else None\n",
        "\n",
        "def do_bleu(model, tok, ds_cfg, max_new_tokens):\n",
        "  ds=load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "  ml=safe_max_len(tok, model); preds, refs=[], []\n",
        "  with torch.no_grad():\n",
        "    for ex in ds:\n",
        "      de,en=ex[\"translation\"][\"de\"], ex[\"translation\"][\"en\"]\n",
        "      prompt=f\"Translate to English:\\nGerman: {de}\\nEnglish:\"\n",
        "      enc=tok(prompt, return_tensors=\"pt\", truncation=True, max_length=ml).to(model.device)\n",
        "      room=ml-enc[\"input_ids\"].shape[1]\n",
        "      cur_new=max(1, min(max_new_tokens, int(room)))\n",
        "      with autocast_ctx():\n",
        "        out=model.generate(**enc, max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "      gen=tok.decode(out[0], skip_special_tokens=True)\n",
        "      hyp=gen.split(\"English:\")[-1].strip().split(\"\\n\")[0].strip() or gen.strip()\n",
        "      preds.append(hyp); refs.append([en])\n",
        "  return float(bleu_metric.compute(predictions=preds, references=refs)[\"score\"])"
      ],
      "metadata": {
        "id": "TeCCKNq59MeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #================== KD/LoRA ==================\n",
        "def maybe_wrap_lora(model, r=32, alpha=64, dropout=0.05):\n",
        "    target_modules = [\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"]  # BLOOM\n",
        "    cfg = LoraConfig(\n",
        "        r=r, lora_alpha=alpha, lora_dropout=dropout,\n",
        "        target_modules=target_modules, bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "    )\n",
        "    model = get_peft_model(model, cfg)\n",
        "    try: model.print_trainable_parameters()\n",
        "    except: pass\n",
        "    return model\n",
        "\n",
        "def kd_loss(student_logits, teacher_logits, labels, T: float, alpha: float):\n",
        "    sT = student_logits[..., :-1, :] / T\n",
        "    tT = teacher_logits[..., :-1, :] / T\n",
        "    lab = labels[..., 1:]\n",
        "    kd = F.kl_div(F.log_softmax(sT, dim=-1), F.softmax(tT, dim=-1), reduction=\"batchmean\") * (T*T)\n",
        "    ce = F.cross_entropy(student_logits[..., :-1, :].transpose(1, 2), lab, ignore_index=-100)\n",
        "    return alpha*kd + (1.0-alpha)*ce\n",
        "\n",
        "class KDTrainer(Trainer):\n",
        "    def __init__(self, *args, teacher=None, temperature=2.5, alpha=0.7, alpha_start=0.9, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        assert teacher is not None, \"Teacher-Modell erforderlich.\"\n",
        "        self.teacher = teacher.eval()\n",
        "        for p in self.teacher.parameters(): p.requires_grad_(False)\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.alpha_start = alpha_start\n",
        "\n",
        "    def _alpha_now(self):\n",
        "        ep = getattr(self.state, \"epoch\", None)\n",
        "        if ep is None or self.args.num_train_epochs <= 1:\n",
        "            return self.alpha\n",
        "        ratio = min(max(ep / self.args.num_train_epochs, 0.0), 1.0)\n",
        "        return (1 - ratio) * self.alpha_start + ratio * self.alpha\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n",
        "        labels = inputs.get(\"labels\", None)\n",
        "        if labels is None:\n",
        "            labels = inputs[\"input_ids\"].clone()\n",
        "            if \"attention_mask\" in inputs:\n",
        "                labels[inputs[\"attention_mask\"]==0] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            t_out = self.teacher(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs.get(\"attention_mask\")\n",
        "            )\n",
        "        s_out = model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs.get(\"attention_mask\")\n",
        "        )\n",
        "        s_logits, t_logits = s_out.logits, t_out.logits\n",
        "        T = self.temperature\n",
        "        alpha_now = self._alpha_now()\n",
        "\n",
        "        sT = s_logits[..., :-1, :] / T\n",
        "        tT = t_logits[..., :-1, :] / T\n",
        "        lab = labels[..., 1:]\n",
        "\n",
        "        kd = F.kl_div(F.log_softmax(sT, dim=-1), F.softmax(tT, dim=-1), reduction=\"batchmean\") * (T*T)\n",
        "        ce = F.cross_entropy(s_logits[..., :-1, :].transpose(1, 2), lab, ignore_index=-100)\n",
        "        loss = alpha_now*kd + (1.0-alpha_now)*ce\n",
        "        return (loss, s_out) if return_outputs else loss"
      ],
      "metadata": {
        "id": "ACRZbgnA9Y7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Datenaufbereitung ==================\n",
        "def build_datasets(tokenizer, dataset=\"wikitext\", config=\"wikitext-2-raw-v1\",\n",
        "                   train_split=KD_CFG[\"train_split\"], eval_split=KD_CFG[\"eval_split\"],\n",
        "                   block_size=KD_CFG[\"block_size\"]):\n",
        "    ds_train = load_dataset(dataset, config, split=train_split)\n",
        "    ds_eval  = load_dataset(dataset, config, split=eval_split)\n",
        "    def tok_fn(batch):\n",
        "        return tokenizer(batch[\"text\"], return_attention_mask=False, truncation=False)\n",
        "    train_tok = ds_train.map(tok_fn, batched=True, remove_columns=ds_train.column_names)\n",
        "    eval_tok  = ds_eval.map(tok_fn,  batched=True, remove_columns=ds_eval.column_names)\n",
        "    def group_texts(examples):\n",
        "        concatenated = sum(examples[\"input_ids\"], [])\n",
        "        total_len = (len(concatenated) // block_size) * block_size\n",
        "        concatenated = concatenated[:total_len]\n",
        "        return {\"input_ids\": [concatenated[i:i+block_size] for i in range(0, total_len, block_size)]}\n",
        "    train_blocks = train_tok.map(group_texts, batched=True)\n",
        "    eval_blocks  = eval_tok.map(group_texts,  batched=True)\n",
        "    return train_blocks, eval_blocks"
      ],
      "metadata": {
        "id": "oqFGZWad9mVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # ================== Modelle laden ==================\n",
        "def load_teacher_student():\n",
        "    tok = AutoTokenizer.from_pretrained(STUDENT_ID, use_fast=True)\n",
        "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "    tok.padding_side=\"right\"\n",
        "\n",
        "    t_bnb = BitsAndBytesConfig(load_in_8bit=True) if KD_CFG[\"teacher_8bit\"] else None\n",
        "    teacher = AutoModelForCausalLM.from_pretrained(\n",
        "        TEACHER_ID, device_map=\"auto\",\n",
        "        quantization_config=t_bnb\n",
        "    ).eval()\n",
        "    for p in teacher.parameters():\n",
        "        p.requires_grad_(False)\n",
        "\n",
        "    s_bnb = None\n",
        "    if KD_CFG[\"student_4bit\"]:\n",
        "        s_bnb = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "    student = AutoModelForCausalLM.from_pretrained(\n",
        "        STUDENT_ID,\n",
        "        device_map=\"auto\" if (KD_CFG[\"student_4bit\"] or KD_CFG[\"teacher_8bit\"]) else None,\n",
        "        quantization_config=s_bnb\n",
        "    )\n",
        "\n",
        "    student = prepare_model_for_kbit_training(student, use_gradient_checkpointing=False)\n",
        "    student.config.use_cache = False\n",
        "\n",
        "    if KD_CFG[\"use_lora\"]:\n",
        "        student = maybe_wrap_lora(student, r=32, alpha=64, dropout=0.05)\n",
        "\n",
        "    try:\n",
        "        student.enable_input_require_grads()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    student.train()\n",
        "\n",
        "    trainable = sum(p.numel() for p in student.parameters() if p.requires_grad)\n",
        "    if trainable == 0:\n",
        "        raise RuntimeError(\"Keine trainierbaren Parameter im Student! Prüfe LoRA-Targets/QLoRA-Setup.\")\n",
        "    print(f\"[Sanity] Trainable params: {trainable:,}\")\n",
        "\n",
        "    return tok, teacher, student"
      ],
      "metadata": {
        "id": "KpThEhkp9pb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== TrainingArguments (robust gegen API-Änderungen) ==================\n",
        "def build_training_args(output_dir, bf16_ok):\n",
        "    # gewünschte Defaults\n",
        "    desired = dict(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=KD_CFG[\"bsz\"],\n",
        "        gradient_accumulation_steps=KD_CFG[\"grad_accum\"],\n",
        "        learning_rate=KD_CFG[\"lr\"],\n",
        "        num_train_epochs=KD_CFG[\"epochs\"],\n",
        "        logging_steps=KD_CFG[\"logging_steps\"],\n",
        "        save_steps=KD_CFG[\"save_steps\"],\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\",\n",
        "        fp16=not bf16_ok, bf16=bf16_ok,\n",
        "        gradient_checkpointing=False,\n",
        "        ddp_find_unused_parameters=False,\n",
        "        # folgende sind versionsabhängig:\n",
        "        evaluation_strategy=\"epoch\",   # alias: eval_strategy (neuere Versionen)\n",
        "        optim=\"adamw_bnb_8bit\",\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=KD_CFG[\"warmup_ratio\"],\n",
        "        weight_decay=KD_CFG[\"weight_decay\"],\n",
        "        max_grad_norm=KD_CFG[\"max_grad_norm\"],\n",
        "        logging_first_step=True\n",
        "    )\n",
        "    params = inspect.signature(TrainingArguments.__init__).parameters\n",
        "    # Map/Drop je nach Signatur\n",
        "    if \"evaluation_strategy\" not in params:\n",
        "        if \"eval_strategy\" in params:\n",
        "            desired[\"eval_strategy\"] = desired.pop(\"evaluation_strategy\")\n",
        "        else:\n",
        "            desired.pop(\"evaluation_strategy\", None)\n",
        "    if \"optim\" not in params:\n",
        "        desired.pop(\"optim\", None)\n",
        "    if \"logging_first_step\" not in params:\n",
        "        desired.pop(\"logging_first_step\", None)\n",
        "    # Nur erlaubte Keys übergeben (failsafe)\n",
        "    filtered = {k:v for k,v in desired.items() if k in params}\n",
        "    return TrainingArguments(**filtered)\n",
        "\n",
        "# ================== Training + Eval orchestrieren ==================\n",
        "def train_kd_once():\n",
        "    tok, teacher, student = load_teacher_student()\n",
        "    train_ds, eval_ds = build_datasets(tok)\n",
        "    collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
        "\n",
        "    bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "    args = build_training_args(os.path.join(project_dir, \"train_out\"), bf16_ok)\n",
        "\n",
        "    trainer = KDTrainer(\n",
        "        model=student, teacher=teacher,\n",
        "        temperature=KD_CFG[\"temperature\"], alpha=KD_CFG[\"alpha\"], alpha_start=KD_CFG[\"alpha_start\"],\n",
        "        args=args, train_dataset=train_ds, eval_dataset=eval_ds, data_collator=collator\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    # Speichern (bei QLoRA/LoRA: Adapter)\n",
        "    save_dir=os.path.join(project_dir, \"student_kd\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    student.save_pretrained(save_dir)\n",
        "    tok.save_pretrained(save_dir)\n",
        "\n",
        "    # Eval & Generation (kurz)\n",
        "    ml=safe_max_len(tok, student); warmup(student, tok, ml)\n",
        "    ppl = do_ppl(student, tok, EVAL[\"ppl\"])\n",
        "    bleu = do_bleu(student, tok, EVAL[\"bleu\"], EVAL[\"max_new_tokens\"])\n",
        "    samples, n_tok = do_gen(student, tok, EVAL[\"max_new_tokens\"])\n",
        "\n",
        "    with open(os.path.join(project_dir, f\"{RESULT_BASENAME}_samples_student.txt\"),\"w\",encoding=\"utf-8\") as f:\n",
        "        for i,t in enumerate(samples,1): f.write(f\"--- Beispiel {i} ---\\n{t}\\n\\n\")\n",
        "\n",
        "    return {\"ppl\": ppl, \"bleu\": bleu, \"tokens_out\": int(n_tok), \"save_dir\": save_dir}, samples, tok, student"
      ],
      "metadata": {
        "id": "cCOr91Eo93Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Ausführung (mit CodeCarbon pro Phase) ==================\n",
        "prefix=f\"{RESULT_BASENAME}_b560_from_b3b_qlo\"\n",
        "m_train, (train_res, samples, tok, student) = measure_phase(\"train\", train_kd_once, prefix)\n",
        "m_ppl, _  = measure_phase(\"eval_ppl\",  lambda: do_ppl(student, tok, EVAL[\"ppl\"]), prefix)\n",
        "m_bleu, _ = measure_phase(\"eval_bleu\", lambda: do_bleu(student, tok, EVAL[\"bleu\"], EVAL[\"max_new_tokens\"]), prefix)\n",
        "m_gen, _  = measure_phase(\"gen\",       lambda: do_gen(student, tok, EVAL[\"max_new_tokens\"]), prefix)\n",
        "\n",
        "# ================== Ergebnisse sammeln & speichern ==================\n",
        "row = dict(\n",
        "    teacher_id=TEACHER_ID, student_id=STUDENT_ID,\n",
        "    kd_temperature=KD_CFG[\"temperature\"], kd_alpha=KD_CFG[\"alpha\"],\n",
        "    block_size=KD_CFG[\"block_size\"], epochs=KD_CFG[\"epochs\"],\n",
        "    ppl=train_res[\"ppl\"], bleu=train_res[\"bleu\"], tokens_out=train_res[\"tokens_out\"],\n",
        "    notes=f\"GPU={gpu_name}, VRAM={vram_total_gb:.1f} GB; Teacher 8-bit={KD_CFG['teacher_8bit']}; Student 4-bit={KD_CFG['student_4bit']}; LoRA={KD_CFG['use_lora']}\"\n",
        ")\n",
        "totals = dict(\n",
        "    time_s = m_train[\"time_s\"] + m_ppl[\"time_s\"] + m_bleu[\"time_s\"] + m_gen[\"time_s\"],\n",
        "    energy_kwh = m_train[\"energy_kwh\"] + m_ppl[\"energy_kwh\"] + m_bleu[\"energy_kwh\"] + m_gen[\"energy_kwh\"],\n",
        "    co2_kg = m_train[\"co2_kg\"] + m_ppl[\"co2_kg\"] + m_bleu[\"co2_kg\"] + m_gen[\"co2_kg\"]\n",
        ")\n",
        "row.update({\n",
        "    \"time_s_total\": totals[\"time_s\"],\n",
        "    \"energy_kwh_total\": totals[\"energy_kwh\"],\n",
        "    \"co2_kg_total\": totals[\"co2_kg\"],\n",
        "    \"kg_per_kwh\": (totals[\"co2_kg\"]/totals[\"energy_kwh\"]) if totals[\"energy_kwh\"] else None\n",
        "})\n",
        "\n",
        "df = pd.DataFrame([row])\n",
        "df_phases = pd.DataFrame([m_train, m_ppl, m_bleu, m_gen])\n",
        "df.to_csv(os.path.join(project_dir, f\"{RESULT_BASENAME}_results.csv\"), index=False)\n",
        "df_phases.to_csv(os.path.join(project_dir, f\"{RESULT_BASENAME}_results_per_phase.csv\"), index=False)\n",
        "\n",
        "print(\"\\n=== KD Ergebnisse (Student) ===\\n\", df)\n",
        "print(\"\\n=== Phasen (Zeit/Energie/CO2) ===\\n\", df_phases)\n",
        "print(\"Gespeichert:\", os.path.join(project_dir, f\"{RESULT_BASENAME}_results.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e617ae50a4ee4bf68f35a674098e0f10",
            "4566d338f1da41be8ba1ae1772a47a5c",
            "14a716da684047cb806f34383e91f844",
            "f57ed9294c8e49099ad0c296fd7b2bcb",
            "17a23b34028240e4b64dcbbaa818fda4",
            "a89ba6feba534595b34c0958cc31d0be",
            "5f05edd8179c49a5877f3fc133e2b2ac",
            "49e9ab9756e24dfda2c1de308677ee4e",
            "db42606c5dde45138ad016e995e47c6c",
            "d5f0a5c91b6f4cf99b2fe3379e167a3c",
            "c79b982fd0b844399393285b5b487354",
            "b406f345bdad48d58dc17f449c23ecdc",
            "1b30ba65b82247adb991d9079355b24d",
            "7f07128f35cc447888022d0ca119cc0b",
            "2529768f9c8f4a638ef77fa2552acece",
            "dc28bf2ba5984555b22642c2e5565142",
            "4d512eff443e49df8cb5482b06915d29",
            "3978c2d1984a417189f3455552efcc1d",
            "5e2788f30d9c424598739e531cb65166",
            "6b4822d9e0aa469c862cbaa8ab57c4ef",
            "62dc46f04e3d410cb3ff36fbfc0b1c35",
            "9e0b9adc534048878dcafe37db3b7e4f",
            "0dcf42b081e94bdbbb01905096a2422c",
            "731f1818cba54c9e962208c08c92e562",
            "5aaefd0d4f2f458594e2ebf45a10ef48",
            "43de65f900634411a969d21c5e485075",
            "6a71f5d38db54ca3a37928752f13b271",
            "005cfab7bc144e45b2a8f260e85bac17",
            "ceeeb94e1f3a49a4a92f42a6d17edfb2",
            "b8db0f97b0664b7bb41078f5ee9c140c",
            "9046288e630349ef8ccaaca994d01911",
            "37499718187b46f18ea5bac98dbed02f",
            "676b5c828d79465c85958260d1e543ac",
            "f8c5b6810fe1437bb3be463e9e75ef68",
            "f08dd151c02f43f785d9e4bb019f20e1",
            "9f2f8f7367894685972058df1a6dc76e",
            "5c1de68d7d1746bbbc82c1deb14bbd34",
            "f4a73e61102f45b390cd8f383b20db06",
            "16bbaa4a67a9497495e8ae7146e0cbbf",
            "e59da0acf1584b739e586126c380aa3f",
            "308560771f264abba0d6c8f4f53ebbee",
            "2e99ee77907749349dc4d4e61b9a1533",
            "e979db4c316444219435260be48ab6d0",
            "84b7da8af53746f59391c196d877431a",
            "d1132405d6f348258d1a2b19655c7eed",
            "593ed23299714543a2b7b516882926c2",
            "af36de72696c45cbab919aee9a1bef65",
            "16a2e3e19c90436290eb7d5c371a6447",
            "82bc6b185a6948fdaa854eb1eaf085d6",
            "c845b4632095477e89b852ac73e73652",
            "76e32ac415be438aa42579fe1547b2d5",
            "7242c0fc7e1842fca4fece78d6bd35ac",
            "550de08b0fa04d90b55bd0ba04ff4a1f",
            "e798b258e8884b5b97cc09783c5a0288",
            "b5de317ba0f64bf8ac7d0f6b783d512f",
            "788bd2eb5511497b95a685c20f18b12e",
            "6175bd46d3d144b7b49effcdb8285cb9",
            "7ba548a428ab4a46a33288f6d7a1541f",
            "b92d16265dea464893dd1a1aa5722797",
            "f4efafb510be4da99705ddc972860123",
            "59692f0ffd334e5d858cef6d8a2c5746",
            "3bf2560d1969442e90072d64ccb2f194",
            "b7dc846c36ae480c95eab276fad31c65",
            "927ffea2602f47a4aea4aa827e4268c1",
            "4d8306f529c9405ab38f6e6436330b91",
            "7e70759d25c74cd2b1176ce25bb1b628",
            "a20187dc8b3441df91b2ef91eb4ef71b",
            "247e5418c7d245f0a0b806cefa97da1b",
            "089eda36a1d04b9785589ce6e98bb6eb",
            "3bcaf70aa67143d38a65caa3a2aea0fd",
            "57f53b26d58e400eb74dce9ab20208a6",
            "155dc1be4ce7462abaca77a63f4ad2e8",
            "a6493982059544ef81a5cfcaee17b242",
            "569c99e4c514496dae237ca2fe9d12a4",
            "c0f6a768a6c24be9833f66d36ecd14c6",
            "e421ac6a4fa6408b96bfc2321a85b09d",
            "460c63fc9e534bc4a20d271f2f798927",
            "c0b6a62c9040428cbf9e79f04028ef36",
            "f60ea565d0924da78bb0d2c3080ce10c",
            "d216c3497eae4eaa94e650fe2285f029",
            "8180510b99224ad0bc4834d1e1fa778b",
            "9ae150e57157424aa7f4b8f381b82fd9",
            "6442428baf9b4603af2256879510424a",
            "02a55cfdbd714da9aa86dee9c9fbfd05",
            "d83f73e036f9494bb5569b496543aea9",
            "5912f82c6f954644b21dcefbb3f0019b",
            "eada9fd911a54addacfa729fe260e26c",
            "4ce1c4ef1846465f91c01072ddb004d9",
            "51719be44bb44ab1be6c86e2ca208b17",
            "6a70a6ccb1ac459b84994d7fc594ceb0",
            "e1d6653aade24139a181002fcb3b61c0",
            "f80cd8dd83e640a9922e261b3c0231b0",
            "d993a94b7b0d496f985f378b2b73fa15",
            "5d0ff124f2774b618207417005f128d3",
            "ef13df3d575e499ab3f31e0814b50fc8",
            "9cf9d697cb7146348ce29116f98f2d96",
            "de9cbd7b46174cb482890f5e951759e7",
            "c765e466c18b40d18cebe1968d975aa8",
            "85d16f8993c349a396a9e0b649cb7104",
            "5e9f109bb64548fa99340e3e05a5ad69",
            "6f50e769f34c46fd964395db85da10ce",
            "050f035f09fd415e8e8d91c0ee2cd1b4",
            "5d4c0042c9c64ffb9ca9e01b11db014b",
            "ae8c3f3945bc44c7923f9f9d6f74bf09",
            "586368410f814b6794c9676f4a236799",
            "e0f7e3bfbc9d4c5d980d0e4afbcab4f3",
            "85784568a8414e49ba43235370366402",
            "7198ce073e6744a882be2220bf562162",
            "fe491f9ae7454140816fb01e55f28289",
            "4b397497c1ea4560a20d7ed4d5e9dc51",
            "5b49391716f244e2b18b565cc53357b5",
            "c048fd1303984dd385f31088c3e96cef",
            "0e35e3ef3b6d4954b8f9ed90bbfe8525",
            "64b65971eda64386bb6046de4a59bc88",
            "b0ac114ebc484b47a31fb52be8e84af0",
            "5c8cf97281cf46ed85ef0258f31259a8",
            "3e52013910e944d38f7e112c33b275d1",
            "3a119bb9f0044cd4aee1fc885fc8bbad",
            "59de643cda4c4c98aa58174a54170131",
            "97ab7d027f8b46d3ab7ebfc6122d7068",
            "2e204acc86254e1582e2c67f35015974",
            "2408d0e9f07a4457b8badefd8f126f46",
            "92c96690d0dd493e9009587df4d6f2d6",
            "08f9f642f4354400af3543cbd1e97dce",
            "3bb4882514554ab68939351460f6227e",
            "e9186c23f39d4d1eb01d7a404f510efd",
            "fc903b74f1084e11a76f441b1b2927ae",
            "f4bbaf6017174a63b3c54fe113480b4c",
            "1c7117031a924fa0af5b8802097201e9",
            "ebf2bd41636d49ba98560d613a0abf15",
            "9849a2041ea841bf8b08e5d3f21da3c8",
            "c4217dcd7b714ad295f4e174b6d175ac",
            "5f753c3504cf44e499f6203ba79c9815",
            "f3d1f60552b947c3841707863cb85567",
            "6ae9b2c0e6e8405c9f01156fdbfc5970",
            "eab78622534a402fbdd8493ffa5edb0a",
            "5e9a2601005b44ad8416d24b926df66d",
            "b8f15f77602a47fab9b9101e977352ec",
            "563deff3734b4c4bb87a11947717ca65",
            "ecc482d1eb064cb486f822c6e02c6479",
            "6712dae4386c440f92dea61523dfe6d0",
            "4a528a53b12d40baa203402197f67f71",
            "9a2fe64f4bf24824b0f26ce0b51c78d9",
            "3d1db4562cfe48c19608c67c5e9bbf3c",
            "54bf523c345d4f6192f824b40564a1a1",
            "e87334f052da4dd0bb045ccdcf4a9d8d",
            "5033d01402194f7fbeef883a8ef92599",
            "2ba067332e714fdea3492993a97f67ae",
            "8bd9adb19c3f4a9a9d81b7c8ec4d3de6",
            "b60934b57bfb453dabf29ca7c7af4b3b",
            "b3564d73b1ef4f6fa9c8df633fa987a8",
            "415a94b7a6374cc7abd6ec8698364bdb",
            "150ce4d27ffb48ee9faee672370c28a6",
            "f00873e7ec0a4ea9b5af5513d3aa4e71",
            "9b7cbe3c68414b9f8659c5025e67b213",
            "30db6c2e9c484db9b3181304789f5863",
            "00cf3fc3acb848c0bd7816b0291a161a",
            "4f619740e5254e6baa8ebe8b38b5a531",
            "ff49265f4f3842afab0ca2ba3a3ec2a5",
            "65a6087925b445819fc36acdc4077c67",
            "c9fe4cc0a0a742e8ba2d41b1c94bd00b",
            "d50edee5bdf54ee8ad8683d1de125605",
            "ccd29ae70abd4c71892c9bc6c99afeae",
            "1ac1c6bc0c8a45e08d9b2b6cd795e6a6",
            "48751e008bde4555ae653581f1aacdb1",
            "30ed46f264e44d1d82415847ae3dac68",
            "8917f22f0edf432ca49124cbbbf03dc4",
            "effde7e2fc624b94adaf626378eedfe7",
            "bf0b3b760d5e47adb563ceff0029f6cd",
            "d32f5e825a5243879bba0681f20642b4",
            "74626d30a1e143ec8ed75487f1a90576",
            "7d507458b7b4489c9453c1855dffee8c",
            "989b56e9852441008d2fa6c0a6c38582",
            "df44d9519db94d3c8b90adabe85050de",
            "7d7a6742730347b7ba41d7d58352cca4",
            "b8e277e1ae274b87949034ae7dff0703",
            "d8714a4d6e844ebcba314f83ba0e8205",
            "11af798288cd4793a84ab53b6db751bb",
            "83c4dbf375e144d5aa7d95c41bfd65d8",
            "8002644cf7b943168e54e396f825ce0c",
            "a8471602f186470f917a5e2205d55d2b",
            "bcd11e6fa3124550b5ec2ab3f7aca063",
            "6671bd7e83fe4eeabc379647a1cfa177",
            "a451fd7104ed447abd8aefcffc671928",
            "82355177ff0f4d79ae3f4e41c7c94be0",
            "277e1ea11a254ca2aa06d97ea41938e8",
            "90e1b833df254f7c94b3c6f305357455",
            "c70444191eab448ba5f8c3636c415c23",
            "560c86fbca984f4198e1755b88f1272a",
            "44d1d18a66b34135a166c91695b00092",
            "a3947d06c87d474ca9c71af663ee2e6d",
            "39ff5031b1c14b5d8d565a56c821c8be",
            "e48f5a70a22440b883d123f700d0b570",
            "31147aa9c24e41b28a21bc38d3c09ca7",
            "1b36d3af669e47d4bd39cffe223f76cf",
            "52c5c7fd62214e4a9bfa2e68ab9d0077",
            "a03b4325507645f9b5c9fce201d87b87",
            "0f51719ba4b44264965a3bcab3dd7fc2",
            "fc753cbb1d364dd0b6ed184d703972af",
            "31272d1850bc44ceb6a502d11a096d23",
            "72b051bac5044a708af424464cda9bc6",
            "b4583494234e49449b6aab9f86304e4a",
            "36cd2e5a0cae4640a0ec7b606f1bf717",
            "9067c9bb2bf64831ba4388f7959e1bbe",
            "eff180fc787b45269d80559bbdb55aeb",
            "069e21194264479caf3baf70df2b826a",
            "d228be854a2c4c58bcbad2e17b917377",
            "2fbb2650e56b4e558222c06d095d9344",
            "951087e7c5af4e1fb34f2d22839ddd6d",
            "b5c469e58a8642da8b0675d2984eb17d",
            "4d6b4bde004a4736a2a2e598ec83b55c",
            "55cf35c6d9284203aca8331a4e653c8e",
            "0e3728751dd64e54805c77535d94d2f7",
            "2dd9e0ea69c9412ebf73d3c95ae8cde9",
            "de1f3d16e54641beb88e09c684ce9620",
            "969bfaaf0be54a2c988c9f9427cc8fe0",
            "824dfa6e502f438881cf92b5af56e539",
            "06230f69fa9744ce924b4a66a23271fd",
            "2ecedbb572b64a31816e66b9a34887af",
            "a677da35c20848c2b43094a8661b01ef",
            "1d37cd8cd4d54cd4b28fbdd3292884f9",
            "d3c4c95ae8e04f6aa9c6c524a65887a4",
            "5c65494958fe4e96984b3970fac6dabf",
            "36425c282ee2478ca18c7cd991a0d934",
            "0cd4b5678bf147b2ac79ba5e03dde51e",
            "a22cc9d192ec4ab7a95616dcae7c2665",
            "67361ac6d51a418189534c771f3d3575",
            "84c56184b94240c19cfec0fe2a178de3",
            "b61d1c2e157549eeabfae7d10124c859",
            "2653723609fd43558362736e65a8a088",
            "7555f76df2dc4d16a69f54e710ac2583",
            "7fc68cfab7c841848250adee7a783de9",
            "e2e944f994324aaa9114514144e7fd2b",
            "89e3f1d506f2422298fb0aadac865f80",
            "11d5a999f8c1407d9eccf43d5677b2bb",
            "277bbc77e60b404084c6a645c90ff73d",
            "aaa06f65d43647a185dee7b27d8ccc52",
            "84591fca7a6f4984afa29b5f89380b46",
            "3cec7581770845789cab77208cabba06",
            "cb43fea2761b45f1a932e5fd733f00e4",
            "d45c2464c1284713bce80194d4a2c428",
            "642fad79cdeb41bb9baf1a2a74893099",
            "dccbc3da4858466299660caa03f83a0a",
            "632d0063134c4e3aa2dec32a8f421622",
            "887c1f7b6816494ab587166825469d90",
            "c8e5754fd1214eb68b5523399e5be36b",
            "17d584c739fa445085ef2b2c8da365f6",
            "70b8269de47f4889886b1eb2b4494a46",
            "12b2aaa3d8854499986e6cfd142b63c9",
            "b61e573e23ee4e6fa7d3726d7a7429b8",
            "43441493657a4904b955ea33ee16f5cc",
            "f4ee67562cf342b19f2ad01729ed2265",
            "a80ec49b27084677add210c934e95f6a",
            "9800a3d34fbe41e7856b0f667b8358a4",
            "1b52100de3ab41c5a4e8af0f5dc99317",
            "2e01850b1ea44bb1b546cce66ff354b4",
            "dcb101df6e574a32ba3f8198d4471791",
            "61220d0c909c40bcb421029baab3b419",
            "eb3b103988e945eb8bc4aa9deccd7200",
            "f64dcc3cc9ba4984aca46e7e74d69bc6",
            "61ab91d5b3b740c79b6af30919dbe803",
            "54ec7b73e102402ebc6addf28a290989",
            "1d8f1aaa65034d06a67920c4a783a54e",
            "fc08afee7c8244a096cedbce06aa85db",
            "140db1b576d44ca382a8073088795acd",
            "323134458ed24ec6bd612ddb7164c12f",
            "03e892e6298b4149b5cacda7e5c73258",
            "172e409b82294fceaa868f0a64b35d01",
            "c4fb386365f543b3aa41b9f5bd93cadd",
            "c46b4f39bafa4f098b2332f5d8398a3a",
            "f69c8cbc4db8498bbe16194484a32cb7",
            "bb242e8c74814eacac1b57fb19d077cc",
            "5402a10b92ac467899ea2710c23fc8ec",
            "920f22381dc4437ca61cacbe4d05cfad",
            "91cd165048444d2baf2e3547df210c8d",
            "2df5f7d602ad466a848b045c4b0b5c27",
            "507d25948b8e4e5bbbfe9a788b188ed1",
            "53ca7e651f5743dfae5eab7baf3cee8b",
            "1737c159f8e34d95a5fd78d11f24277f",
            "0743e1e37fe34718bada1398825a46dc",
            "651da781707841e7928d12d6ed450275",
            "74e4d97c32014e35a52f7bba2a3c12fe",
            "8c160f113fcb42f0beeb3fdd3ccf8dfc",
            "d4071aa8c4cf4132a95f8187cea24b98",
            "73dcf951f7f34ae9bbb4578bda3b407f",
            "e3fcce3eaa204cb694348325eb7ae348",
            "44d0874fca6b47d6a6ed759e64a54ce7",
            "b5088775413a4097b225957644b625fa",
            "719a119323064ade9e09626e5983db0f",
            "315d37c6faa34e46b675d8fbe7cc64a6",
            "6c109cea45534182b9cd3c340c6588cd",
            "6f3b9112d3f34f17b6cfcc029ad332b5",
            "d37d41df94ca441ab37460229bc53243",
            "7d536b0fa7ed4ea59c9382731f30be35",
            "ef8b582dc74c41aa9f58a0ececbed47f",
            "fb10971a81ee4e8b960aae7ed61f9bdb",
            "cb8da2e38cf7426ab41ce7cce740efa0"
          ]
        },
        "id": "TUH2HQmW970_",
        "outputId": "91a42ba3-2d34-4870-81fb-da806f89e2e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e617ae50a4ee4bf68f35a674098e0f10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b406f345bdad48d58dc17f449c23ecdc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0dcf42b081e94bdbbb01905096a2422c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8c5b6810fe1437bb3be463e9e75ef68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.01G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1132405d6f348258d1a2b19655c7eed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "788bd2eb5511497b95a685c20f18b12e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a20187dc8b3441df91b2ef91eb4ef71b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 12,582,912 || all params: 571,797,504 || trainable%: 2.2006\n",
            "[Sanity] Trainable params: 12,582,912\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0b6a62c9040428cbf9e79f04028ef36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51719be44bb44ab1be6c86e2ca208b17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e9f109bb64548fa99340e3e05a5ad69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b49391716f244e2b18b565cc53357b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2408d0e9f07a4457b8badefd8f126f46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f753c3504cf44e499f6203ba79c9815"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d1db4562cfe48c19608c67c5e9bbf3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3672 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b7cbe3c68414b9f8659c5025e67b213"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/44 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30ed46f264e44d1d82415847ae3dac68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3672 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8714a4d6e844ebcba314f83ba0e8205"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/44 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c70444191eab448ba5f8c3636c415c23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [45/45 06:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>963277.875000</td>\n",
              "      <td>3489.836182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>221281.437500</td>\n",
              "      <td>1986.615234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>221281.437500</td>\n",
              "      <td>1723.510620</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc753cbb1d364dd0b6ed184d703972af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00000-of-00003.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5c469e58a8642da8b0675d2984eb17d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00001-of-00003.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d37cd8cd4d54cd4b28fbdd3292884f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00002-of-00003.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fc68cfab7c841848250adee7a783de9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/validation-00000-of-00001.parquet:   0%|          | 0.00/474k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dccbc3da4858466299660caa03f83a0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/test-00000-of-00001.parquet:   0%|          | 0.00/509k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9800a3d34fbe41e7856b0f667b8358a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "140db1b576d44ca382a8073088795acd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2df5f7d602ad466a848b045c4b0b5c27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44d0874fca6b47d6a6ed759e64a54ce7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== KD Ergebnisse (Student) ===\n",
            "             teacher_id             student_id  kd_temperature  kd_alpha  \\\n",
            "0  bigscience/bloom-3b  bigscience/bloom-560m             2.5       0.7   \n",
            "\n",
            "   block_size  epochs         ppl      bleu  tokens_out  \\\n",
            "0        1024       3  329.866691  1.197423          96   \n",
            "\n",
            "                                               notes  time_s_total  \\\n",
            "0  GPU=NVIDIA A100-SXM4-40GB, VRAM=39.6 GB; Teach...    588.259584   \n",
            "\n",
            "   energy_kwh_total  co2_kg_total  kg_per_kwh  \n",
            "0          0.026248      0.012357    0.470783  \n",
            "\n",
            "=== Phasen (Zeit/Energie/CO2) ===\n",
            "        phase      time_s  energy_kwh    co2_kg\n",
            "0      train  541.466463    0.024993  0.011766\n",
            "1   eval_ppl    4.675794    0.000124  0.000058\n",
            "2  eval_bleu   35.884409    0.000963  0.000454\n",
            "3        gen    6.232917    0.000168  0.000079\n",
            "Gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_kd/bloom_kd_results.csv\n"
          ]
        }
      ]
    }
  ]
}