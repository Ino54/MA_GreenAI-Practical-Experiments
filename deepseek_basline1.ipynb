{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNOb6sF0KEA8KevxZ2tfSt5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ino54/MA_GreenAI-Practical-Experiments/blob/main/deepseek_basline1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZzdF1O8MKeC",
        "outputId": "ba6ae86a-41d2-49bd-8703-480ca72e25f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "# --- Requirements schreiben & installieren\n",
        "%%writefile requirements.txt\n",
        "transformers\n",
        "accelerate\n",
        "bitsandbytes\n",
        "datasets\n",
        "evaluate\n",
        "sacrebleu\n",
        "codecarbon>=2.5,<3\n",
        "pynvml>=11.5.0\n",
        "psutil\n",
        "numpy\n",
        "pandas\n",
        "huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirements.txt\n",
        "# störende Konflikte entfernen\n",
        "!pip uninstall -y -q google-genai firebase-admin || true\n",
        "!pip show fief-client || echo \"fief-client nicht installiert\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zNwL5v5MgK0",
        "outputId": "dd3eb3d8-732e-4c9c-e7cf-19983322da35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.6/517.6 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.34.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mName: fief-client\n",
            "Version: 0.20.0\n",
            "Summary: Fief Client for Python\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: François Voron <contact@fief.dev>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: httpx, jwcrypto\n",
            "Required-by: codecarbon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Google Drive mounten ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Projektordner setzen (wie bei BLOOM-Baseline) ---\n",
        "import os, pathlib, re\n",
        "project_path = \"/content/drive/MyDrive/LLM-Effizienz/4_2_Baseline\"\n",
        "pathlib.Path(project_path).mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(project_path)\n",
        "print(\"Arbeitsordner:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXoWa1Oc0YfQ",
        "outputId": "bfcbd265-95e1-47b0-97d9-b0417195f56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Arbeitsordner: /content/drive/MyDrive/LLM-Effizienz/4_2_Baseline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HOHyxD0C0YbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# DeepSeek-7B Baseline INT8 & INT4 (deepseek-ai/deepseek-llm-7b-base)\n",
        "# - gleiche Messlogik wie BLOOM-Baseline (gen/ppl/bleu pro Phase)\n",
        "# - Standort: GCP europe-west10 (Berlin) / DEU\n",
        "# - CSVs: baseline_deepseek7b_int8*.csv & baseline_deepseek7b_int4*.csv\n",
        "# ===========================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Hugging Face Login via Colab-Secret (optional) ---\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(hf_token)\n",
        "    print(\"Hugging Face Login erfolgreich!\")\n",
        "else:\n",
        "    print(\"WARNUNG: Kein HF_TOKEN gefunden – öffentliche Modelle meist trotzdem ladbar.\")\n",
        "\n",
        "# --- Imports & Setup ---\n",
        "import warnings; warnings.filterwarnings(\"ignore\")\n",
        "import time, math, gc, platform, inspect\n",
        "from dataclasses import dataclass, asdict\n",
        "from contextlib import nullcontext\n",
        "from typing import Optional, Tuple\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import psutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
        "from codecarbon import EmissionsTracker, __version__ as cc_ver\n",
        "\n",
        "print(\"CodeCarbon-Version:\", cc_ver)\n",
        "\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_total_gb = torch.cuda.get_device_properties(0).total_memory/(1024**3)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.set_grad_enabled(False)\n",
        "else:\n",
        "    gpu_name = \"CPU\"\n",
        "    vram_total_gb = 0.0\n",
        "\n",
        "print(f\"Device: {device} | GPU: {gpu_name} | VRAM: {vram_total_gb:.1f} GB | Torch {torch.__version__} | Python {platform.python_version()}\")\n",
        "\n",
        "# ========== Standort-Konfiguration ==========\n",
        "USE_GCP_REGION = True  # True => google/europe-west10 (Berlin), False => deutscher Strommix (DEU)\n",
        "COUNTRY_ISO_CODE = \"DEU\"\n",
        "CLOUD_PROVIDER   = \"google\"\n",
        "CLOUD_REGION     = \"europe-west10\"  # Berlin\n",
        "\n",
        "# -------- CodeCarbon Helfer & Fallbacks --------\n",
        "import os as _os, time as _time\n",
        "\n",
        "def tracker_kwargs_base():\n",
        "    base = dict(log_level=\"error\", output_dir=\".\")\n",
        "    try:\n",
        "        sig = inspect.signature(EmissionsTracker.__init__)\n",
        "        if \"measure_power_secs\" in sig.parameters:\n",
        "            base[\"measure_power_secs\"] = 1\n",
        "        if \"tracking_mode\" in sig.parameters:\n",
        "            base[\"tracking_mode\"] = \"process\"\n",
        "        if USE_GCP_REGION:\n",
        "            if \"cloud_provider\" in sig.parameters:\n",
        "                base[\"cloud_provider\"] = CLOUD_PROVIDER\n",
        "            if \"cloud_region\" in sig.parameters:\n",
        "                base[\"cloud_region\"] = CLOUD_REGION\n",
        "            if \"country_iso_code\" in sig.parameters:\n",
        "                base[\"country_iso_code\"] = COUNTRY_ISO_CODE\n",
        "        else:\n",
        "            if \"country_iso_code\" in sig.parameters:\n",
        "                base[\"country_iso_code\"] = COUNTRY_ISO_CODE\n",
        "            if \"cloud_provider\" in sig.parameters:\n",
        "                base[\"cloud_provider\"] = None\n",
        "            if \"cloud_region\" in sig.parameters:\n",
        "                base[\"cloud_region\"] = None\n",
        "    except Exception:\n",
        "        pass\n",
        "    return base\n",
        "\n",
        "def make_tracker_named(project_name: str, output_file: str):\n",
        "    cache_dir = f\"/content/.codecarbon_cache_{project_name}_{int(_time.time())}\"\n",
        "    _os.environ[\"CODECARBON_CACHE_DIR\"] = cache_dir\n",
        "    # evtl. alten Lock entfernen (best effort)\n",
        "    for d in (_os.path.expanduser(\"~/.codecarbon\"), \"/content/.codecarbon\"):\n",
        "        lock_file = _os.path.join(d, \"codecarbon.lock\")\n",
        "        if os.path.exists(lock_file):\n",
        "            try: os.remove(lock_file)\n",
        "            except: pass\n",
        "    return EmissionsTracker(project_name=project_name, output_file=output_file, **tracker_kwargs_base())\n",
        "\n",
        "def safe_start(tracker):\n",
        "    try:\n",
        "        tracker.start(); return True\n",
        "    except Exception as e:\n",
        "        print(f\"[CodeCarbon] Start fehlgeschlagen: {e} → Fallback 0/0.\"); return False\n",
        "\n",
        "def safe_stop(tracker, started: bool):\n",
        "    if not started:\n",
        "        return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "    try:\n",
        "        return tracker.stop()\n",
        "    except Exception as e:\n",
        "        print(f\"[CodeCarbon] Stop fehlgeschlagen: {e} → Fallback 0/0.\")\n",
        "        return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "\n",
        "def unpack_emissions(em):\n",
        "    if em is None:\n",
        "        return 0.0, 0.0\n",
        "    if hasattr(em, \"energy_consumed\") and hasattr(em, \"emissions\"):\n",
        "        try: return float(em.energy_consumed), float(em.emissions)\n",
        "        except Exception: pass\n",
        "    if isinstance(em, dict):\n",
        "        e = em.get(\"energy_consumed\", 0.0); c = em.get(\"emissions\", em.get(\"emissions_kg\", 0.0))\n",
        "        try: return float(e), float(c)\n",
        "        except Exception: return 0.0, 0.0\n",
        "    try:\n",
        "        return 0.0, float(em)\n",
        "    except Exception:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "def read_energy_from_log(path: str) -> float:\n",
        "    try:\n",
        "        if not os.path.exists(path):\n",
        "            return 0.0\n",
        "        df = pd.read_csv(path)\n",
        "        for c in [\"energy_consumed\",\"energy_consumed_kwh\",\"energy_consumed (kWh)\",\"energy (kWh)\"]:\n",
        "            if c in df.columns:\n",
        "                return float(df[c].iloc[-1])\n",
        "        for c in df.columns:\n",
        "            n = c.lower()\n",
        "            if \"energy\" in n and \"kwh\" in n:\n",
        "                return float(df[c].iloc[-1])\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def measure_phase(phase_name: str, fn, log_prefix: str):\n",
        "    logfile = f\"{log_prefix}_{phase_name}.csv\"\n",
        "    tracker = make_tracker_named(project_name=f\"{log_prefix}_{phase_name}\", output_file=logfile)\n",
        "    started = safe_start(tracker)\n",
        "    t0 = time.time()\n",
        "    result = fn()\n",
        "    t1 = time.time()\n",
        "    em_raw = safe_stop(tracker, started)\n",
        "    energy_kwh, co2_kg = unpack_emissions(em_raw)\n",
        "    if not energy_kwh or energy_kwh == 0.0:\n",
        "        energy_from_csv = read_energy_from_log(logfile)\n",
        "        if energy_from_csv:\n",
        "            energy_kwh = energy_from_csv\n",
        "    return {\"phase\": phase_name, \"time_s\": t1 - t0, \"energy_kwh\": energy_kwh, \"co2_kg\": co2_kg}, result\n",
        "\n",
        "# --- Evaluation-Config (wie BLOOM) ---\n",
        "MODEL_ID = \"deepseek-ai/deepseek-llm-7b-base\"\n",
        "ALIASES = {\"int8\": \"ds7b_int8\", \"int4\": \"ds7b_int4\"}\n",
        "\n",
        "EVAL = {\n",
        "    \"max_new_tokens\": 32,\n",
        "    \"ppl_dataset\":  {\"name\":\"wikitext\",\"config\":\"wikitext-2-raw-v1\",\"split\":\"test[:1%]\"},\n",
        "    \"bleu_dataset\": {\"name\":\"wmt14\",\"config\":\"de-en\",\"split\":\"test[:32]\"},\n",
        "}\n",
        "def parse_subset_count(split_str: str, default=32):\n",
        "    m = re.search(r\":\\s*(\\d+)\\s*\\]$\", split_str or \"\"); return int(m.group(1)) if m else default\n",
        "BLEU_N = parse_subset_count(EVAL[\"bleu_dataset\"][\"split\"], default=32)\n",
        "\n",
        "PROMPTS = [\n",
        "    \"Schreibe einen kurzen Absatz über nachhaltige KI.\",\n",
        "    \"Erkläre in einfachen Worten, was Quantisierung in neuronalen Netzen ist.\",\n",
        "    \"Nenne drei Vorteile von Mixture-of-Experts-Modellen.\"\n",
        "]\n",
        "\n",
        "def autocast_ctx():\n",
        "    return torch.autocast(device_type=\"cuda\", dtype=torch.float16) if device==\"cuda\" else nullcontext()\n",
        "\n",
        "def capture_memory():\n",
        "    ram = psutil.Process().memory_info().rss\n",
        "    valloc = torch.cuda.memory_allocated() if device==\"cuda\" else 0\n",
        "    vres  = torch.cuda.memory_reserved()  if device==\"cuda\" else 0\n",
        "    return ram, valloc, vres\n",
        "\n",
        "def bytes_to_gb(b): return float(b)/(1024**3)\n",
        "bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "@dataclass\n",
        "class BaselineResult:\n",
        "    model_id: str\n",
        "    alias: str\n",
        "    precision: str\n",
        "    time_s: float\n",
        "    energy_kwh: float\n",
        "    co2_kg: float\n",
        "    tokens_out: int\n",
        "    ram_GB: float\n",
        "    vram_alloc_GB: float\n",
        "    vram_reserved_GB: float\n",
        "    ppl: Optional[float] = None\n",
        "    bleu: Optional[float] = None\n",
        "    notes: str = \"\"\n",
        "\n",
        "# --- Utility ---\n",
        "def safe_max_len(tok, model, fallback=2048, upper=100000):\n",
        "    cand = getattr(tok, \"model_max_length\", None)\n",
        "    if isinstance(cand, int) and 0 < cand < upper: return cand\n",
        "    cand = getattr(getattr(model, \"config\", None), \"max_position_embeddings\", None)\n",
        "    if isinstance(cand, int) and 0 < cand < upper: return cand\n",
        "    return fallback\n",
        "\n",
        "# --- Laden erzwungen INT8 / INT4 (mit OOM-Fallback) ---\n",
        "def load_model_variant(model_id: str, variant: str):\n",
        "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    if tok.pad_token_id is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    tok.padding_side = \"left\"\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    if variant == \"int8\":\n",
        "        try:\n",
        "            bnb8 = BitsAndBytesConfig(load_in_8bit=True)\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_id,\n",
        "                device_map=\"auto\",\n",
        "                quantization_config=bnb8,\n",
        "                low_cpu_mem_usage=True,\n",
        "            )\n",
        "            return tok, model, \"int8\", \"erzwungen int8\"\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(\"[Info] OOM bei INT8 → Fallback auf INT4 (NF4).\")\n",
        "            else:\n",
        "                print(\"[Info] INT8 fehlgeschlagen → Fallback auf INT4 (NF4).\", repr(e))\n",
        "            # weiter zu INT4\n",
        "\n",
        "    # INT4 (NF4)\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "    bnb4 = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb4,\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "    note = \"erzwungen int4\" if variant==\"int4\" else \"int4 (Fallback)\"\n",
        "    return tok, model, \"int4\", note\n",
        "\n",
        "# --- Warmup ---\n",
        "def warmup(model, tok, max_len):\n",
        "    with torch.no_grad(), autocast_ctx():\n",
        "        dummy = tok(\"Hello\", return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
        "        _ = model.generate(**dummy, max_new_tokens=1, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "\n",
        "# --- Inferenz & Metriken ---\n",
        "def simple_generate(model, tok, prompts, max_new_tokens=32):\n",
        "    model.eval(); total_gen_tokens, texts = 0, []\n",
        "    max_len = safe_max_len(tok, model)\n",
        "    for p in prompts:\n",
        "        enc = tok(p, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
        "        input_ids = enc[\"input_ids\"].to(model.device)\n",
        "        attn = enc.get(\"attention_mask\", None)\n",
        "        if attn is not None: attn = attn.to(model.device)\n",
        "        room = max_len - input_ids.shape[1]\n",
        "        cur_new = max(1, min(max_new_tokens, int(room)))\n",
        "        with torch.no_grad(), autocast_ctx():\n",
        "            out_ids = model.generate(\n",
        "                input_ids=input_ids, attention_mask=attn,\n",
        "                max_new_tokens=cur_new, do_sample=False,\n",
        "                pad_token_id=tok.eos_token_id\n",
        "            )\n",
        "        gen_len = out_ids.shape[1] - input_ids.shape[1]\n",
        "        total_gen_tokens += int(gen_len)\n",
        "        texts.append(tok.decode(out_ids[0], skip_special_tokens=True))\n",
        "    return texts, total_gen_tokens\n",
        "\n",
        "def eval_perplexity(model, tok, ds_cfg):\n",
        "    ds = load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    max_len = safe_max_len(tok, model); losses = []\n",
        "    with torch.no_grad():\n",
        "        for t in ds[\"text\"]:\n",
        "            if not isinstance(t, str) or len(t.strip()) < 4: continue\n",
        "            enc = tok(t, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
        "            ids = enc[\"input_ids\"].to(model.device)\n",
        "            with autocast_ctx(): out = model(ids, labels=ids)\n",
        "            losses.append(float(out.loss.detach().cpu()))\n",
        "    return math.exp(np.mean(losses)) if losses else None\n",
        "\n",
        "def eval_bleu_llm(model, tok, ds_cfg, max_new_tokens=32):\n",
        "    ds = load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    max_len = safe_max_len(tok, model); preds, refs = [], []\n",
        "    with torch.no_grad():\n",
        "        for ex in ds:\n",
        "            de, en = ex[\"translation\"][\"de\"], ex[\"translation\"][\"en\"]\n",
        "            prompt = f\"Translate to English:\\nGerman: {de}\\nEnglish:\"\n",
        "            inputs = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
        "            room = max_len - inputs[\"input_ids\"].shape[1]\n",
        "            cur_new = max(1, min(max_new_tokens, int(room)))\n",
        "            with autocast_ctx():\n",
        "                out = model.generate(**inputs, max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "            gen = tok.decode(out[0], skip_special_tokens=True)\n",
        "            seg = gen.split(\"English:\")[-1].strip()\n",
        "            hyp = seg.split(\"\\n\")[0].strip() or gen.strip()\n",
        "            preds.append(hyp); refs.append([en])\n",
        "    return float(bleu_metric.compute(predictions=preds, references=refs)[\"score\"])\n",
        "\n",
        "# --- Baseline-Lauf (per Phase) für eine Variante ---\n",
        "def run_variant_baseline(model_id: str, alias: str, variant: str):\n",
        "    print(f\"\\n### Starte Baseline (per Phase) [{variant}]: {alias}\")\n",
        "    tok, model, prec, note = load_model_variant(model_id, variant)\n",
        "    max_len = safe_max_len(tok, model)\n",
        "    warmup(model, tok, max_len)  # Warm-up außerhalb der Messung\n",
        "    log_prefix = f\"deepseek7b_{alias}\"\n",
        "\n",
        "    def _do_gen(): return simple_generate(model, tok, PROMPTS, EVAL[\"max_new_tokens\"])\n",
        "    gen_metrics, (examples, tokens_out) = measure_phase(\"gen\", _do_gen, log_prefix)\n",
        "\n",
        "    def _do_ppl(): return eval_perplexity(model, tok, EVAL[\"ppl_dataset\"])\n",
        "    ppl_metrics, ppl = measure_phase(\"ppl\", _do_ppl, log_prefix)\n",
        "\n",
        "    if device == \"cuda\": torch.cuda.empty_cache()\n",
        "    def _do_bleu(): return eval_bleu_llm(model, tok, EVAL[\"bleu_dataset\"], EVAL[\"max_new_tokens\"])\n",
        "    bleu_metrics, bleu = measure_phase(\"bleu\", _do_bleu, log_prefix)\n",
        "\n",
        "    total_time   = gen_metrics[\"time_s\"] + ppl_metrics[\"time_s\"] + bleu_metrics[\"time_s\"]\n",
        "    total_energy = gen_metrics[\"energy_kwh\"] + ppl_metrics[\"energy_kwh\"] + bleu_metrics[\"energy_kwh\"]\n",
        "    total_co2    = gen_metrics[\"co2_kg\"] + ppl_metrics[\"co2_kg\"] + bleu_metrics[\"co2_kg\"]\n",
        "\n",
        "    ram, valloc, vres = capture_memory()\n",
        "\n",
        "    per_phase_df = pd.DataFrame([gen_metrics, ppl_metrics, bleu_metrics])\n",
        "    per_phase_df[\"alias\"] = alias\n",
        "    per_phase_df[\"model_id\"] = model_id\n",
        "    per_phase_df[\"precision\"] = prec\n",
        "    per_phase_df[\"tokens_out\"] = [tokens_out, None, None]\n",
        "    per_phase_df[\"ppl\"] = [None, ppl, None]\n",
        "    per_phase_df[\"bleu\"] = [None, None, bleu]\n",
        "\n",
        "    # abgeleitete Kennzahlen\n",
        "    per_phase_df[\"wh_total\"] = per_phase_df[\"energy_kwh\"] * 1000.0\n",
        "    per_phase_df[\"tokens_s\"] = None\n",
        "    per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"tokens_s\"] = (\n",
        "        per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"tokens_out\"]\n",
        "        / per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"time_s\"]\n",
        "    )\n",
        "    per_phase_df[\"wh_per_token\"] = None\n",
        "    per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"wh_per_token\"] = (\n",
        "        per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"wh_total\"]\n",
        "        / per_phase_df.loc[per_phase_df[\"phase\"]==\"gen\", \"tokens_out\"]\n",
        "    )\n",
        "    per_phase_df[\"s_per_example\"] = None\n",
        "    per_phase_df.loc[per_phase_df[\"phase\"]==\"bleu\", \"s_per_example\"] = (\n",
        "        per_phase_df.loc[per_phase_df[\"phase\"]==\"bleu\", \"time_s\"] / float(BLEU_N)\n",
        "    )\n",
        "    per_phase_df[\"wh_per_example\"] = None\n",
        "    per_phase_df.loc[per_phase_df[\"phase\"]==\"bleu\", \"wh_per_example\"] = (\n",
        "        per_phase_df.loc[per_phase_df[\"phase\"]==\"bleu\", \"wh_total\"] / float(BLEU_N)\n",
        "    )\n",
        "    per_phase_df[\"kg_per_kwh\"] = (per_phase_df[\"co2_kg\"] / per_phase_df[\"energy_kwh\"]).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    print(f\"\\nPer-Phase ({alias}) — Standort:\",\n",
        "          f\"GCP {CLOUD_REGION}\" if USE_GCP_REGION else f\"Ländermix {COUNTRY_ISO_CODE}\")\n",
        "    print(per_phase_df[[\n",
        "        \"phase\",\"time_s\",\"energy_kwh\",\"co2_kg\",\"kg_per_kwh\",\n",
        "        \"tokens_out\",\"ppl\",\"bleu\",\"tokens_s\",\"wh_per_token\",\"s_per_example\",\"wh_per_example\"\n",
        "    ]])\n",
        "\n",
        "    res = BaselineResult(\n",
        "        model_id=model_id, alias=alias, precision=prec,\n",
        "        time_s=total_time, energy_kwh=total_energy, co2_kg=total_co2,\n",
        "        tokens_out=int(tokens_out),\n",
        "        ram_GB=bytes_to_gb(ram), vram_alloc_GB=bytes_to_gb(valloc), vram_reserved_GB=bytes_to_gb(vres),\n",
        "        ppl=ppl, bleu=bleu, notes=f\"{note}; GPU={gpu_name}, VRAM={vram_total_gb:.1f} GB\"\n",
        "    )\n",
        "    return res, examples, per_phase_df\n",
        "\n",
        "# --- Ausführen für INT8 und INT4 ---\n",
        "variants = [\"int8\", \"int4\"]\n",
        "for variant in variants:\n",
        "    alias = ALIASES[variant]\n",
        "    res, ex, phase_df = run_variant_baseline(MODEL_ID, alias, variant)\n",
        "\n",
        "    # ---- Speichern (CSV + Beispiele) ----\n",
        "    out_dir = project_path\n",
        "    df = pd.DataFrame([asdict(res)])\n",
        "    df[\"kg_per_kwh\"] = (df[\"co2_kg\"] / df[\"energy_kwh\"]).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    res_csv   = os.path.join(out_dir, f\"baseline_deepseek7b_{variant}_results.csv\")\n",
        "    phase_csv = os.path.join(out_dir, f\"baseline_deepseek7b_{variant}_per_phase.csv\")\n",
        "    df.to_csv(res_csv, index=False)\n",
        "    phase_df.to_csv(phase_csv, index=False)\n",
        "    print(\"\\nErgebnisse (gesamt):\")\n",
        "    print(df[[\"model_id\",\"alias\",\"precision\",\"time_s\",\"energy_kwh\",\"co2_kg\",\"kg_per_kwh\",\n",
        "              \"tokens_out\",\"ppl\",\"bleu\",\"ram_GB\",\"vram_alloc_GB\",\"vram_reserved_GB\",\"notes\"]])\n",
        "    print(\"Gespeichert (gesamt):\", res_csv)\n",
        "    print(\"Gespeichert (per Phase):\", phase_csv)\n",
        "\n",
        "    samples_path = os.path.join(out_dir, f\"baseline_samples_{alias}.txt\")\n",
        "    with open(samples_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, txt in enumerate(ex, 1):\n",
        "            f.write(f\"--- Beispiel {i} ({alias}) ---\\n{txt}\\n\\n\")\n",
        "    print(\"Beispiele gespeichert:\", samples_path)\n",
        "\n",
        "    print(\"\\nEmissions-Logs (pro Phase):\")\n",
        "    print(f\" - deepseek7b_{alias}_gen.csv\")\n",
        "    print(f\" - deepseek7b_{alias}_ppl.csv\")\n",
        "    print(f\" - deepseek7b_{alias}_bleu.csv\")\n"
      ],
      "metadata": {
        "id": "ZyxFt-isMLCV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}