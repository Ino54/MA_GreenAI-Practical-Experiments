{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ino54/MA_GreenAI-Practical-Experiments/blob/main/bloom_quantisierung.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GTZI2k273OY",
        "outputId": "29c34e09-8fe4-4594-e5e3-55b3cd049cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "transformers\n",
        "accelerate\n",
        "bitsandbytes\n",
        "datasets\n",
        "evaluate\n",
        "sacrebleu\n",
        "codecarbon>=2.5,<3\n",
        "pynvml>=11.5.0\n",
        "psutil\n",
        "numpy\n",
        "pandas\n",
        "huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -r requirements.txt\n",
        "!pip uninstall -y -q google-genai firebase-admin || true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMVIrTfz8ACB",
        "outputId": "85a09b72-abd9-4e7b-ef68-fa9cc653f6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.6/517.6 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.34.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Google Drive mounten (ohne Ordneranlage) ---\n",
        "import os, shutil, time, pathlib\n",
        "from google.colab import drive\n",
        "MOUNTPOINT=\"/content/drive\"\n",
        "already=os.path.isdir(os.path.join(MOUNTPOINT,\"MyDrive\"))\n",
        "if not already and os.path.isdir(MOUNTPOINT) and os.listdir(MOUNTPOINT):\n",
        "    backup=f\"/content/drive_stale_{int(time.time())}\"\n",
        "    shutil.move(MOUNTPOINT, backup)\n",
        "    os.makedirs(MOUNTPOINT, exist_ok=True)\n",
        "drive.mount(MOUNTPOINT, force_remount=(not already))\n",
        "\n",
        "# --- Zielordner prüfen (muss existieren) ---\n",
        "work_dir=\"/content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_quantisierung\"\n",
        "if not os.path.isdir(work_dir):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Zielordner fehlt: {work_dir}\\n\"\n",
        "        \"Bitte diesen Ordner manuell in Google Drive anlegen und das Notebook erneut starten.\"\n",
        "    )\n",
        "os.chdir(work_dir)\n",
        "project_dir=work_dir\n",
        "print(\"Arbeitsordner:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWZxRl358Iy7",
        "outputId": "c85f6911-048b-4ac0-8dfb-a7c85de8458c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Arbeitsordner: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_quantisierung\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hugging Face Login via Colab-Secret  ---\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(hf_token); print(\"Hugging Face Login erfolgreich!\")\n",
        "else:\n",
        "    print(\"WARNUNG: Kein HF_TOKEN gefunden.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DkO6gtiPNL8",
        "outputId": "be41251f-0ca9-4efa-e9f7-0a1ab3acd6de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face Login erfolgreich!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Imports & Setup -----------------\n",
        "import re, math, gc, platform, warnings, inspect\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np, pandas as pd, torch, psutil\n",
        "from contextlib import nullcontext\n",
        "from types import SimpleNamespace\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "RESULT_BASENAME = \"bloom_quant\"\n",
        "set_seed(42)\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device==\"cuda\":\n",
        "  gpu_name=torch.cuda.get_device_name(0)\n",
        "  vram_total_gb=torch.cuda.get_device_properties(0).total_memory/(1024**3)\n",
        "  torch.backends.cuda.matmul.allow_tf32=True\n",
        "else:\n",
        "  gpu_name=\"CPU\"; vram_total_gb=0.0\n",
        "print(f\"Device: {device} | GPU: {gpu_name} | VRAM={vram_total_gb:.1f} GB | Torch {torch.__version__} | Py {platform.python_version()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCyNluQ_8evl",
        "outputId": "195b2e29-abfa-4122-9457-ac33bda375a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | GPU: NVIDIA A100-SXM4-40GB | VRAM=39.6 GB | Torch 2.8.0+cu126 | Py 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- CodeCarbon ----\n",
        "def _cc_supported_kwargs():\n",
        "    base=dict(log_level=\"error\", output_dir=\".\", measure_power_secs=1, tracking_mode=\"process\")\n",
        "    try:\n",
        "        params=inspect.signature(EmissionsTracker.__init__).parameters\n",
        "        if \"cloud_provider\" in params: base[\"cloud_provider\"]=\"google\"\n",
        "        if \"cloud_region\"  in params: base[\"cloud_region\"]=\"europe-west10\"  # GCP-Region\n",
        "        if \"country_iso_code\" in params: base[\"country_iso_code\"]=\"DEU\"\n",
        "    except Exception: pass\n",
        "    return base\n",
        "def make_tracker(name,out): return EmissionsTracker(project_name=name, output_file=out, **_cc_supported_kwargs())\n",
        "def safe_start(tr):\n",
        "    try: tr.start(); return True\n",
        "    except Exception as e: print(\"[CodeCarbon] Start fehlgeschlagen:\", e); return False\n",
        "def safe_stop(tr, started):\n",
        "    if not started: return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "    try: return tr.stop()\n",
        "    except Exception as e: print(\"[CodeCarbon] Stop fehlgeschlagen:\", e); return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "def unpack(em):\n",
        "    if hasattr(em,\"energy_consumed\") and hasattr(em,\"emissions\"):\n",
        "        try: return float(em.energy_consumed), float(em.emissions)\n",
        "        except: pass\n",
        "    if isinstance(em, dict):\n",
        "        e=em.get(\"energy_consumed\",0.0); c=em.get(\"emissions\", em.get(\"emissions_kg\",0.0))\n",
        "        try: return float(e), float(c)\n",
        "        except: return 0.0,0.0\n",
        "    try: return 0.0, float(em)\n",
        "    except: return 0.0,0.0\n",
        "def read_energy_from_log(path):\n",
        "    try:\n",
        "        if not os.path.exists(path): return 0.0\n",
        "        df=pd.read_csv(path)\n",
        "        for c in [\"energy_consumed\",\"energy_consumed_kwh\",\"energy_consumed (kWh)\",\"energy (kWh)\"]:\n",
        "            if c in df.columns: return float(df[c].iloc[-1])\n",
        "        for c in df.columns:\n",
        "            if \"energy\" in c.lower() and \"kwh\" in c.lower(): return float(df[c].iloc[-1])\n",
        "    except: pass\n",
        "    return 0.0\n",
        "def measure_phase(phase, fn, prefix):\n",
        "    logfile=os.path.join(project_dir, f\"{prefix}_{phase}.csv\")\n",
        "    tr=make_tracker(f\"{prefix}_{phase}\", logfile)\n",
        "    import time as _t\n",
        "    st=safe_start(tr); t0=_t.time(); res=fn(); t1=_t.time()\n",
        "    e=safe_stop(tr, st); ekwh, co2=unpack(e)\n",
        "    if ekwh==0.0:\n",
        "        ek=read_energy_from_log(logfile)\n",
        "        if ek: ekwh=ek\n",
        "    return {\"phase\":phase,\"time_s\":t1-t0,\"energy_kwh\":ekwh,\"co2_kg\":co2}, res"
      ],
      "metadata": {
        "id": "MdlaaHOE8ot_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Eval -----------------\n",
        "MODELS=[(\"bigscience/bloom-560m\",\"b560\"),(\"bigscience/bloom-3b\",\"b3b\")]\n",
        "EVAL={\"max_new_tokens\":32,\n",
        "      \"ppl\":{\"name\":\"wikitext\",\"config\":\"wikitext-2-raw-v1\",\"split\":\"test[:1%]\"},\n",
        "      \"bleu\":{\"name\":\"wmt14\",\"config\":\"de-en\",\"split\":\"test[:32]\"}}\n",
        "def parse_subset_count(s, default=32):\n",
        "  m=re.search(r\":\\s*(\\d+)\\s*\\]$\", s or \"\"); return int(m.group(1)) if m else default\n",
        "BLEU_N=parse_subset_count(EVAL[\"bleu\"][\"split\"], 32)\n",
        "PROMPTS=[\n",
        "  \"Schreibe einen kurzen Absatz über nachhaltige KI.\",\n",
        "  \"Erkläre in einfachen Worten, was Quantisierung in neuronalen Netzen ist.\",\n",
        "  \"Nenne drei Vorteile von Mixture-of-Experts-Modellen.\"\n",
        "]\n",
        "bleu_metric=evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# ----------------- Helpers -----------------\n",
        "def autocast_ctx():\n",
        "  return torch.autocast(device_type=\"cuda\", dtype=torch.float16) if device==\"cuda\" else nullcontext()\n",
        "def bytes_to_gb(b): return float(b)/(1024**3)\n",
        "def capture_memory():\n",
        "  ram=psutil.Process().memory_info().rss\n",
        "  valloc=torch.cuda.memory_allocated() if device==\"cuda\" else 0\n",
        "  vres =torch.cuda.memory_reserved()  if device==\"cuda\" else 0\n",
        "  return ram, valloc, vres\n",
        "def safe_max_len(tok, model, fallback=2048, upper=100000):\n",
        "  cand=getattr(tok,\"model_max_length\",None)\n",
        "  if isinstance(cand,int) and 0<cand<upper: return cand\n",
        "  cand=getattr(getattr(model,\"config\",None),\"max_position_embeddings\",None)\n",
        "  if isinstance(cand,int) and 0<cand<upper: return cand\n",
        "  return fallback\n",
        "def warmup(model, tok, max_len):\n",
        "  with torch.no_grad(), autocast_ctx():\n",
        "    x=tok(\"Hello\", return_tensors=\"pt\", truncation=True, max_length=max_len).to(model.device)\n",
        "    _=model.generate(**x, max_new_tokens=1, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "\n",
        "def do_gen(model, tok, max_new_tokens):\n",
        "  total, texts=0, []\n",
        "  ml=safe_max_len(tok, model)\n",
        "  for p in PROMPTS:\n",
        "    enc=tok(p, return_tensors=\"pt\", truncation=True, max_length=ml).to(model.device)\n",
        "    room=ml-enc[\"input_ids\"].shape[1]\n",
        "    cur_new=max(1, min(max_new_tokens, int(room)))\n",
        "    with torch.no_grad(), autocast_ctx():\n",
        "      out=model.generate(**enc, max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "    total+=int(out.shape[1]-enc[\"input_ids\"].shape[1])\n",
        "    texts.append(tok.decode(out[0], skip_special_tokens=True))\n",
        "  return texts, total\n",
        "\n",
        "def do_ppl(model, tok, ds_cfg):\n",
        "  ds=load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "  ml=safe_max_len(tok, model); losses=[]\n",
        "  with torch.no_grad():\n",
        "    for t in ds[\"text\"]:\n",
        "      if not isinstance(t,str) or len(t.strip())<4: continue\n",
        "      enc=tok(t, return_tensors=\"pt\", truncation=True, max_length=ml).to(model.device)\n",
        "      with autocast_ctx():\n",
        "        out=model(enc[\"input_ids\"], labels=enc[\"input_ids\"])\n",
        "      losses.append(float(out.loss.detach().cpu()))\n",
        "  return math.exp(np.mean(losses)) if losses else None\n",
        "\n",
        "def do_bleu(model, tok, ds_cfg, max_new_tokens):\n",
        "  ds=load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "  ml=safe_max_len(tok, model); preds, refs=[], []\n",
        "  with torch.no_grad():\n",
        "    for ex in ds:\n",
        "      de,en=ex[\"translation\"][\"de\"], ex[\"translation\"][\"en\"]\n",
        "      prompt=f\"Translate to English:\\nGerman: {de}\\nEnglish:\"\n",
        "      enc=tok(prompt, return_tensors=\"pt\", truncation=True, max_length=ml).to(model.device)\n",
        "      room=ml-enc[\"input_ids\"].shape[1]\n",
        "      cur_new=max(1, min(max_new_tokens, int(room)))\n",
        "      with autocast_ctx():\n",
        "        out=model.generate(**enc, max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "      gen=tok.decode(out[0], skip_special_tokens=True)\n",
        "      hyp=gen.split(\"English:\")[-1].strip().split(\"\\n\")[0].strip() or gen.strip()\n",
        "      preds.append(hyp); refs.append([en])\n",
        "  return float(bleu_metric.compute(predictions=preds, references=refs)[\"score\"])\n",
        "\n",
        "def load_quantized(model_id, variant):\n",
        "  tok=AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "  tok.padding_side=\"left\"; tok.pad_token=tok.eos_token\n",
        "  if variant==\"8bit\":\n",
        "    bnb=BitsAndBytesConfig(load_in_8bit=True)\n",
        "  elif variant==\"4bit\":\n",
        "    bnb=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "                           bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float16)\n",
        "  else:\n",
        "    raise ValueError(\"variant must be '8bit' or '4bit'\")\n",
        "  if device==\"cuda\": torch.cuda.empty_cache(); gc.collect()\n",
        "  model=AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=bnb)\n",
        "  return tok, model, variant\n",
        "\n",
        "def run_once(model_id, alias, variant):\n",
        "  tok, model, prec=load_quantized(model_id, variant)\n",
        "  ml=safe_max_len(tok, model); warmup(model, tok, ml)\n",
        "  prefix=f\"{RESULT_BASENAME}_{alias}_{variant}\"\n",
        "\n",
        "  m_gen,(samples,n_tok)=measure_phase(\"gen\",  lambda: do_gen(model,tok,EVAL[\"max_new_tokens\"]), prefix)\n",
        "  m_ppl, p            =measure_phase(\"ppl\",  lambda: do_ppl(model,tok,EVAL[\"ppl\"]),             prefix)\n",
        "  if device==\"cuda\": torch.cuda.empty_cache()\n",
        "  m_bleu, b          =measure_phase(\"bleu\", lambda: do_bleu(model,tok,EVAL[\"bleu\"],EVAL[\"max_new_tokens\"]), prefix)\n",
        "\n",
        "  time_s=m_gen[\"time_s\"]+m_ppl[\"time_s\"]+m_bleu[\"time_s\"]\n",
        "  energy=m_gen[\"energy_kwh\"]+m_ppl[\"energy_kwh\"]+m_bleu[\"energy_kwh\"]\n",
        "  co2   =m_gen[\"co2_kg\"]+m_ppl[\"co2_kg\"]+m_bleu[\"co2_kg\"]\n",
        "  ram=psutil.Process().memory_info().rss\n",
        "  valloc=torch.cuda.memory_allocated() if device==\"cuda\" else 0\n",
        "  vres =torch.cuda.memory_reserved()  if device==\"cuda\" else 0\n",
        "\n",
        "  row=dict(model_id=model_id, alias=alias, variant=variant, precision=prec,\n",
        "           time_s=time_s, energy_kwh=energy, co2_kg=co2,\n",
        "           kg_per_kwh=(co2/energy if energy else None),\n",
        "           tokens_out=int(n_tok), ppl=p, bleu=b,\n",
        "           ram_GB=ram/(1024**3), vram_alloc_GB=valloc/(1024**3), vram_reserved_GB=vres/(1024**3),\n",
        "           notes=f\"GPU={gpu_name}, VRAM={vram_total_gb:.1f} GB\")\n",
        "  return row, samples, (m_gen,m_ppl,m_bleu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7c07396874a34e8caf1e8c5c945554ad",
            "c9f1b9e1333b47b3bcf65417f68d153a",
            "4b0ab1d373c74a3bb249d74836a72a5b",
            "27560ba7908b407a9483da83fd7fd9fe",
            "ae261ac084504f929f75cc6c44cac4ab",
            "6b84550ca1254db1a4db0c42d1017726",
            "6c2efd6fab774ffa994ef3dda4380fb0",
            "299a94307c484694a099893fc6d34aef",
            "274437284dec4a93bdd0f02d402455bd",
            "47ec86baa51846ff8b997feb3ea2e16b",
            "403d7514594741cf85140719b2aa7019"
          ]
        },
        "id": "-JYNhi9h8zd7",
        "outputId": "eca7e899-8d90-4b71-ad1b-c9e28441985f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c07396874a34e8caf1e8c5c945554ad"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Ausführung und Speichern-----------------\n",
        "all_rows, phase_tables=[],[]\n",
        "for mid, alias in MODELS:\n",
        "  for variant in [\"8bit\",\"4bit\"]:\n",
        "    print(f\"\\n### {alias} – {variant}\")\n",
        "    row, samples, phases=run_once(mid, alias, variant)\n",
        "    all_rows.append(row)\n",
        "    dfp=pd.DataFrame(list(phases)); dfp[\"alias\"]=alias; dfp[\"variant\"]=variant\n",
        "    dfp[\"kg_per_kwh\"]=(dfp[\"co2_kg\"]/dfp[\"energy_kwh\"]).replace([np.inf,-np.inf],np.nan)\n",
        "    phase_tables.append(dfp)\n",
        "    with open(os.path.join(project_dir, f\"{RESULT_BASENAME}_samples_{alias}_{variant}.txt\"),\"w\",encoding=\"utf-8\") as f:\n",
        "      for i,t in enumerate(samples,1): f.write(f\"--- Beispiel {i} ---\\n{t}\\n\\n\")\n",
        "\n",
        "df=pd.DataFrame(all_rows)\n",
        "df_phase=pd.concat(phase_tables, ignore_index=True)\n",
        "df.to_csv(os.path.join(project_dir, f\"{RESULT_BASENAME}_results.csv\"), index=False)\n",
        "df_phase.to_csv(os.path.join(project_dir, f\"{RESULT_BASENAME}_results_per_phase.csv\"), index=False)\n",
        "print(\"\\nErgebnisse:\\n\", df)\n",
        "print(\"\\nPer-Phase:\\n\", df_phase)\n",
        "print(\"Gespeichert:\", os.path.join(project_dir, f\"{RESULT_BASENAME}_results.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5eb8c0d40bd0490b99727c7ba3b0301a",
            "47fca6633d7945b1811adf0c8ce13016",
            "8e90633df5784f048a1ddeed314030dc",
            "cb58bebbbf0a431eb704824805ec16ac",
            "05f6846576184791930096dfb3f9cefa",
            "7439749d6e1b4a7499754c2417abfe18",
            "e4baeb032ca942d1bda8b2806f14bfb9",
            "f8b511cc89794b688e729fb0ff895a2c",
            "f3032dfd7d7342e8ac8f5a6ff63f9ff5",
            "8621677b802f42b18b64802068f6d3bb",
            "cada70a9719d4e3097085e3537fd302f",
            "f053a55c59754cf4aef07ad0f2a09fc2",
            "4c215c8f12384208bebcb001b8ba1f29",
            "f847e4fd4b96486c855eda8f9ff70f8e",
            "034e342dae12434d9fa9946bda8e62ed",
            "8941f2847066427ca973dfe7ca8bd5a5",
            "91a36db568644f45a583a383ef0cb8a6",
            "1ef58763501c462aa0639c4e8c303857",
            "ff0976a20ebd408b85f676c1b5d747dc",
            "562353b49497455484ea555c896b0b21",
            "d013fc5168ba41c892cafe6235cd385e",
            "e568c3fa734d4c56b1651baf249a1c21",
            "52ffbd346ae24c2c8a88a55d296e8596",
            "3d9af259fac54726bb80a64da110b0f8",
            "81c4ce26e2ff48f299c91ab60092951c",
            "efbea3d3761d42b2a79a2edc7100b457",
            "f96a07ae0f814b9186362892b6e9edd2",
            "113a8f3e708c4a9092ff0fd108259314",
            "d859caa42e044002b8d8209aec662d64",
            "6bda29f3299f44eab686b04101911fc4",
            "0f17b505f4ae42c78e008b48545cef50",
            "cfeb538ec1f840f89eb864e2fc31c042",
            "68fa3ba8e5654b1b984d18ff0b92f877",
            "1d07f565aefe47a48e4c3de0ada847ed",
            "8e05fe490fe1477fbeffa88035f116e2",
            "9afb4195aac342e1ad12c6585ec1490f",
            "07ede2e70d7b47b080823a7a97452bb8",
            "a715a76205564f42ae3e9394c293cdc2",
            "2e7f578bbbca4e31974e6dfcb8295de2",
            "0de209a161814a958d55fe0670508e5f",
            "ac25527d542747469585fe7cb4da6064",
            "8b74a40d347641d0b1bb798abb1b334a",
            "f5c1641dd27c4c7bbe83396cfd30c9bc",
            "904d92eb5f844e90ac11bca9bc13a665",
            "f098fdf90644401097ede73c0362f747",
            "be2d1eada3a74ff38405872de6a965f4",
            "0f91e38575fe439881da25158b708812",
            "9da03b8d56e64fe697670acd6be5e698",
            "e3c810d9b5604277a028591574e4c7d3",
            "5b50080f9f40492285b44a660479dfa1",
            "1c693072f5704962a2feb347a55ccef6",
            "593ecaaa90aa468ba8bec5593c3f776d",
            "ee38bbd747b0456cb7024171f6bec948",
            "efe106b446fc4a72a772cf7d15ed5acf",
            "b2c24e68490240f5b9711258a6d6ec00",
            "47db726f42f0499ebf668c85439c1490",
            "58629df831c4423abc763d38058009fb",
            "7d2cb45dd0b3445e9173d2010077ef42",
            "17686ebd22a3450698f624d8db403eb7",
            "bfcfc1ebf4ae4b98ab4b5f7275c469f3",
            "712b4d639f3743c7b36aa1fac4dfe4a5",
            "0245e43ce98b4db6a3e054d6b8a74dd2",
            "e09cf0eb2dfa4e8ca8dda396e3ee0c43",
            "ca997497a6a84d1fb9a20f2341175357",
            "b105c689573b41f581563d9bbbc0f7a7",
            "f0ca373169b94ab2adb55f51b0722547",
            "4ee574fe2a644c58970bf0aae88bac2f",
            "9779314e2cf54daf8b0642e13f7e8d74",
            "a0ec07986d384879b52a621057a61653",
            "ea2d5f65bf0b423485497669265d4d1b",
            "394384e131ba4328be858e2217cbe196",
            "7bff8e9628ba46fcaf03c5706a4c8431",
            "be675a751fa841e5a9c68a15910e394d",
            "269c83d46aa54986b31e9f54c0c47508",
            "dc0a9262246b49459a93e56d5ba1143a",
            "72ff5fa716ea4f70a3c794f96671d223",
            "0cb88462a0c04514a6fdd15623ff7173",
            "ae186bf02e4b473e8bf02ac9a3a4d81f",
            "4f0e2c99c86747ba95de582546977238",
            "d4cb344d3c2147e880eccfc5485c201c",
            "66682b791d0d4ad98551b6ef3f6547d0",
            "a9df5f348bf0431f95abdda4ff571e96",
            "458fad0e4d9440b085c5315689d77262",
            "e3f18ae6a1e8408e9d0ee61df581edee",
            "467c0603309947d8b0d9b9c0d801f815",
            "6afdbf0c34d8486d87222cf0db2fc73a",
            "6550de3240ed458b8ea5b150ace76c78",
            "146e4d48650f4c12afc7b4816042a90a",
            "cf195022c3124fdcae480371663db313",
            "a4183d47e03c438f948670bcc2f18510",
            "f22d48ac567c48b8841878610539c1df",
            "dcbae20cb4ea4b6fab35c910b61db35e",
            "9bb51d2935234ca6b237dfc972f85ea4",
            "e92b2c3aad344f749862811b9ee0036f",
            "14eb0c169fd94ee699d742d4be032503",
            "94baa684d38444828b866d215307a547",
            "76163bce159d48a095189077ebe23e78",
            "62a223eff16c434ebdc88fdbdc198a14",
            "7395c1394e284de8892c44694cb13f6d",
            "d64e40156a3a489c8a4e2ee78a259db2",
            "7152b19d16fb41128b3ace85c65c7ea0",
            "10712c5c495545edb8603e968fb4f32e",
            "3c31ccdbf09b40c28e0a7ebcbbeffa3b",
            "d37163e9991c4d84ac960f6552707e00",
            "d2af3a4e6ece4be186dd6673db7875f8",
            "9d28ecd486cc43b2a1364e7f742bfc5b",
            "81e479383b6e4bce86240a826e0d5f87",
            "7ac7319517b848ef958f811cdb8b601c",
            "d16f7945c93c4ac9b512148ae1fdf4bd",
            "c4a91fd63f97472b9ef190bf2cb37ed5",
            "ee5892bef0ed41ad97c9eaae4ec854d2",
            "931a6ded3e7d4baf882b98fde5493b07",
            "d80d6c6b0ac842de80d0ab14d4164bc4",
            "5ef741dddd8b4e2fafe3c21909dcf4e6",
            "7de87bb0cfaf4594ab88540ab04bba75",
            "8c00935229454055a6f376f62d0fad29",
            "725b7759578944dea29e6394e949e53e",
            "a23008fa38d0422ca83f812f3e55b47d",
            "cf6020c7d401434e995ea8f9e79307c0",
            "7bc7cef1a6ec449badcf09337c544a2b",
            "ee706c7013004697ae19045510a67a2c",
            "5aa5cad8a5b845a39302d4f70ce645d7",
            "a0bba83744224d3ebc3ac2c774d5781c",
            "72aa2cdd3d0342bc95e21e5188788a9d",
            "543a2a7823664740a5f28c398933aa9f",
            "ef4a46098e8e42dc97086c69acb41034",
            "e1e2b78c1ded4acbac269a367bd34a29",
            "6889b8ac65eb4f94ad500dc03e8a476d",
            "970f8b8889994cf1b26b331f07656f0e",
            "1281b517be634d8cb4ac68779c4f122a",
            "5e3c8f5d450f4f9e801177b3082d7027",
            "b82a1d68c1834ed18d0c4735fed77b6f",
            "1e93f9e81a3e44a6b737400452ab0e04",
            "b759f1b0e07e4b33a320add3fb9a6c3a",
            "508ca55e05084a6198a4e502719811ec",
            "23a4e3a1b9fc47c0ba7e55bbf77099ee",
            "c719ca210edf4107ae44c899399f1804",
            "8460d3c505344bae827ef6d0f82ef0b9",
            "3de478195d604d9dbf2e713d380f0d43",
            "da657276a2e24cf1adb324e360f75910",
            "801d22c1d02c497b8702aa16b8b38b14",
            "444c63f6542f48b789674c766c4aa31e",
            "488c0067b13546b6b422848eaecdf05c",
            "7c2cb20d0f98435abc25495cf8529d7b",
            "af926ef4d3c84d3c825545d33d9e41f1",
            "81c3080eb74d43b79fdddd4a960f5d6d",
            "20b8fda2fd4443e8a540c3cf6614eefe",
            "99d95894953b446fb4d0544de0da4177",
            "4ab9649bc6e6425a9432a40146424ebf",
            "80ac3587cf4149f9a952b145c2c79126",
            "aea978661c4a41f0ac288fd66071809b",
            "fa5cf0b04094413cb605a67fc63d84f2",
            "26800cb3b8c3442189b1044fd15bb5d0",
            "7f691e226d4d4bebb67ad9f915b6c7c6",
            "a0f30e19315f473c8c74b79571f7ab51",
            "f8fcec5d08f24fd582f77e5d0ff60711",
            "93874cdb568345c7a82e8e13e78a0129",
            "d8444f8bb5e640fcbedcb92aff666333",
            "b3a897eeb623441e8f131e12df877db6",
            "a12c7ba403b9421287425667aca8ae8a",
            "c86e26ecca6c4a5e99c13ef6952e7d5a",
            "c57d12d0c53d4ad794f63fd9de2490b2",
            "fea64d88474d46258e6d20b495152547",
            "eddc82909a1d495aa30beaa68a5910ff",
            "6e545e34c53f4f48a6f13aa862ce5485",
            "038d22a096b949bb97e436d641a26744",
            "5a8bc4f304634c60ac8ce794072f2007",
            "7a432c37079a422f8759c101ef79565d",
            "49b0d9b632dc4833ae8e1927a5aae2d6",
            "7b8653fc084c47968a9401679d7a036f",
            "f645a1d2696b4043a04e9dc24f137bad",
            "7aa7d5e102ee4bb789e59f04e13c50e6",
            "ca94e0f243dc4458a28bff4955594ba3",
            "58364443c8d14eadb85e932fc587ebe4",
            "97f4ce651334444780fab6dd65d86deb",
            "aa612b793aac4e14b9f573969684e775",
            "10b15bcd033d45889e874c7f92eec1be",
            "cfedf02adc364061a0098efc06aa60de",
            "59e011490e1a404894e6a5d36e98e58f",
            "ca6c3aca64904201b3b608071d6becc4",
            "66a6f985320c4aa5abdec605ecaf0a2d",
            "47d5f610bf804290adcdee3b654f12fc",
            "c7e5b8e113834ba38af1507891f3330e",
            "5278a3286fa0410ba76f3095b5275c80",
            "aa2ef2f648e1429085fbbf3df1a6966b",
            "1129d29e8e0f471695b9e0602218ec58",
            "75a4db86ba8c4f0fad3ae424a49d2889",
            "0a98cdcc26674d3f8476866536b686eb",
            "80cbda7f8de54321ad1ccd32348e73ba",
            "7404923473da4bb1b76d68cff64338e0",
            "9800f5132eed4deb9208ec089cb00c7b",
            "25f0c12d082b45fb855ce97ca83772e7",
            "4e5ac47f946f4cbb9c71a494a9d8c038",
            "63489ef1c92a41f2a86615f5b6d72bdf",
            "cb1718e552b8481381ae0e796ec63e39",
            "adf87e37f0414699b3f548e59824bd49",
            "31ba3eeef51e48ff8e3868b54c22244d",
            "a6f1178641f043c0b65e9759be7e2ca2",
            "209ac90628a647f28cef5f133710be3f",
            "9e7b7419aded4f30abd5916e0c2cdc4c",
            "82976b2d544b49899e28297ee84bd77a",
            "aa32ff32a3e8443295838f3627cc44d6",
            "630c6af66e8f4dfbbabfda27bf05ccf8",
            "54ad6b4236be431bb0dafd867399859a",
            "19f66f622db24ed7b9e42019027102e1",
            "4af65b37c8b4488886ed5e0f7d0d411b",
            "19c15ab38fbd4e069208eed9864bfb3d",
            "b239a132d1fb4f4d935f339b8c2938b7",
            "db3c6926536f4c8b91e1d42ac0ee18e1",
            "1fd745d9b7a94c57bb891a8a1f16f18c",
            "e0542fb6b0c14f829c7c5dd222024421",
            "d334908beaa94d14976e6647914d3fb4",
            "7379bfa88f6f4ab7b3d36cbd4a76695f",
            "47dd2dcb58b44920a024033a9ed08989",
            "a1fbd30567d247d4bb4d91703b4e786f",
            "dfbb82d27d7f44c4a333d0d862c3b7f5",
            "71740530d7c04283a82fd62862dd19ed",
            "ce5b3c9ffde140b6ac207ffb7552e0a8",
            "b77c07a5404d4bcd844b8a67750cd5fd",
            "ae2d5dca7a9e407e9bbe8f7109c05f13",
            "643e1762b4264a18964c7e618a3b9e4f",
            "3389512004e44ec4bee3ea6dba81a573",
            "e36659d77c854ff5b5fcef4d79c9cbc5",
            "cc688191cc0e4a47b53649dc0e6a5edc",
            "47c1dd6d054840dcba8b17ec9b63d632",
            "3fa953a39e074351a15469abeec10b6b",
            "c8fa0d9a8f6a44749d4364046623f76a",
            "8a41a50263924ad1864e675ac0f5ccb8",
            "c7eb6e6b66d24767afdbfbc8fb8db34f",
            "b0bfd1c40ee048b9be7e9c16efb9634f",
            "d2d0d06360be486aac013abc647b35ca",
            "a17466836aaf4d12b6a5770a22cf9fb0",
            "350563557d374a798f5f7a860336e210",
            "2c788608d57e47fb9ab0de3cb66f685d",
            "8e87e2155fa64e7ca819f8b8f7403cb7",
            "d091e6e71630421a8143eed1a39676d5",
            "50ca1d3fa1dd4373ba32502b0b737c1a",
            "8979f6d6f9fc4b7aa5cb88be118ed5ab",
            "8a4c65ee959242ba8ab40b8f6e7e34d4",
            "5f62c44f66f347628ace228a49b3140f",
            "a54dcc43c20c4ca1a1f33851435b3c56",
            "e637c0b56e864b219c35e267b33a46c1",
            "cea5070834194c3c82bdd97f03e103f5",
            "b597cda04e184ea1a460d0c5b7419459",
            "9709fc515543463f9dd9cd5630cf5b9e",
            "a68454d4f06640d2bc1671d849bb2d29",
            "2a9d25b49aa84915a02de3e14e282143",
            "0afc47eeb88a44e2b5f741e860b7a9cc",
            "feb189cf70be46eaa4731616e9364088",
            "363da6b2612e47fba5e2093da72f4486",
            "dcf01454107b40a5a8b75f027bff646c",
            "f7129a508b0a406c852c14a834af5e78",
            "f83605c7a3614e969fcb9cf9bf971da4",
            "a183dfb01aaa441792dd369565da2363",
            "4255e69000644b86a333ea46f38bd40e",
            "7aee6370ecc046e6841a4a971c8c75fa",
            "ecf17be2e3364c888318f1253ca8d0c5",
            "167f5e51d3cd4ec4b93512552365621f",
            "07c41a4748b640e5bfaa5bf5b04a7cdd",
            "71f24caf902148958c0e0fd7609df9f7",
            "6214da13823a4131a4e87a4fd6ced18e",
            "ff39d72d46dc4faabcf8ec9198f35cf9",
            "8cfaea379c9d462e96e12f88ed954481",
            "3e081bbd4a2a4111b6d98d5a1cf9c08b",
            "da673d8350f64128bd8774bf43ca8e01",
            "9791baa7f2ff4717bd92579876adf6ca",
            "3bfbfeffdb8b46d6b0ebc8beba067006",
            "3502c81a6b71421785ab2280762b0985",
            "bb3cb59153e94a80927a02fced4e28a8",
            "95921184933f412a898ab9e7475b805f",
            "3a8a1ecae5fa454c8063d34a560d20f3",
            "81e36d382fe14ac6b32806de985f84b6",
            "daaef2b0ab1a4ec88d2a3793ee504734",
            "a75109b82ae840dba4a0c8d679305f3c",
            "e88ce9504f9f4dda95b8fd3d6cd4a7df",
            "21fe93c78e534190a3e7611c49464133",
            "d037f5b8e2614685960e05646946100b",
            "87796b7e58a44a6187d13d3f55b3546f",
            "7c5ea9b1a94b411a82f372eade7aabb7",
            "90b876e4a217462aa87f999497920796",
            "9b03e7ae254b4a60ad46e326db05e993",
            "ee2498f423094460a4f521a9f1e8dfff",
            "30f1b8b82837490d955bb17b9b1a797a",
            "24be2bf9743546bdb63e5c66bbdde5ae",
            "a785dc4789e74126aa026963ef49db96",
            "232ee30f277b4d2fb95c6e4ee5d59801"
          ]
        },
        "id": "aoDYFq4782vz",
        "outputId": "a6a844a8-1c2b-493d-db8b-ebc78ca89fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### b560 – 8bit\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5eb8c0d40bd0490b99727c7ba3b0301a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f053a55c59754cf4aef07ad0f2a09fc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52ffbd346ae24c2c8a88a55d296e8596"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d07f565aefe47a48e4c3de0ada847ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f098fdf90644401097ede73c0362f747"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47db726f42f0499ebf668c85439c1490"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ee574fe2a644c58970bf0aae88bac2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae186bf02e4b473e8bf02ac9a3a4d81f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf195022c3124fdcae480371663db313"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d64e40156a3a489c8a4e2ee78a259db2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee5892bef0ed41ad97c9eaae4ec854d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5aa5cad8a5b845a39302d4f70ce645d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e93f9e81a3e44a6b737400452ab0e04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00000-of-00003.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c2cb20d0f98435abc25495cf8529d7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00001-of-00003.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0f30e19315f473c8c74b79571f7ab51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00002-of-00003.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "038d22a096b949bb97e436d641a26744"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/validation-00000-of-00001.parquet:   0%|          | 0.00/474k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10b15bcd033d45889e874c7f92eec1be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/test-00000-of-00001.parquet:   0%|          | 0.00/509k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a98cdcc26674d3f8476866536b686eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "209ac90628a647f28cef5f133710be3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fd745d9b7a94c57bb891a8a1f16f18c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "643e1762b4264a18964c7e618a3b9e4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### b560 – 4bit\n",
            "\n",
            "### b3b – 8bit\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a17466836aaf4d12b6a5770a22cf9fb0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cea5070834194c3c82bdd97f03e103f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a183dfb01aaa441792dd369565da2363"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da673d8350f64128bd8774bf43ca8e01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.01G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21fe93c78e534190a3e7611c49464133"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### b3b – 4bit\n",
            "\n",
            "Ergebnisse:\n",
            "                 model_id alias variant precision      time_s  energy_kwh  \\\n",
            "0  bigscience/bloom-560m  b560    8bit      8bit  102.584526    0.002717   \n",
            "1  bigscience/bloom-560m  b560    4bit      4bit   50.847029    0.001373   \n",
            "2    bigscience/bloom-3b   b3b    8bit      8bit  114.898404    0.003214   \n",
            "3    bigscience/bloom-3b   b3b    4bit      4bit   64.176644    0.001901   \n",
            "\n",
            "     co2_kg  kg_per_kwh  tokens_out         ppl       bleu    ram_GB  \\\n",
            "0  0.001230    0.452621          96  314.554896   3.742804  2.950699   \n",
            "1  0.000622    0.452621          96  294.980380   2.578659  3.375652   \n",
            "2  0.001455    0.452621          96  222.560595  10.368502  3.885921   \n",
            "3  0.000861    0.452621          96  223.345802  11.548166  3.891594   \n",
            "\n",
            "   vram_alloc_GB  vram_reserved_GB                                    notes  \n",
            "0       0.770445          1.046875  GPU=NVIDIA A100-SXM4-40GB, VRAM=39.6 GB  \n",
            "1       0.643933          1.048828  GPU=NVIDIA A100-SXM4-40GB, VRAM=39.6 GB  \n",
            "2       3.407967          4.304688  GPU=NVIDIA A100-SXM4-40GB, VRAM=39.6 GB  \n",
            "3       2.341395          3.175781  GPU=NVIDIA A100-SXM4-40GB, VRAM=39.6 GB  \n",
            "\n",
            "Per-Phase:\n",
            "    phase      time_s  energy_kwh    co2_kg alias variant  kg_per_kwh\n",
            "0    gen    7.550085    0.000200  0.000091  b560    8bit    0.452621\n",
            "1    ppl    5.076226    0.000134  0.000061  b560    8bit    0.452621\n",
            "2   bleu   89.958215    0.002383  0.001079  b560    8bit    0.452621\n",
            "3    gen    4.203106    0.000114  0.000051  b560    4bit    0.452621\n",
            "4    ppl    2.125680    0.000058  0.000026  b560    4bit    0.452621\n",
            "5   bleu   44.518243    0.001202  0.000544  b560    4bit    0.452621\n",
            "6    gen    9.528618    0.000267  0.000121   b3b    8bit    0.452621\n",
            "7    ppl    3.426680    0.000098  0.000044   b3b    8bit    0.452621\n",
            "8   bleu  101.943106    0.002849  0.001290   b3b    8bit    0.452621\n",
            "9    gen    5.265048    0.000154  0.000070   b3b    4bit    0.452621\n",
            "10   ppl    2.441573    0.000088  0.000040   b3b    4bit    0.452621\n",
            "11  bleu   56.470023    0.001660  0.000751   b3b    4bit    0.452621\n",
            "Gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_quantisierung/bloom_quant_results.csv\n"
          ]
        }
      ]
    }
  ]
}