{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ino54/MA_GreenAI-Practical-Experiments/blob/main/deepseek_hardware.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPtLyLChvZzL",
        "outputId": "7e249c88-5e5d-46d8-a3a2-75bad199865a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "# ---------- Requirements ----------\n",
        "%%writefile requirements.txt\n",
        "transformers>=4.44\n",
        "accelerate>=0.33\n",
        "bitsandbytes\n",
        "datasets>=2.20\n",
        "evaluate>=0.4\n",
        "sacrebleu>=2.4\n",
        "codecarbon>=2.5,<3\n",
        "pynvml>=12,<13\n",
        "psutil\n",
        "numpy==2.0.2\n",
        "pandas==2.2.2\n",
        "huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U -r requirements.txt --no-warn-conflicts\n",
        "# potenzielle Konflikte leise entfernen\n",
        "!pip uninstall -y -q google-genai firebase-admin || true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQDxkKOGvkpp",
        "outputId": "c5c66189-9058-42f5-bd87-4f7c303005db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m131.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.6/517.6 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.2/291.2 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Hugging Face Login via Colab-Secret ----------\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(hf_token); print(\"Hugging Face Login erfolgreich!\")\n",
        "else:\n",
        "    print(\"WARNUNG: Kein HF_TOKEN gefunden\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQSN1rkzvklj",
        "outputId": "d86ed9eb-a035-4a7e-872a-380e7624c420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face Login erfolgreich!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Drive mounten & Zielordner prüfen ----------\n",
        "import os, shutil, time, pathlib, platform, gc, re, math, warnings, inspect\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from google.colab import drive\n",
        "MOUNTPOINT = \"/content/drive\"\n",
        "already = os.path.isdir(os.path.join(MOUNTPOINT, \"MyDrive\"))\n",
        "if not already and os.path.isdir(MOUNTPOINT) and os.listdir(MOUNTPOINT):\n",
        "    backup = f\"/content/drive_stale_{int(time.time())}\"\n",
        "    shutil.move(MOUNTPOINT, backup)\n",
        "    os.makedirs(MOUNTPOINT, exist_ok=True)\n",
        "drive.mount(MOUNTPOINT, force_remount=(not already))\n",
        "\n",
        "work_dir = \"/content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/deepseek_hardware\"\n",
        "pathlib.Path(work_dir).mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(work_dir)\n",
        "project_dir = work_dir\n",
        "print(\"Arbeitsordner:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsDBTwUlvkis",
        "outputId": "ea08a1e6-7c74-4d68-f01c-21e49672a58f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Arbeitsordner: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/deepseek_hardware\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Imports & Device ----------\n",
        "import numpy as np, pandas as pd\n",
        "import torch, psutil\n",
        "from contextlib import nullcontext\n",
        "from types import SimpleNamespace\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig, set_seed\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "# PyTorch Runtimes (Performance)\n",
        "import os as _os\n",
        "_os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # weniger Fragmentierung\n",
        "\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_total_gb = torch.cuda.get_device_properties(0).total_memory/(1024**3)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except: pass\n",
        "else:\n",
        "    gpu_name = \"CPU\"; vram_total_gb = 0.0\n",
        "print(f\"Device: {device} | GPU: {gpu_name} | VRAM={vram_total_gb:.1f} GB | Torch {torch.__version__} | Py {platform.python_version()}\")\n",
        "\n",
        "RESULT_BASENAME = \"deepseek_hardware_4bit\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBx4dcgbvkfj",
        "outputId": "39a2ad41-87f3-4f55-e793-87fb3208f648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | GPU: NVIDIA A100-SXM4-40GB | VRAM=39.6 GB | Torch 2.8.0+cu126 | Py 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Flags für Hardware-Optimierung ----------\n",
        "APPLY_TORCH_COMPILE     = False    # bei längeren Läufen True probieren\n",
        "PINNED_MEM              = True     # Host-Puffer pinnen\n",
        "NON_BLOCKING            = True     # non_blocking Transfers zur GPU\n",
        "USE_BF16_AUTOCAST       = (device==\"cuda\" and torch.cuda.get_device_capability(0)[0] >= 8)  # Ampere+\n",
        "ATTN_IMPL               = \"sdpa\"   # Qwen/DeepSeek: SDPA ist schnell/stabil\n",
        "\n",
        "# Eval-Config (identisch zum BLOOM-Notebook, aber hier nur ein Modell)\n",
        "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "ALIAS    = \"r1q15b\"\n",
        "\n",
        "EVAL = {\n",
        "    \"max_new_tokens\": 32,\n",
        "    \"ppl\":  {\"name\":\"wikitext\",\"config\":\"wikitext-2-raw-v1\",\"split\":\"test[:1%]\"},\n",
        "    \"bleu\": {\"name\":\"wmt14\",\"config\":\"de-en\",\"split\":\"test[:32]\"},\n",
        "}\n",
        "PROMPTS = [\n",
        "    \"Schreibe einen kurzen Absatz über nachhaltige KI.\",\n",
        "    \"Erkläre in einfachen Worten, was Quantisierung in neuronalen Netzen ist.\",\n",
        "    \"Nenne drei Vorteile von Mixture-of-Experts-Modellen.\"\n",
        "]\n",
        "bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# Schutz: max. Eingabelänge (senkt VRAM, stabilisiert Bench)\n",
        "MAX_LEN_CAP = 128  # ggf. 192/128 für noch weniger VRAM\n",
        "\n",
        "# Warnungsfreie, deterministische Generation (keine temperature/top_p-Flags)\n",
        "GENCFG = GenerationConfig(do_sample=False, temperature=None, top_p=None, top_k=None, num_beams=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a899ef1bb52946879ea360c17e80fd82",
            "ae39c3ccadd14507b39c8e1087b02072",
            "0f13af374faf4f48b281752c509450dd",
            "4341fb5719c54708b068c39a100507e2",
            "e6918f4b866344e2a9fd26dede9460e9",
            "49617c15488846938ea52a408da37ef9",
            "1b421f9e4b7d40c38ac2f87935921c1d",
            "b4e533c1cf9e45ebb08797c6a22fa0b7",
            "0edef36fe02a46fdbacdc6883ce7e4f6",
            "3458e790ed9e463d8421503213307ba3",
            "46edf2eff39e438083f27cf9d5308e00"
          ]
        },
        "id": "cUEpM0tjvz3_",
        "outputId": "19264578-7d8b-48fd-e800-5730f3d34ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a899ef1bb52946879ea360c17e80fd82"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- CodeCarbon-Helfer ----------\n",
        "def _cleanup_cc_locks():\n",
        "    for p in [\n",
        "        \"/tmp/.codecarbon.lock\",\n",
        "        _os.path.expanduser(\"~/.codecarbon/codecarbon.lock\"),\n",
        "        \"/content/.codecarbon/codecarbon.lock\",\n",
        "    ]:\n",
        "        try:\n",
        "            if _os.path.exists(p): _os.remove(p)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "_os.environ[\"CODECARBON_CACHE_DIR\"] = f\"/content/.cc_cache_hw_{int(time.time())}\"\n",
        "\n",
        "def _cc_supported_kwargs():\n",
        "    base = dict(log_level=\"error\", output_dir=\".\", measure_power_secs=1, tracking_mode=\"process\")\n",
        "    try:\n",
        "        params = inspect.signature(EmissionsTracker.__init__).parameters\n",
        "        if \"allow_multiple_runs\" in params: base[\"allow_multiple_runs\"] = True\n",
        "        if \"cloud_provider\" in params:      base[\"cloud_provider\"] = \"google\"\n",
        "        if \"cloud_region\" in params:        base[\"cloud_region\"]   = \"europe-west10\"\n",
        "        if \"country_iso_code\" in params:    base[\"country_iso_code\"] = \"DEU\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    return base\n",
        "\n",
        "def make_trk(name, out):\n",
        "    _cleanup_cc_locks()\n",
        "    return EmissionsTracker(project_name=name, output_file=out, **_cc_supported_kwargs())\n",
        "\n",
        "def start(tr):\n",
        "    try:\n",
        "        tr.start(); return True\n",
        "    except Exception:\n",
        "        _cleanup_cc_locks()\n",
        "        try:\n",
        "            tr.start(); return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "def stop(tr, st):\n",
        "    if not st:\n",
        "        return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "    try:\n",
        "        return tr.stop()\n",
        "    except Exception:\n",
        "        return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "\n",
        "def unpack(em):\n",
        "    if hasattr(em, \"energy_consumed\") and hasattr(em, \"emissions\"):\n",
        "        try: return float(em.energy_consumed), float(em.emissions)\n",
        "        except: return 0.0, 0.0\n",
        "    if isinstance(em, dict):\n",
        "        e = em.get(\"energy_consumed\", 0.0)\n",
        "        c = em.get(\"emissions\", em.get(\"emissions_kg\", 0.0))\n",
        "        try: return float(e), float(c)\n",
        "        except: return 0.0, 0.0\n",
        "    try: return 0.0, float(em)\n",
        "    except: return 0.0, 0.0\n",
        "\n",
        "def read_energy(path):\n",
        "    try:\n",
        "        if not _os.path.exists(path): return 0.0\n",
        "        df = pd.read_csv(path)\n",
        "        for c in [\"energy_consumed\",\"energy_consumed_kwh\",\"energy_consumed (kWh)\",\"energy (kWh)\"]:\n",
        "            if c in df.columns: return float(df[c].iloc[-1])\n",
        "        for c in df.columns:\n",
        "            if \"energy\" in c.lower() and \"kwh\" in c.lower():\n",
        "                return float(df[c].iloc[-1])\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def measure(phase, fn, prefix):\n",
        "    logfile = _os.path.join(project_dir, f\"{prefix}_{phase}.csv\")\n",
        "    tr = make_trk(f\"{prefix}_{phase}\", logfile)\n",
        "    import time as _t\n",
        "    st = start(tr); t0 = _t.time(); res = fn(); t1 = _t.time()\n",
        "    em = stop(tr, st); ekwh, co2 = unpack(em)\n",
        "    if ekwh == 0.0:\n",
        "        ek = read_energy(logfile)\n",
        "        if ek: ekwh = ek\n",
        "    return {\"phase\": phase, \"time_s\": t1-t0, \"energy_kwh\": ekwh, \"co2_kg\": co2}, res"
      ],
      "metadata": {
        "id": "jxpvqkhnvzyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Loader: DeepSeek in 4-bit (NF4) + HW-Tweaks ----------\n",
        "def load_model_4bit_optimized(model_id: str):\n",
        "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    if tok.pad_token_id is None: tok.pad_token = tok.eos_token\n",
        "    tok.padding_side = \"left\"\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    bnb4 = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=(torch.bfloat16 if USE_BF16_AUTOCAST else torch.float16),\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb4,\n",
        "        attn_implementation=ATTN_IMPL,  # SDPA\n",
        "    )\n",
        "\n",
        "    # Channels-last kann Speicher/Cache hit verbessern\n",
        "    if device==\"cuda\":\n",
        "        model = model.to(memory_format=torch.channels_last)\n",
        "\n",
        "    # torch.compile: optional (für lange Läufe)\n",
        "    if APPLY_TORCH_COMPILE and hasattr(torch, \"compile\"):\n",
        "        try:\n",
        "            model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=False)\n",
        "            print(\"[HW] torch.compile aktiv (reduce-overhead).\")\n",
        "        except Exception as e:\n",
        "            print(\"[HW] torch.compile nicht möglich → normal weiter:\", repr(e))\n",
        "\n",
        "    model.eval()\n",
        "    return tok, model"
      ],
      "metadata": {
        "id": "xmUT4lCRvzpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Pinned Memory & Utils ----------\n",
        "def _pin_and_move(batch, device):\n",
        "    out = {}\n",
        "    for k, v in batch.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            if PINNED_MEM and v.device.type == \"cpu\":\n",
        "                try: v = v.pin_memory()\n",
        "                except Exception: pass\n",
        "            out[k] = v.to(device, non_blocking=NON_BLOCKING)\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def get_model_device(model):\n",
        "    try: return next(model.parameters()).device\n",
        "    except StopIteration: return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def autocast_ctx():\n",
        "    if device!=\"cuda\": return nullcontext()\n",
        "    return torch.autocast(device_type=\"cuda\", dtype=(torch.bfloat16 if USE_BF16_AUTOCAST else torch.float16))\n",
        "\n",
        "def safe_max_len(tok, model, fallback=2048, upper=100000):\n",
        "    cand = getattr(tok, \"model_max_length\", None)\n",
        "    if isinstance(cand, int) and 0 < cand < upper: return min(cand, fallback)\n",
        "    cand = getattr(getattr(model, \"config\", None), \"max_position_embeddings\", None)\n",
        "    if isinstance(cand, int) and 0 < cand < upper: return min(cand, fallback)\n",
        "    return fallback\n",
        "\n",
        "def capture_memory():\n",
        "    ram = psutil.Process().memory_info().rss\n",
        "    valloc = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "    vres  = torch.cuda.memory_reserved()  if torch.cuda.is_available() else 0\n",
        "    return ram, valloc, vres\n",
        "\n",
        "def bytes_to_gb(b): return float(b)/(1024**3)"
      ],
      "metadata": {
        "id": "YhWJYHIXwOEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Evaluation (batched, exakte Tokenzählung) ----------\n",
        "def simple_generate(model, tok, prompts, max_new_tokens=32):\n",
        "    dev = get_model_device(model)\n",
        "    ml = min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    enc = tok(prompts, return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "    enc = _pin_and_move(enc, dev)\n",
        "    room = ml - enc[\"input_ids\"].shape[1]\n",
        "    cur_new = max(1, min(max_new_tokens, int(room)))\n",
        "    with torch.inference_mode(), autocast_ctx():\n",
        "        out = model.generate(**enc, max_new_tokens=cur_new, generation_config=GENCFG, pad_token_id=tok.eos_token_id)\n",
        "\n",
        "    texts, total_gen_tokens = [], 0\n",
        "    max_input_len = int(enc[\"input_ids\"].shape[1])\n",
        "    pad_id = tok.pad_token_id\n",
        "    eos_id = tok.eos_token_id\n",
        "    for i in range(out.size(0)):\n",
        "        seq = out[i]\n",
        "        gen_slice = seq[max_input_len:]\n",
        "        gen_i = 0\n",
        "        for t in gen_slice.tolist():\n",
        "            if t == eos_id:\n",
        "                gen_i += 1\n",
        "                break\n",
        "            if (pad_id is not None) and (t == pad_id):\n",
        "                break\n",
        "            gen_i += 1\n",
        "        total_gen_tokens += gen_i\n",
        "        texts.append(tok.decode(seq, skip_special_tokens=True))\n",
        "    return texts, total_gen_tokens\n",
        "\n",
        "def eval_perplexity(model, tok, ds_cfg):\n",
        "    dev = get_model_device(model)\n",
        "    ds = load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    ml = min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    losses = []\n",
        "    with torch.inference_mode(), autocast_ctx():\n",
        "        for t in ds[\"text\"]:\n",
        "            if not isinstance(t, str) or len(t.strip()) < 4: continue\n",
        "            enc = tok(t, return_tensors=\"pt\", truncation=True, max_length=ml)\n",
        "            enc = _pin_and_move(enc, dev)\n",
        "            out = model(enc[\"input_ids\"], labels=enc[\"input_ids\"])\n",
        "            losses.append(float(out.loss.detach().cpu()))\n",
        "    return math.exp(np.mean(losses)) if losses else None\n",
        "\n",
        "bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "def eval_bleu_llm(model, tok, ds_cfg, max_new_tokens=32, batch_size=8):\n",
        "    dev = get_model_device(model)\n",
        "    ds = load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    ml = min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    preds, refs = [], []\n",
        "    with torch.inference_mode(), autocast_ctx():\n",
        "        batch_prompts, batch_refs = [], []\n",
        "        for ex in ds:\n",
        "            de, en = ex[\"translation\"][\"de\"], ex[\"translation\"][\"en\"]\n",
        "            prompt = f\"Translate to English:\\nGerman: {de}\\nEnglish:\"\n",
        "            batch_prompts.append(prompt); batch_refs.append(en)\n",
        "            if len(batch_prompts) >= batch_size:\n",
        "                enc = tok(batch_prompts, return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "                enc = _pin_and_move(enc, dev)\n",
        "                room = ml - enc[\"input_ids\"].shape[1]\n",
        "                cur_new = max(1, min(max_new_tokens, int(room)))\n",
        "                out = model.generate(**enc, max_new_tokens=cur_new, generation_config=GENCFG, pad_token_id=tok.eos_token_id)\n",
        "                for i in range(out.size(0)):\n",
        "                    gen = tok.decode(out[i], skip_special_tokens=True)\n",
        "                    hyp = gen.split(\"English:\")[-1].strip().split(\"\\n\")[0].strip() or gen.strip()\n",
        "                    preds.append(hyp); refs.append([batch_refs[i]])\n",
        "                batch_prompts, batch_refs = [], []\n",
        "        if batch_prompts:\n",
        "            enc = tok(batch_prompts, return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "            enc = _pin_and_move(enc, dev)\n",
        "            room = ml - enc[\"input_ids\"].shape[1]\n",
        "            cur_new = max(1, min(max_new_tokens, int(room)))\n",
        "            out = model.generate(**enc, max_new_tokens=cur_new, generation_config=GENCFG, pad_token_id=tok.eos_token_id)\n",
        "            for i in range(out.size(0)):\n",
        "                gen = tok.decode(out[i], skip_special_tokens=True)\n",
        "                hyp = gen.split(\"English:\")[-1].strip().split(\"\\n\")[0].strip() or gen.strip()\n",
        "                preds.append(hyp); refs.append([batch_refs[i]])\n",
        "    return float(bleu_metric.compute(predictions=preds, references=refs)[\"score\"])"
      ],
      "metadata": {
        "id": "91WnBf4JwUuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Run (warmup/gen/ppl/bleu) ----------\n",
        "def run_once(model_id: str, alias: str):\n",
        "    print(f\"\\n### Starte Hardware-Optimierung 4-bit: {alias} ({model_id})\")\n",
        "\n",
        "    def _do_load():\n",
        "        tok, model = load_model_4bit_optimized(model_id)\n",
        "        # Warmup (batch=2) für stabilere Kernelwahl/Compile\n",
        "        dev = get_model_device(model); ml = min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "        wenc = tok([\"Warmup token 1\", \"Warmup token 2\"], return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "        wenc = _pin_and_move(wenc, dev)\n",
        "        with torch.inference_mode(), autocast_ctx():\n",
        "            _ = model.generate(**wenc, max_new_tokens=1, generation_config=GENCFG, pad_token_id=tok.eos_token_id)\n",
        "        return tok, model\n",
        "\n",
        "    m_warm, (tok, model) = measure(\"warmup\", _do_load, f\"{RESULT_BASENAME}_{alias}\")\n",
        "\n",
        "    m_gen,  (samples, n_tok) = measure(\"gen\",\n",
        "        lambda: simple_generate(model, tok, PROMPTS, EVAL[\"max_new_tokens\"]),\n",
        "        f\"{RESULT_BASENAME}_{alias}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    m_ppl,  ppl  = measure(\"ppl\",\n",
        "        lambda: eval_perplexity(model, tok, EVAL[\"ppl\"]),\n",
        "        f\"{RESULT_BASENAME}_{alias}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    m_bleu, bleu = measure(\"bleu\",\n",
        "        lambda: eval_bleu_llm(model, tok, EVAL[\"bleu\"], EVAL[\"max_new_tokens\"], batch_size=8),\n",
        "        f\"{RESULT_BASENAME}_{alias}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    total_time   = m_warm[\"time_s\"] + m_gen[\"time_s\"] + m_ppl[\"time_s\"] + m_bleu[\"time_s\"]\n",
        "    total_energy = m_warm[\"energy_kwh\"] + m_gen[\"energy_kwh\"] + m_ppl[\"energy_kwh\"] + m_bleu[\"energy_kwh\"]\n",
        "    total_co2    = m_warm[\"co2_kg\"]     + m_gen[\"co2_kg\"]     + m_ppl[\"co2_kg\"]     + m_bleu[\"co2_kg\"]\n",
        "\n",
        "    steady_time   = m_gen[\"time_s\"] + m_ppl[\"time_s\"] + m_bleu[\"time_s\"]\n",
        "    steady_energy = m_gen[\"energy_kwh\"] + m_ppl[\"energy_kwh\"] + m_bleu[\"energy_kwh\"]\n",
        "    steady_co2    = m_gen[\"co2_kg\"]     + m_ppl[\"co2_kg\"]     + m_bleu[\"co2_kg\"]\n",
        "\n",
        "    ram = psutil.Process().memory_info().rss\n",
        "    valloc = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "    vres  = torch.cuda.memory_reserved()  if torch.cuda.is_available() else 0\n",
        "\n",
        "    per_phase = [m_warm, m_gen, m_ppl, m_bleu]\n",
        "    for p in per_phase:\n",
        "        p[\"alias\"] = alias\n",
        "        p[\"model_id\"] = model_id\n",
        "\n",
        "    row = dict(\n",
        "        model_id=model_id, alias=alias, precision=\"int4 (NF4, double-quant, sdpa)\",\n",
        "        time_s=total_time, energy_kwh=total_energy, co2_kg=total_co2,\n",
        "        steady_time_s=steady_time, steady_energy_kwh=steady_energy, steady_co2_kg=steady_co2,\n",
        "        kg_per_kwh=(total_co2/total_energy) if total_energy else None,\n",
        "        tokens_out=int(n_tok), ppl=ppl, bleu=bleu,\n",
        "        ram_GB=bytes_to_gb(ram), vram_alloc_GB=bytes_to_gb(valloc), vram_reserved_GB=bytes_to_gb(vres),\n",
        "        notes=f\"compile={APPLY_TORCH_COMPILE}; pinned={PINNED_MEM}; nb={NON_BLOCKING}; MAX_LEN_CAP={MAX_LEN_CAP}; bf16_autocast={USE_BF16_AUTOCAST}\"\n",
        "    )\n",
        "\n",
        "    # Samples speichern\n",
        "    samples_path = os.path.join(project_dir, f\"{RESULT_BASENAME}_samples_{alias}.txt\")\n",
        "    with open(samples_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, txt in enumerate(samples, 1):\n",
        "            f.write(f\"--- Beispiel {i} ({alias}) ---\\n{txt}\\n\\n\")\n",
        "    print(\"Beispiele gespeichert:\", samples_path)\n",
        "\n",
        "    return row, per_phase"
      ],
      "metadata": {
        "id": "pmN42mw9wUp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Ausführen & Speichern ----------\n",
        "row, phases = run_once(MODEL_ID, ALIAS)\n",
        "df  = pd.DataFrame([row])\n",
        "dfp = pd.DataFrame(phases)\n",
        "\n",
        "out_csv = os.path.join(project_dir, f\"{RESULT_BASENAME}_results.csv\")\n",
        "df.to_csv(out_csv, index=False)\n",
        "\n",
        "out_phase_csv = os.path.join(project_dir, f\"{RESULT_BASENAME}_per_phase.csv\")\n",
        "dfp.to_csv(out_phase_csv, index=False)\n",
        "\n",
        "print(\"\\nErgebnisse (gesamt):\")\n",
        "print(df)\n",
        "print(\"Gespeichert (gesamt):\", out_csv)\n",
        "\n",
        "print(\"\\nPer-Phase Übersicht:\")\n",
        "print(dfp[[\"alias\",\"phase\",\"time_s\",\"energy_kwh\",\"co2_kg\"]].sort_values([\"alias\",\"phase\"]))\n",
        "print(\"Gespeichert (per Phase):\", out_phase_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e6582e9dc6b24298a243192e87b5a0d2",
            "a00c3bf37d4a4abdba898d3263ab3da1",
            "393e31a9afd842c4bce60e3472761858",
            "3351b872892747ff83f553f95155a183",
            "6536305591ad48b38432b91187e6177f",
            "59e07bdcda5f4a56adf95dc4ebcbb7cf",
            "4e105e7a7cb44427a10fbe9d348b4c9b",
            "96b703b9e07b4acfb419bcddab1c8f29",
            "f1a014ebef9e40958043c6d0fbecf163",
            "80661a307016430989f5df7b8951fbc9",
            "8fccdc769ce146b1a4e0a93af7ae14db",
            "c42c1c7e7b23460d8fe87825cc103370",
            "7f6c4214322e435fba647a5da1e2d9ce",
            "d2b7288ee0d74f88ae281809ea1da571",
            "5a00b0897bd3494797c10a07c120368b",
            "0aae0b5ff5234757a0fc0416e1bdd2d7",
            "b8bd9825cbed4a9db3be2ba9c5e68a68",
            "468ef00e9c444379ad2f4494a9b5954c",
            "e4e33329ab9e450492c9f75a2bc3062a",
            "c72dc582556e4036ab31d28a972b7a85",
            "807cde7f467f4ed8bc10c802001dfdb0",
            "81f1db5b7be445639183a8536eb74a3f",
            "7324b490d5c24038b0dec9abf6b34048",
            "06adc527d74e4ad899e1a1559bc4e051",
            "48ce1e8001084a9b88cc7f9eccac69ba",
            "89ad8f20ab384d37bba0cf678acc9a9e",
            "857133643ab145328ccecd0a7f1e4588",
            "f7d2e3ecf10347f3a03cf73988881c1f",
            "e675efbccf07419093a9bf28ec95d85b",
            "9af406ccc49c42deb6e47f44bcdcd2ef",
            "137b415cc7984b6a8b0b03f24baa7401",
            "35163053dede48c69e36d38c05830d92",
            "9fefed88c36747daaa013495a3891466",
            "80dd95ea17f7444ea4d73f00fbeb9bd9",
            "91958ed1699b4075851b35e8e2745393",
            "058e0bdf3c024978948016af392cc983",
            "1bd6efab5c02469b9895ddff10cd7a42",
            "156fa1d74d52423384d5cd2569e18dc5",
            "b9abce1d06124909a0ea2dc3c067512e",
            "0c7f10f26989476ba3c70fae54434540",
            "ffd227d744b74fc5bd74185e35fac5d0",
            "5d62ee796db645e981609204933d8ef5",
            "675b7e8efb7846d98902556aa4ed338b",
            "09d739e7e9d74b9384eabf95bb92ad9f",
            "ae78acfb40674120bb0725042e9b59e0",
            "5797635563014e50bb16a44e4a42a022",
            "74889e2aa8f341a297583f8471716d0e",
            "1a4652efe7ca4eb09b09bfe4c709de53",
            "1a671e4759bf4aca935908d71f29adeb",
            "5ce844e0267b484dbee37adc29361d30",
            "f07bf826472d4547a45795332b290854",
            "7094570ff2c748dfbee6313904623d4b",
            "878e128fa04b4ef59e70064a4fa19b72",
            "e96668a0d20f421ead46dd73a4583fc0",
            "545a10dfa1484dc99d4ad40a8a45c2cc",
            "13aefb92bf604014a054be05168e9d80",
            "4724045f027247c7a098541542b090a2",
            "c9db99012f5d4547a07900ab09aaf6e1",
            "dd2c9d338f2a499698d066d40fda469a",
            "f99fbf7540554785be3dac7bd44d5248",
            "b6b3ae0506c84a17a57b8e032f4e6bce",
            "dbfa07336a054396b6253e03e0a397a4",
            "56265a2680b947b6a0e0ed3bbec96aa3",
            "fe6e4206f34f4a7081a091b73fa7bd6c",
            "6459746ec3cd4dd5b1eff65b51927ba2",
            "ab8e810750574b20974600d2f4f86bee",
            "32d33a99cf9d4f1e93672feb822a7b2f",
            "96a69748238c452bbffcfb9460e99cb7",
            "fb7d6b62f25c4b3f871f04f40cbe6864",
            "feacf0a65bcc468c9a32290b13d0c736",
            "5507f14b40794fd5806f1e6567e4af71",
            "220b92246fee4de2b21930e0bc7f1d6d",
            "f6dbe3ec88644e7ca3037a4789418868",
            "3b0bdc9ba6224cde967c97414156ccc5",
            "e7a725704d4a45ed95f5a47e50fef53e",
            "6062dd77b84b41b6b32263664def20b3",
            "c882557fa8b44c71af923d15c5f9c4ca",
            "b5dbe685ff3e4531aeb1b3f8c70ca04a",
            "3f07ed129bff4a418ea202fe8ce6c7bc",
            "d84d7b1ac7a540d69b9441ee3323a451",
            "5906d33554d14b9391aa67d90b35ca27",
            "507a478e323a476b87d07201fd11bc86",
            "82c76cbb48a7478688325ca6dd534bd0",
            "933d6dfe595441eaa368947c19eafc2b",
            "1a4d7b54377f4038bd5301d9c75f9e77",
            "2b72d8d807c642a59af2aad476de6ec1",
            "c36bfda38d6e401bbd6a487910a5e650",
            "a62a9a8af403410da44e11fb5f0ccede",
            "a307f7bdb91748e0a5ca34fc31ea528f",
            "c8c7979a7e6e4bde94a25e052b632759",
            "f17be26ffb074c0a8073b3a5cfddcb66",
            "a8be3fce07f24a128d0e5d33ab6cb9f5",
            "bcbfda2aa7f64540b1fa932bb1a9fa70",
            "176eeacc5a0640c1b43d02c8b34a427f",
            "32f14f52cb2143729fcb70ea81589bf9",
            "94c65b543eae471cbc3f7e4d412dee60",
            "168f20ff95414c4eb3b5cf9e8b7231b1",
            "463d58d4fca04d45915ae15bd647b844",
            "4203c78ff128486bab9a939fc6d20299",
            "7c81f054a3ad42489774a355f604346f",
            "d28456d9380d4ae983b3cfc8dbe0d7ef",
            "ab544d066f49415ca41be847e47e5a70",
            "65b17bf2ea1d43ddbd0a61052f29fd3e",
            "f000313ca3514ca0809223f3c819c7e6",
            "9cfa24634c77453d8c5b845b74b14e2a",
            "63e3f8d456ca49f4bb8e6d08b0890a80",
            "c9cff00d177840578c386f13233cf1b7",
            "fdb8f76d373b4e07a82bf00145f149cb",
            "1a6d5aacfa52442aad305bab214393d2",
            "52b7000d48b84afcbaf10803cc5c1d65",
            "7a7bbc2e37ce4c4998541409f5810c91",
            "19481ed15c304568945e1ed7bdbd4e7e",
            "4d0fbc0497d245c3bd424c8f3b593d99",
            "90f8fe67d3124fe280066985aa0ea784",
            "1c454c094d3749758c2ea8a53e41e0f7",
            "6dd1578b91a94f79b560018f39c2ac84",
            "856d2ff8cbe741ceba85670d3cd812e4",
            "eaaf7f557b79444c8ac0ba0667c4b422",
            "2d40cea5c844467eb3891b6cf8be5dcf",
            "799fd2a1f35143c78c80f4f18f6d319c",
            "e594272cdb3c4ad68ea6a7833129ddab",
            "1da96aa027c5437783ef618443147c01",
            "f2a3c59b04d64f3480de5aae8c0c0ba2",
            "6856380baf344d1c9286f70ae60d7fd5",
            "af41949fd2ed40d9bdaf6d95cf3f5cf5",
            "14df97edcc3042bea2f12a3d2ec5abfd",
            "3f73ee9349624da1a4eb61a6afefe83f",
            "23b61b26d1564e39a896ff8649b19695",
            "a8324a42d16b4b8b8c7e2aaac3ee27b4",
            "070b5401f992425a94a5ebb311efc957",
            "250c0489c54c43f6ad6eb4b0ee99c26a",
            "1dedc8bde4e94745b3dd581002fc7497",
            "07819957abde45f9a0e8c54cf67679bc",
            "08475dac1df4423db131695ab3133664",
            "fcdb48ed2dfa428ab5fe32bbb17573eb",
            "83ce60b78306414d87a5e392b2ae1715",
            "443601ba5a6c482f8c317b98db85dca3",
            "8fecd7794cd74f9da5b038fe13f5d42d",
            "8208e7e5a22e43caba9e2b84b6bef72a",
            "9ddf48edba7c41ca928bbaf405975104",
            "1706395b6e79474e96ce882901a7246f",
            "ddb17d73b83e4600b90bf0613d741fb9",
            "8e6b018698c74fa7822f41d7b8da4834",
            "b36125179f5e4543b98f86f42c025cac",
            "71058169b30448878dc152bf2ad40cc1",
            "9f1730506e704288a463f499a6e24650",
            "25bd872244ed4081a20775281a4bde41",
            "c58abf5a85364a47953a02951eb3bada",
            "975c12c83ea64de7ad953772871d24ed",
            "d3f19c62fb9d41c39af25ccc8dba8f98",
            "b7ee0fdf44c54b3a99cda910c409b909",
            "0152e94c585447c3befcb876443fd9b5",
            "92b04bc5fffe4190a9e9247c17a3289e",
            "1ed7e9b16fa24fc282693dffb3a73816",
            "107328d4e28640309f99ea0886cb56b5",
            "76571f28c7014b028c5cd36587000bad",
            "1102ade692354f728cb32e3a4815b20e",
            "12cf446b7f4748abb2d43ae82c3358ca",
            "19bf264d6858400dae691307c78d1d6b",
            "c88a9122322c45798df45d7dd96abb59",
            "64a876b2237648fba8108d606d45c298",
            "f0ddbe26ce904494b8a59e5b6354c084",
            "3d0b69ae7d2f44efbe715303efaa598e",
            "6523d2c1b6c34427a1866ba5e9f31471",
            "976716245b3d4101bb735e35f72383f8",
            "89009fbb9f4e41189a2a9fc57f7f2d60",
            "d5354f922ffd477fa65920a563930b2f",
            "ae150eca93a04ab0a4daa73bfc9cf2e8",
            "1f68bb58e7a64d2a9ae8895b16e8760c",
            "323b988d1dad43c5aa8844e8773bfcea",
            "06ce9f3429584f80980942f4768ce137",
            "33ba7ea97bc3428b9f58fb1f30467ac6",
            "dcbb5256f21441c5b0c6a36d88b787bb",
            "c88270b8998949a8a065109c847b4bbb",
            "0bb0a08391dc4b1d99c61d57adb0f30b",
            "ea75781e8ea1495ca6598a1c5a9fa0d3",
            "fb0412c89cf3439f8a577a72a465a093",
            "7e71b36a85424ea88df2c150c939a4f5",
            "7622c460d0cb418688bab8cdc114abc1",
            "11e9a96296d34b129af6c53ed44467e2",
            "9eee73d5aafc480db4daf0f7bb2678b0",
            "72f8e790996742cbbd393c90233da9da",
            "7767828264a641f1ac3e461d0b13025f",
            "6be676da145048cca5e28e545ba2121b",
            "30e9b0c1333047f7a356524c3e0f0190",
            "bd869c4de8894374868a92fb32d44aac",
            "37f943e32446470bb09db9decd586117",
            "a547bf4a309d4aa6870da856878da72c",
            "dd55f2942f234e06968538adc27c0cc2",
            "356ea9350a7f47d7bfde8c561376aced",
            "2a2dd2ad864b460f9e850469a99ecf61",
            "e385cd6cf7a34ab4a882508371371c91",
            "03aa8b18cc72414aa4db5a1ff0b9ca6c",
            "b7987afb928b4ab7a5ef5f7783d58736",
            "12b7837041c64d97a2cf297b4d6a008b",
            "ef53318a6b334287ba63591112e6d89b",
            "94a2cd681e994428b541bfab35ea8ded",
            "5a22f638483d48e0826100f5345a3b41",
            "aa67324f454b4b9d9a118f69ee03d8c8",
            "e3a9b13ace4b4cc59a4020e5e6d7f4b7",
            "1a7c497b4910407683264c671bf31b78",
            "3a23f7036a884541ad750c5cb0a5ead2",
            "650c5a31c09e43da8f7be86c4dcffb4a",
            "c730f25668a04bbab7226180dc3a6471",
            "5f9aade64e054177ab77fa7fdb79c786",
            "448766590b9c41ddbf5a0fc5ef8ea208",
            "0f4efa72f39b4cbfad452aa1e65dce94",
            "9c9dd452d1f24f998b61289af8723386",
            "4d57621f4a9f4991830585f87f9bccde",
            "7fc0d9425ac14cd799b3ae9379aeebc3",
            "a109933da897469f910e06237d447b02",
            "9ce16c6e5cf946818f1073aacceccd94",
            "7bb1a49d49df4005965282e0ded5fc10",
            "bf9db05c82144cbba16d2f16c75217c2",
            "1f2731687c9b4a759cb088f0db6f6487",
            "a3ed0744b794462c9493ab98795d4c3e",
            "fbc8b32f9bc845a79eafab15d615f1cd",
            "a201953a83bf474d94bbe06f3caea020",
            "6d1de7a3789d4fd6a8c3849dbd7f0f9a",
            "5764a27d761147a8b8d18495a08c4098",
            "7d76af341b6f46818a2121182c93f974",
            "1fcbd54b737549a18cc7743f2bf2c956",
            "d0421919b87d4a1fa4a08af9695b738c",
            "2c33c2af9b3143e4b6698e7041f70c9a",
            "5643d3c6ca7a4519b7e1dffc76296724",
            "f95fa0aaebf24da5a2539ea13c3a16e2",
            "e68b547e93284fb29d56c87e44c207ab",
            "1dcdb6bc990545db85c20be697e5a8a7",
            "61f19a2657304e11b62d2a593347a188",
            "e0498a8189c54d4a820c5cf629b1f496",
            "ddc269ab4da64446a8a2afdad8408988"
          ]
        },
        "id": "289rN5mswdw_",
        "outputId": "6069a495-b02b-4358-d7d0-4f11cb8dfa5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 11:10:22] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Starte Hardware-Optimierung 4-bit: r1q15b (deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6582e9dc6b24298a243192e87b5a0d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c42c1c7e7b23460d8fe87825cc103370"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7324b490d5c24038b0dec9abf6b34048"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80dd95ea17f7444ea4d73f00fbeb9bd9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae78acfb40674120bb0725042e9b59e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13aefb92bf604014a054be05168e9d80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32d33a99cf9d4f1e93672feb822a7b2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5dbe685ff3e4531aeb1b3f8c70ca04a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a307f7bdb91748e0a5ca34fc31ea528f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c81f054a3ad42489774a355f604346f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a7bbc2e37ce4c4998541409f5810c91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1da96aa027c5437783ef618443147c01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07819957abde45f9a0e8c54cf67679bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00000-of-00003.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b36125179f5e4543b98f86f42c025cac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00001-of-00003.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "107328d4e28640309f99ea0886cb56b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00002-of-00003.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89009fbb9f4e41189a2a9fc57f7f2d60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/validation-00000-of-00001.parquet:   0%|          | 0.00/474k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb0412c89cf3439f8a577a72a465a093"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/test-00000-of-00001.parquet:   0%|          | 0.00/509k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a547bf4a309d4aa6870da856878da72c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa67324f454b4b9d9a118f69ee03d8c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fc0d9425ac14cd799b3ae9379aeebc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d76af341b6f46818a2121182c93f974"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beispiele gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/deepseek_hardware/deepseek_hardware_4bit_samples_r1q15b.txt\n",
            "\n",
            "Ergebnisse (gesamt):\n",
            "                                    model_id   alias  \\\n",
            "0  deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  r1q15b   \n",
            "\n",
            "                        precision     time_s  energy_kwh    co2_kg  \\\n",
            "0  int4 (NF4, double-quant, sdpa)  67.666614    0.001894  0.000857   \n",
            "\n",
            "   steady_time_s  steady_energy_kwh  steady_co2_kg  kg_per_kwh  tokens_out  \\\n",
            "0      35.728374           0.001073       0.000486    0.452621          96   \n",
            "\n",
            "          ppl      bleu    ram_GB  vram_alloc_GB  vram_reserved_GB  \\\n",
            "0  238.563029  11.30959  4.432037       1.517489          1.583984   \n",
            "\n",
            "                                               notes  \n",
            "0  compile=False; pinned=True; nb=True; MAX_LEN_C...  \n",
            "Gespeichert (gesamt): /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/deepseek_hardware/deepseek_hardware_4bit_results.csv\n",
            "\n",
            "Per-Phase Übersicht:\n",
            "    alias   phase     time_s  energy_kwh    co2_kg\n",
            "3  r1q15b    bleu  27.208193    0.000813  0.000368\n",
            "1  r1q15b     gen   3.180789    0.000102  0.000046\n",
            "2  r1q15b     ppl   5.339391    0.000159  0.000072\n",
            "0  r1q15b  warmup  31.938240    0.000821  0.000372\n",
            "Gespeichert (per Phase): /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/deepseek_hardware/deepseek_hardware_4bit_per_phase.csv\n"
          ]
        }
      ]
    }
  ]
}