{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ino54/MA_GreenAI-Practical-Experiments/blob/main/bloom_framew.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViDJ2HJt2_Dy",
        "outputId": "00a125d0-fea7-41da-ea9d-f8aed27f6655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        " # ---------- Requirements ----------\n",
        "%%writefile requirements.txt\n",
        "transformers>=4.41,<5\n",
        "accelerate>=0.30\n",
        "datasets>=2.19\n",
        "evaluate\n",
        "sacrebleu\n",
        "codecarbon>=2.5,<3\n",
        "pynvml>=11.5.0\n",
        "psutil\n",
        "numpy\n",
        "pandas\n",
        "huggingface_hub\n",
        "optimum>=1.18.0\n",
        "onnx>=1.15.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -r requirements.txt\n",
        "# ONNX Runtime zuerst versuchen, ggf. GPU, sonst CPU\n",
        "try:\n",
        "    import onnxruntime as _ort_test\n",
        "except Exception:\n",
        "    try:\n",
        "        !pip -q install onnxruntime-gpu\n",
        "    except Exception:\n",
        "        !pip -q install onnxruntime\n",
        "!pip uninstall -y -q google-genai firebase-admin || true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g9uLQtT3B6Q",
        "outputId": "33692a4e-4116-4b2a-b53d-2b8ba2ccd088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.6/517.6 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.8/425.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-genai 1.38.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Drive & Ordner ----------\n",
        "import os, shutil, time, platform, gc, math, warnings, inspect\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from google.colab import drive\n",
        "MOUNTPOINT=\"/content/drive\"\n",
        "already=os.path.isdir(os.path.join(MOUNTPOINT,\"MyDrive\"))\n",
        "if not already and os.path.isdir(MOUNTPOINT) and os.listdir(MOUNTPOINT):\n",
        "    backup=f\"/content/drive_stale_{int(time.time())}\"\n",
        "    shutil.move(MOUNTPOINT, backup)\n",
        "    os.makedirs(MOUNTPOINT, exist_ok=True)\n",
        "drive.mount(MOUNTPOINT, force_remount=(not already))\n",
        "\n",
        "work_dir=\"/content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_frameworks\"\n",
        "if not os.path.isdir(work_dir):\n",
        "    raise FileNotFoundError(f\"Zielordner fehlt: {work_dir}\")\n",
        "os.chdir(work_dir)\n",
        "project_dir=work_dir\n",
        "print(\"Arbeitsordner:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdvinBNt3Bvl",
        "outputId": "a4e6852f-12c0-44ec-be32-f6ac222eed10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Arbeitsordner: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_frameworks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- HF Login ----------\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(hf_token); print(\"Hugging Face Login erfolgreich!\")\n",
        "else:\n",
        "    print(\"WARNUNG: Kein HF_TOKEN gefunden.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBuIfslN3B11",
        "outputId": "55ccc892-a519-4a2e-c84a-77b87b8c11d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face Login erfolgreich!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Imports & Device ----------\n",
        "import numpy as np, pandas as pd\n",
        "import torch, psutil\n",
        "from contextlib import nullcontext\n",
        "from types import SimpleNamespace\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "# ORT/Optimum\n",
        "ort_available=False; ort_cuda=False\n",
        "try:\n",
        "    import onnxruntime as ort\n",
        "    from optimum.onnxruntime import ORTModelForCausalLM\n",
        "    ort_available=True\n",
        "    prov=ort.get_available_providers()\n",
        "    ort_cuda=(\"CUDAExecutionProvider\" in prov)\n",
        "    print(f\"[ORT] verfügbar. Provider: {prov}\")\n",
        "    # kleine Compat-Absicherung\n",
        "    if not hasattr(ORTModelForCausalLM, \"_is_stateful\"):\n",
        "        ORTModelForCausalLM._is_stateful=False\n",
        "except Exception as e:\n",
        "    print(\"[ORT] Import/Verfügbarkeit fehlgeschlagen → nur HF-Backend.\", repr(e))\n",
        "\n",
        "# PyTorch Tuning\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True\"\n",
        "set_seed(42)\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device==\"cuda\":\n",
        "    gpu_name=torch.cuda.get_device_name(0)\n",
        "    vram_total_gb=torch.cuda.get_device_properties(0).total_memory/(1024**3)\n",
        "    torch.backends.cuda.matmul.allow_tf32=True\n",
        "    torch.backends.cudnn.benchmark=True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except: pass\n",
        "else:\n",
        "    gpu_name=\"CPU\"; vram_total_gb=0.0\n",
        "print(f\"Device: {device} | GPU: {gpu_name} | VRAM={vram_total_gb:.1f} GB | Torch {torch.__version__} | Py {platform.python_version()}\")\n",
        "\n",
        "RESULT_BASENAME=\"bloom_frameworks\"\n",
        "PINNED_MEM=True; NON_BLOCKING=True; MAX_LEN_CAP=128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEWiCj-q3XA-",
        "outputId": "cb0ff3e0-9e9e-4fa6-8b26-b90bd74d49b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ORT] verfügbar. Provider: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "Device: cuda | GPU: NVIDIA A100-SXM4-80GB | VRAM=79.3 GB | Torch 2.8.0+cu126 | Py 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- CodeCarbon ----------\n",
        "import pandas as _pd\n",
        "def _cleanup_cc_locks():\n",
        "    for p in [\"/tmp/.codecarbon.lock\",\n",
        "              os.path.expanduser(\"~/.codecarbon/codecarbon.lock\"),\n",
        "              \"/content/.codecarbon/codecarbon.lock\"]:\n",
        "        try:\n",
        "            if os.path.exists(p): os.remove(p)\n",
        "        except: pass\n",
        "os.environ[\"CODECARBON_CACHE_DIR\"]=f\"/content/.cc_cache_fw_{int(time.time())}\"\n",
        "\n",
        "def _cc_supported_kwargs():\n",
        "    base=dict(log_level=\"error\", output_dir=\".\", measure_power_secs=1, tracking_mode=\"process\")\n",
        "    try:\n",
        "        params=inspect.signature(EmissionsTracker.__init__).parameters\n",
        "        if \"allow_multiple_runs\" in params: base[\"allow_multiple_runs\"]=True\n",
        "        if \"cloud_provider\" in params: base[\"cloud_provider\"]=\"google\"\n",
        "        if \"cloud_region\" in params: base[\"cloud_region\"]=\"europe-west10\"\n",
        "        if \"country_iso_code\" in params: base[\"country_iso_code\"]=\"DEU\"\n",
        "    except: pass\n",
        "    return base\n",
        "def make_trk(name,out):\n",
        "    _cleanup_cc_locks()\n",
        "    return EmissionsTracker(project_name=name, output_file=out, **_cc_supported_kwargs())\n",
        "def start(tr):\n",
        "    try: tr.start(); return True\n",
        "    except:\n",
        "        _cleanup_cc_locks()\n",
        "        try: tr.start(); return True\n",
        "        except: return False\n",
        "def stop(tr, st):\n",
        "    if not st: return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "    try: return tr.stop()\n",
        "    except: return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "def unpack(em):\n",
        "    if hasattr(em,\"energy_consumed\") and hasattr(em,\"emissions\"):\n",
        "        try: return float(em.energy_consumed), float(em.emissions)\n",
        "        except: return 0.0,0.0\n",
        "    if isinstance(em, dict):\n",
        "        e=em.get(\"energy_consumed\",0.0); c=em.get(\"emissions\", em.get(\"emissions_kg\",0.0))\n",
        "        try: return float(e), float(c)\n",
        "        except: return 0.0,0.0\n",
        "    try: return 0.0, float(em)\n",
        "    except: return 0.0,0.0\n",
        "def read_energy(path):\n",
        "    try:\n",
        "        if not os.path.exists(path): return 0.0\n",
        "        df=pd.read_csv(path)\n",
        "        for c in [\"energy_consumed\",\"energy_consumed_kwh\",\"energy_consumed (kWh)\",\"energy (kWh)\"]:\n",
        "            if c in df.columns: return float(df[c].iloc[-1])\n",
        "        for c in df.columns:\n",
        "            if \"energy\" in c.lower() and \"kwh\" in c.lower(): return float(df[c].iloc[-1])\n",
        "    except: pass\n",
        "    return 0.0\n",
        "def measure(phase, fn, prefix):\n",
        "    logfile=os.path.join(project_dir, f\"{prefix}_{phase}.csv\")\n",
        "    tr=make_trk(f\"{prefix}_{phase}\", logfile)\n",
        "    import time as _t\n",
        "    st=start(tr); t0=_t.time(); res=fn(); t1=_t.time()\n",
        "    em=stop(tr, st); ekwh,co2=unpack(em)\n",
        "    if ekwh==0.0:\n",
        "        ek=read_energy(logfile)\n",
        "        if ek: ekwh=ek\n",
        "    return {\"phase\":phase,\"time_s\":t1-t0,\"energy_kwh\":ekwh,\"co2_kg\":co2}, res\n",
        "\n",
        "# ---------- Eval-Config ----------\n",
        "MODELS=[(\"bigscience/bloom-560m\",\"bloom560m\",\"b560\"),\n",
        "        (\"bigscience/bloom-3b\",\"bloom3b\",\"b3b\")]\n",
        "EVAL={\"max_new_tokens\":32,\n",
        "      \"ppl\":{\"name\":\"wikitext\",\"config\":\"wikitext-2-raw-v1\",\"split\":\"test[:1%]\"},\n",
        "      \"bleu\":{\"name\":\"wmt14\",\"config\":\"de-en\",\"split\":\"test[:32]\"}}\n",
        "PROMPTS=[\"Schreibe einen kurzen Absatz über nachhaltige KI.\",\n",
        "         \"Erkläre in einfachen Worten, was Quantisierung in neuronalen Netzen ist.\",\n",
        "         \"Nenne drei Vorteile von Mixture-of-Experts-Modellen.\"]\n",
        "bleu_metric=evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def autocast_ctx():\n",
        "    return torch.autocast(device_type=\"cuda\", dtype=torch.float16) if torch.cuda.is_available() else nullcontext()\n",
        "def safe_max_len(tok, model, fallback=2048, upper=100000):\n",
        "    cand=getattr(tok,\"model_max_length\",None)\n",
        "    if isinstance(cand,int) and 0<cand<upper: return cand\n",
        "    cand=getattr(getattr(model,\"config\",None),\"max_position_embeddings\",None)\n",
        "    if isinstance(cand,int) and 0<cand<upper: return cand\n",
        "    return fallback\n",
        "def capture_memory():\n",
        "    ram=psutil.Process().memory_info().rss\n",
        "    valloc=torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "    vres =torch.cuda.memory_reserved()  if torch.cuda.is_available() else 0\n",
        "    return ram, valloc, vres\n",
        "def bytes_to_gb(b): return float(b)/(1024**3)\n",
        "\n",
        "def _pin_and_move(batch, device):\n",
        "    out={}\n",
        "    for k,v in batch.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            try:\n",
        "                if v.device.type==\"cpu\": v=v.pin_memory()\n",
        "            except: pass\n",
        "            out[k]=v.to(device, non_blocking=True)\n",
        "        else:\n",
        "            out[k]=v\n",
        "    return out\n",
        "def get_model_device(model):\n",
        "    try: return next(model.parameters()).device\n",
        "    except: return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------- Loader: HF ----------\n",
        "from transformers import AutoModelForCausalLM\n",
        "def load_hf_eager(model_id:str):\n",
        "    tok=AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    tok.padding_side=\"left\"; tok.pad_token=tok.eos_token\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "    model=AutoModelForCausalLM.from_pretrained(\n",
        "        model_id, device_map=\"auto\",\n",
        "        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        low_cpu_mem_usage=True,\n",
        "        attn_implementation=\"eager\",\n",
        "    )\n",
        "    try: model.config.use_cache=True\n",
        "    except: pass\n",
        "    model.eval()\n",
        "    return tok, model, \"hf_eager\"\n",
        "\n",
        "# ---------- Loader: ORT (ONNX-Cache, labels-freies forward) ----------\n",
        "def load_ort(model_id:str):\n",
        "    if not ort_available: raise RuntimeError(\"ORT/Optimum nicht verfügbar.\")\n",
        "    tok=AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    tok.padding_side=\"left\"; tok.pad_token=tok.eos_token\n",
        "    providers=[\"CUDAExecutionProvider\",\"CPUExecutionProvider\"] if ort_cuda else [\"CPUExecutionProvider\"]\n",
        "    onnx_dir=os.path.join(project_dir, f\"onnx_{model_id.split('/')[-1]}\")\n",
        "    if os.path.isdir(onnx_dir) and len(os.listdir(onnx_dir))>0:\n",
        "        model=ORTModelForCausalLM.from_pretrained(\n",
        "            onnx_dir, provider=providers[0], providers=providers,\n",
        "            use_cache=False, use_io_binding=False  # wichtig\n",
        "        )\n",
        "        note=f\"ort_{'cuda' if ort_cuda else 'cpu'}(cached)\"\n",
        "    else:\n",
        "        model=ORTModelForCausalLM.from_pretrained(\n",
        "            model_id, export=True, provider=providers[0], providers=providers,\n",
        "            use_cache=False, use_io_binding=False\n",
        "        )\n",
        "        try:\n",
        "            os.makedirs(onnx_dir, exist_ok=True)\n",
        "            model.save_pretrained(onnx_dir)\n",
        "        except: pass\n",
        "        note=f\"ort_{'cuda' if ort_cuda else 'cpu'}(exported)\"\n",
        "    return tok, model, note\n",
        "\n",
        "def _is_ort_model(m):\n",
        "    try:\n",
        "        return ort_available and isinstance(m, ORTModelForCausalLM)\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# ---------- Generation ----------\n",
        "def simple_generate(model, tok, prompts, max_new_tokens=32):\n",
        "    dev=get_model_device(model)\n",
        "    ml=min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    enc=tok(prompts, return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "    if dev.type==\"cuda\": enc=_pin_and_move(enc, dev)\n",
        "    room=ml-enc[\"input_ids\"].shape[1]\n",
        "    cur_new=max(1, min(max_new_tokens, int(room)))\n",
        "    gen_kwargs=dict(max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "    if _is_ort_model(model): gen_kwargs[\"use_cache\"]=False  # wichtig\n",
        "    with torch.inference_mode(), autocast_ctx():\n",
        "        out=model.generate(**enc, **gen_kwargs)\n",
        "    texts, total= [], 0\n",
        "    max_in=int(enc[\"input_ids\"].shape[1]); pad_id=tok.pad_token_id; eos_id=tok.eos_token_id\n",
        "    for i in range(out.size(0)):\n",
        "        seq=out[i]; gen_slice=seq[max_in:]; gen_i=0\n",
        "        for t in gen_slice.tolist():\n",
        "            if t==eos_id: gen_i+=1; break\n",
        "            if pad_id is not None and t==pad_id: break\n",
        "            gen_i+=1\n",
        "        total+=gen_i; texts.append(tok.decode(seq, skip_special_tokens=True))\n",
        "    return texts, total\n",
        "\n",
        "# ---------- Perplexity ----------\n",
        "import torch.nn.functional as F\n",
        "def eval_perplexity(model, tok, ds_cfg):\n",
        "    dev=get_model_device(model)\n",
        "    ds=load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    ml=min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    losses=[]\n",
        "    with torch.inference_mode(), autocast_ctx():\n",
        "        for t in ds[\"text\"]:\n",
        "            if not isinstance(t,str) or len(t.strip())<4: continue\n",
        "            enc=tok(t, return_tensors=\"pt\", truncation=True, max_length=ml)\n",
        "            if dev.type==\"cuda\": enc=_pin_and_move(enc, dev)\n",
        "            if _is_ort_model(model):\n",
        "                # ORT: kein labels-Forward → Loss manuell\n",
        "                out=model(input_ids=enc[\"input_ids\"],\n",
        "                          attention_mask=enc.get(\"attention_mask\"),\n",
        "                          use_cache=False)\n",
        "                logits=out.logits[..., :-1, :]\n",
        "                labels=enc[\"input_ids\"][..., 1:].clone()\n",
        "                if \"attention_mask\" in enc:\n",
        "                    mask=enc[\"attention_mask\"][..., 1:]\n",
        "                    labels[mask==0]=-100\n",
        "                loss=F.cross_entropy(logits.transpose(1,2), labels, ignore_index=-100)\n",
        "            else:\n",
        "                out=model(enc[\"input_ids\"], labels=enc[\"input_ids\"])\n",
        "                loss=out.loss\n",
        "            losses.append(float(loss.detach().cpu()))\n",
        "    return math.exp(np.mean(losses)) if losses else None\n",
        "\n",
        "# ---------- BLEU ----------\n",
        "def eval_bleu_llm(model, tok, ds_cfg, max_new_tokens=32, batch_size=8):\n",
        "    dev=get_model_device(model)\n",
        "    ds=load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    ml=min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    preds, refs=[], []\n",
        "    with torch.inference_mode(), autocast_ctx():\n",
        "        batch_prompts, batch_refs=[], []\n",
        "        for ex in ds:\n",
        "            de,en=ex[\"translation\"][\"de\"], ex[\"translation\"][\"en\"]\n",
        "            prompt=f\"Translate to English:\\nGerman: {de}\\nEnglish:\"\n",
        "            batch_prompts.append(prompt); batch_refs.append(en)\n",
        "            if len(batch_prompts)>=batch_size:\n",
        "                enc=tok(batch_prompts, return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "                if dev.type==\"cuda\": enc=_pin_and_move(enc, dev)\n",
        "                room=ml-enc[\"input_ids\"].shape[1]\n",
        "                cur_new=max(1, min(max_new_tokens, int(room)))\n",
        "                gen_kwargs=dict(max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "                if _is_ort_model(model): gen_kwargs[\"use_cache\"]=False\n",
        "                out=model.generate(**enc, **gen_kwargs)\n",
        "                for i in range(out.size(0)):\n",
        "                    gen=tok.decode(out[i], skip_special_tokens=True)\n",
        "                    hyp=gen.split(\"English:\")[-1].strip().split(\"\\n\")[0].strip() or gen.strip()\n",
        "                    preds.append(hyp); refs.append([batch_refs[i]])\n",
        "                batch_prompts, batch_refs=[], []\n",
        "        if batch_prompts:\n",
        "            enc=tok(batch_prompts, return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "            if dev.type==\"cuda\": enc=_pin_and_move(enc, dev)\n",
        "            room=ml-enc[\"input_ids\"].shape[1]\n",
        "            cur_new=max(1, min(max_new_tokens, int(room)))\n",
        "            gen_kwargs=dict(max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "            if _is_ort_model(model): gen_kwargs[\"use_cache\"]=False\n",
        "            out=model.generate(**enc, **gen_kwargs)\n",
        "            for i in range(out.size(0)):\n",
        "                gen=tok.decode(out[i], skip_special_tokens=True)\n",
        "                hyp=gen.split(\"English:\")[-1].strip().split(\"\\n\")[0].strip() or gen.strip()\n",
        "                preds.append(hyp); refs.append([batch_refs[i]])\n",
        "    return float(bleu_metric.compute(predictions=preds, references=refs)[\"score\"])\n",
        "\n",
        "# ---------- Runner ----------\n",
        "def run_backend(model_id:str, alias:str, backend:str):\n",
        "    print(f\"\\n### Starte Framework: {backend} | {alias} ({model_id})\")\n",
        "    if backend==\"hf\":\n",
        "        loader=load_hf_eager\n",
        "    elif backend==\"ort\":\n",
        "        if not ort_available:\n",
        "            print(\"[ORT] nicht verfügbar → Backend übersprungen.\"); return None, []\n",
        "        loader=load_ort\n",
        "    else:\n",
        "        raise ValueError(\"Unbekanntes Backend\")\n",
        "\n",
        "    def _do_load():\n",
        "        tok, model, note=loader(model_id)\n",
        "        dev=get_model_device(model); ml=min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "        wenc=tok([\"Warmup token 1\",\"Warmup token 2\"], return_tensors=\"pt\",\n",
        "                 truncation=True, max_length=ml, padding=True)\n",
        "        if dev.type==\"cuda\": wenc=_pin_and_move(wenc, dev)\n",
        "        gen_kwargs=dict(max_new_tokens=1, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "        if _is_ort_model(model): gen_kwargs[\"use_cache\"]=False\n",
        "        with torch.inference_mode(), autocast_ctx():\n",
        "            _=model.generate(**wenc, **gen_kwargs)\n",
        "        return tok, model, note\n",
        "\n",
        "    m_warm, (tok, model, backend_note)=measure(\"warmup\", _do_load, f\"{RESULT_BASENAME}_{alias}_{backend}\")\n",
        "\n",
        "    m_gen, (samples, n_tok)=measure(\"gen\",\n",
        "        lambda: simple_generate(model, tok, PROMPTS, EVAL[\"max_new_tokens\"]),\n",
        "        f\"{RESULT_BASENAME}_{alias}_{backend}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    m_ppl, ppl=measure(\"ppl\",\n",
        "        lambda: eval_perplexity(model, tok, EVAL[\"ppl\"]),\n",
        "        f\"{RESULT_BASENAME}_{alias}_{backend}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    m_bleu, bleu=measure(\"bleu\",\n",
        "        lambda: eval_bleu_llm(model, tok, EVAL[\"bleu\"], EVAL[\"max_new_tokens\"], batch_size=8),\n",
        "        f\"{RESULT_BASENAME}_{alias}_{backend}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    total_time=m_warm[\"time_s\"]+m_gen[\"time_s\"]+m_ppl[\"time_s\"]+m_bleu[\"time_s\"]\n",
        "    total_energy=m_warm[\"energy_kwh\"]+m_gen[\"energy_kwh\"]+m_ppl[\"energy_kwh\"]+m_bleu[\"energy_kwh\"]\n",
        "    total_co2=m_warm[\"co2_kg\"]+m_gen[\"co2_kg\"]+m_ppl[\"co2_kg\"]+m_bleu[\"co2_kg\"]\n",
        "\n",
        "    ram,valloc,vres=capture_memory()\n",
        "    per_phase=[m_warm,m_gen,m_ppl,m_bleu]\n",
        "    for p in per_phase:\n",
        "        p[\"alias\"]=alias; p[\"model_id\"]=model_id; p[\"backend\"]=backend_note\n",
        "\n",
        "    row=dict(\n",
        "        model_id=model_id, alias=alias, backend=backend_note,\n",
        "        precision=f\"{'fp16' if device=='cuda' else 'fp32'}\",\n",
        "        time_s=total_time, energy_kwh=total_energy, co2_kg=total_co2,\n",
        "        kg_per_kwh=(total_co2/total_energy) if total_energy else None,\n",
        "        tokens_out=int(n_tok), ppl=ppl, bleu=bleu,\n",
        "        ram_GB=bytes_to_gb(ram), vram_alloc_GB=bytes_to_gb(valloc), vram_reserved_GB=bytes_to_gb(vres),\n",
        "        notes=f\"MAX_LEN_CAP={MAX_LEN_CAP}; pinned={PINNED_MEM}; nb={NON_BLOCKING}\"\n",
        "    )\n",
        "\n",
        "    samples_path=os.path.join(project_dir, f\"{RESULT_BASENAME}_samples_{alias}_{backend}.txt\")\n",
        "    with open(samples_path,\"w\",encoding=\"utf-8\") as f:\n",
        "        for i,txt in enumerate(samples,1):\n",
        "            f.write(f\"--- Beispiel {i} ({alias}, {backend_note}) ---\\n{txt}\\n\\n\")\n",
        "    print(\"Beispiele gespeichert:\", samples_path)\n",
        "\n",
        "    return row, per_phase"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3faeedfb3ae94a498b3cefd0cc26e902",
            "58c72a4eec48427ea62d626d99984632",
            "eaa9a6757fc442fcadb7071218d24086",
            "670d67f4db634e9f9f166be0575903f4",
            "6a6425c354114215991cdb8b2fe3766f",
            "c70d424973744a39a37ea5fc125dc1ef",
            "a342c5cd668f446e8cac950602547f63",
            "2551dee5200347aa9a5acfc5489ac41c",
            "76212e0a4b3b477791d1edc65cb6d69c",
            "2aea93eafd9343d8bcc5be29342ac22c",
            "2b246fc8cf1a4c14b2b1efd5ef099869"
          ]
        },
        "id": "JbUCmxJ03W93",
        "outputId": "718a25e4-48e0-4731-c122-7a4689b892c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3faeedfb3ae94a498b3cefd0cc26e902"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Ausführen & Speichern ----------\n",
        "all_rows, all_phases=[], []\n",
        "backends=[\"hf\",\"ort\"]\n",
        "for mid, long_name, alias in MODELS:\n",
        "    for be in backends:\n",
        "        res=run_backend(mid, alias, be)\n",
        "        if res is None: continue\n",
        "        row, phases=res\n",
        "        if row is None: continue\n",
        "        all_rows.append(row); all_phases.extend(phases)\n",
        "\n",
        "df=pd.DataFrame(all_rows).sort_values([\"alias\",\"backend\"]).reset_index(drop=True)\n",
        "dfp=pd.DataFrame(all_phases)\n",
        "dfp[\"wh_total\"]=dfp[\"energy_kwh\"]*1000.0\n",
        "dfp[\"phase\"]=pd.Categorical(dfp[\"phase\"], categories=[\"warmup\",\"gen\",\"ppl\",\"bleu\"], ordered=True)\n",
        "\n",
        "out_csv=os.path.join(project_dir, f\"{RESULT_BASENAME}_results.csv\"); df.to_csv(out_csv, index=False)\n",
        "out_phase_csv=os.path.join(project_dir, f\"{RESULT_BASENAME}_per_phase.csv\"); dfp.to_csv(out_phase_csv, index=False)\n",
        "\n",
        "print(\"\\nErgebnisse (gesamt):\"); print(df); print(\"Gespeichert (gesamt):\", out_csv)\n",
        "print(\"\\nPer-Phase Übersicht:\")\n",
        "print(dfp[[\"alias\",\"backend\",\"phase\",\"time_s\",\"energy_kwh\",\"co2_kg\"]].sort_values([\"alias\",\"backend\",\"phase\"]))\n",
        "print(\"Gespeichert (per Phase):\", out_phase_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dd01af7c1cd54fd1ac449298b3b2be59",
            "0e809397cd4d486197b6b2e970cb3f76",
            "f6ec87947eec4c17ac644723495571d8",
            "85ef4611b55e4a7fb80ba710ed5a4dfe",
            "d6d51d2e88434da68c1b86edb0c61960",
            "a6408e4bf0aa454ab603d775ff4f1911",
            "af670b6047524e5e85aa1eef4199afa3",
            "ab59679998e64ce1abfabb2963ecbd5b",
            "e80b448e85c04c8d9a04df6b648d373c",
            "6e346d959f29470eb4f059fc941136a8",
            "39db00bf7deb43c7be330c8f083eac69",
            "8c9a96391db34e6eb90efe6a678f835f",
            "e9b5164295774172b6eb278678cc4e6e",
            "be74c1f71bdd47fba0d3f77282b2ef6b",
            "73b0f01abb3345bca0151bcea739af2d",
            "46457d5a8d914f59bed948c7bc1cd787",
            "7e6a1d0eed2a4d4b918635696ef9504f",
            "f3880f7463274254bedb9704d096e027",
            "0eee5653512146bd9e47acda6cce3410",
            "3e97a53adc7049c080a7b80d60105cdc",
            "a0054dbfe366425e9456413910cdf823",
            "ed27e623fe6245219478ce9781a4535a",
            "6c9a9afb929b484685a7584c05570091",
            "c9a74d62be3e4b0983afee85216deacc",
            "2e84cc6d19c045e29037338f4d18ba20",
            "a1d978471cd440e481fd12db94d174f7",
            "f4e46c353acf4679bd260f9bd92aa956",
            "db53c6129eb4484e870fe887187772ef",
            "8426c1ccdd71499bb345727313033a44",
            "ce8cbfba6d5348ff841ea1f2d96c4c1a",
            "bf6be5b2ee1e4ef88b75c9ab093231a5",
            "dbe035c7d7cd47d393ee4b0d40f64152",
            "0171298558df457a93bd4f2750d3443d",
            "124354e6fc594168aa96cee5822ebdd2",
            "a1939c00975a4529bedc457660059806",
            "45fdcac551684c5ebd6d41bf3b04a322",
            "dcb6f126fb634fceb35deb08dd188718",
            "267d08834f1242f9b169657689ac7964",
            "db7dc1c20fed45a79f2b98778b4d32ad",
            "0f8aca4d4e2049f2934c13c70ff75cf2",
            "9a714bbb225f4425b86b4f7c96a47e67",
            "90cadbd2707f454c9f1b6009fd4b1d4c",
            "82bb72369c3f48d194963a19eea07658",
            "16bd39fa0ff9445ea68eae5b4b428034",
            "b51801d71d534411a9bc27ae971b71c6",
            "fec5c53c08e14c0ba1908daa1289ff26",
            "6ee6835e6e2e4dbb94546f6f4604ff24",
            "e01cf4db5b144c82813820c1daf50abc",
            "543236e3bc9c448d8d5be1249dc4b5fa",
            "449a5c7bc80041d48882e37ca24367fa",
            "cad9888c82434be897eaba81b947fa7d",
            "df65de0fdde94b849de72df3239f54aa",
            "1e55e213c327405aa078b570207b831e",
            "10211d6f5c9143df854e594becdceded",
            "90583fe02b4d49748ec5f3341876b419",
            "45aa2529d1f84cf7b97de726e394dd7a",
            "dc289dcd7f9949ef9f3eb35ef8a90fdb",
            "4dd87fc1cf924c4b9c2c058a2ec78823",
            "4cda308c485a4ff19fdfe9f3030f8a73",
            "dd48e749313044ca8e54530140bc59fb",
            "2d793b92475b407c86fa2f568e4258ee",
            "b33328455b16404997baefdd90554592",
            "e92649ddb9224f9b85411de2171ce6c0",
            "af2b039e134f46e288172a018f49771f",
            "aa953dc966284a30a79263b3b536b1b5",
            "33f843e3b5cd4ab699147efe6aa1b23d",
            "7340e2ce26804c6fb5a56422dbc8bcb4",
            "8b276d129e5b4a4a999a8af50c423942",
            "e1b0bf29d5644f41af84c7849b5ed3db",
            "902ddce7e9064591a7bade115b387093",
            "03ea2b89a2c0452991aa1644a31220b3",
            "4e3d616d47dc4be58b8283cfe1baf2dc",
            "df64ed0fce004fe49e2e731ed11d5d55",
            "9b02692db18a46788907a1b195b0fa4d",
            "cac3b23033a64576a4bdf950a60508fe",
            "8172bf672f3844d4a57a7a975dee1250",
            "0ce9eb6f099c4ac9bbf932ba7f0672be",
            "92cbafe5d7cf45daa6bdff4f958e1d57",
            "df7eb62435614aa68b3da27da6291c53",
            "55d2c7cc150245f99ddcb4d4579528e8",
            "3c39a7e0a9ce4455aa6042ddf67815aa",
            "b0de9ce4bb2d4409872166e20dea7cc2",
            "23c0f25fc4604601ae3b5a6f5f2a0e7b",
            "71f26d08caca415984c800a861c2cd81",
            "4e387e2532874dda9dee4915677050dc",
            "dd9fd71b83bc4732af6c51f6183ffde5",
            "d2aa9a96e0b34694b6ef4825e5a58883",
            "c8d90f23691447498698808df205c53c",
            "ecbd01861fa44af1a277052cd02d6055",
            "51487913a4024cc3bf598d7bb466378d",
            "31db5845f94a45a3a0603777f4c3817a",
            "5d36f73116bc4e27b82fff152531d19f",
            "11941a07a3dc4c92bba7565f9df291da",
            "e93d1195199d4b489583581a451296c5",
            "d04baa6c37fc4dfdad77fbc9c320fb48",
            "253d1dbba5af42168074c85d01a9fa9d",
            "7f046d3c956d4410a06c535578ab15aa",
            "51019265d60545539c4dfa5d1c85a185",
            "96457ccb7910427694a07dedbc749bcf",
            "184febe372134fdfa509f656ca7b84be",
            "c7a7d7beeeb4493d8adf981e58743b63",
            "4c09aeff763546fea2974d59f34c58d7",
            "890177c1172845b1bc7bd7b935435fbe",
            "a359228b0a2a4bf3bbe164d86e6ac076",
            "cbdd3b7f0d47411091c9b573fc7b1770",
            "c160ddf86d0b4ff8a0678126f9f8067c",
            "49eea1c537684e2284c8b0c95b6e8b23",
            "0dcf703071264ebd853a2d355d7863f9",
            "4c391a6b3d41469cbcc67324de41aa87",
            "1ee0648367fb49898f33a3eb29d44943",
            "488226225ce04225b44e2337d8fb0e3b",
            "4c55a85fcb9b462186fe60e9cd3f5341",
            "b8f030b657b44e2fb975b89c75c7170f",
            "03612549e94f4440b602e084213ef429",
            "34af3c7e1333409b9fb6caca0e50be91",
            "562025aebc274bd0a10f739567ea8a37",
            "8f94e4a8e68141e29c2d52205d7ec344",
            "f41e5033b1444f8b9a38113119833389",
            "712c4e23c66240a59d5a11c1e84dddfc",
            "e7279a3678a84188aeaf06987cbdce17",
            "a051869f1a884c4680ed386d4929eeff",
            "357eb7ce020d4278aea35b0851946c8c",
            "011873e144ba4c7bb955a4ad1005bd27",
            "9d12eddf52354163a4188576f098f27a",
            "e13c80f06be44fa5a788432d68741a2c",
            "dde46f7981cf4189bc75a6185e52fa13",
            "7bbdbebeee8145b8b376513c48d86ec4",
            "64138247a85f4d659a6176d2464a76e8",
            "d2d85b291f24436185105fe8d36cc62f",
            "6a50f00065ae4e068a23548f18fdf553",
            "f886dfedd86f466fb57ef59e363975e7",
            "13230f920b924377af5f2c6422c77e71",
            "8ee28a8d82b946d7a0d66f575b7157a1",
            "0c0ba3f9a19c4540ab634125c48792e9",
            "d21fe26a0456436baf7c55b6a72b58fd",
            "e1f055697afc41f5a3dd79ccc901f486",
            "0a55f6666e7e4f9a9d82d5b48da65eeb",
            "68bc2ee46dce42139684f0fc791ac991",
            "e067f1a20d0a4cd587cf5782b94f5c47",
            "a1085175d4514d818e5ea3cac67cf229",
            "64b8dd6ec1b94776b7ef84ff35ddf041",
            "209085a6725f4c69b5f3e1415b389b7c",
            "131963f1fa4c4861b43b73cd560a2697",
            "c84d397b6750404495175db180957929",
            "3eb8a6219f9245a98ff8a84369a4ad37",
            "b4c4ceea18bc4f5aa06e3eb579eea551",
            "b472c99818c84f6596e9ee760cda8a8a",
            "4654ff00d49346ef9080fc0e2527f715",
            "baa8afb882e84d1ca468021c40180548",
            "d760949a67a44a07875b3b92db5c7256",
            "d9d0893906384c09be5bbed6be12b6e6",
            "9aafb8a8a40042228346473f995ade22",
            "7ef273dc2fde40808910c4ba97c1413a",
            "15850bce9a944e62aada17e154af9903",
            "2352129532aa4a6fa2c6c43a877fa9db",
            "274bac9ad2be41f699c09b3caf825814",
            "c0ba1c4a41394cbba1fe2a83d820b85d",
            "a1271a7bdbb14006947cda35df47f3da",
            "48ff500c68ea4dad8519eeabaa713ff2",
            "4dd6dcec43804b1ea6907e278e62abf3",
            "f3e1d106d4c54b2abbef8b8022c166df",
            "63ee62a396a24d829640ca1f10b000bd",
            "bfd879058143486da26e11ff52c05307",
            "516209cc8a824dbbbc4224f189636ba9",
            "cbdb2b6d2b1e4012b9b000a2d2016afc",
            "4cca673eae6a464da3506a0f487e7eda",
            "35a4eaf8d50d411da42a4bd5a0f0ffd1",
            "66ef502a8ddd49b2897ead310e888ae5",
            "8103a2fe4daa4c33b8d403f068c63fb3",
            "ae78c7ab649e4df9a0dd37f3f50ec10f",
            "946c45e209e94b3197c9383aaa4dc9db",
            "a11bc2901264434aa20da4faad4a8c9b",
            "6c40b29a1d294e07920ddc6b086a41f4",
            "be710293d242485ca942fde67b44489d",
            "626e0a903bac47bba04245db09a12d4d",
            "40ade023aac8459bb692a259a69452f3",
            "1004f7a67d63445cb16b922257464380",
            "79e2e8d356d848f4bb8f1600e6513d4b",
            "3401e30def734d52ab1310d9785833b8",
            "ac104151365c43818e665b77fb7843d2",
            "95ed6253099b44f6b926337dad25a0a4",
            "abc0086420734aec8d8f2ba5f9983bb4",
            "2ede9d43f91549c89cdd0c9bd361fe3e",
            "2476ffd32fa54bbf9b7d75edf8da6e40",
            "cab6e56e81624f46aa9f4f0c8852f9cb",
            "7fc3c74ffda1479f90d7bc9fb8f92b93",
            "4510f9ab98434b05a4332fe4282215df",
            "c4a580e757bb4715aeeadcbbf0ef5959",
            "c46f11b2510140f3aa5fc9ab71ac76c7",
            "edc9b3771ad349cc92d0ba97978e5551",
            "cb2a06ac258f47a8b89cd0cae61c3045",
            "610a5c7a206643d1bc0eb02eb2842c0b",
            "9abd21e4bf9c45fd80aa7609c6948a47",
            "f556dcfd2b7940a5a500436363da102a",
            "ba54d8b0458e4950adc850b1c4c58c5d",
            "3d67667bcf924af9932bc92eb4625623",
            "aff73fe8c2494d4fae144502c59e59b4",
            "e9450f12c5f4441194e25bdd65731d61",
            "7e04bde7e1294d7985be8cb958475706",
            "6f765e4e7db749c2a5cda9bb2e564fd8",
            "27b6c472adf34364a09883a231475210",
            "a76583c42b7a4fae89b93132c213b13e",
            "80e2be6694c44726b7a7afe76c209da9",
            "dbaa1aa7be6d4004b9a49e9aa2d54b89",
            "a3e55e79ced74b659eda143357718c3e",
            "b55d8fbd66e449deb6a2a43b286c397f",
            "615b8b16447b4961a2dff95de0c8d01b",
            "aa07970f183049a1aa56418661b11549",
            "92e2a1c496794854892777ee9fb0da2d",
            "cedea7c0cf14472cb911c7ea841d7374",
            "237509150f1248f2b51e9fc0054c0365",
            "c70e01cb2cba42879dee62c1915cafcb",
            "51eed94fe6784192b2566ab3449a993a",
            "f42be3773f164202a753b48c065a4c27",
            "d134b11031fe485c9cef1b99f4bd2528",
            "6b84afa5b201474a8ce9f20373187b7b",
            "f5cadb74328c45069fbe50be04e4de91",
            "6e951bfe789247749364c03808e4e87c",
            "eb8bf1220d754e748ef929d3ecbaeaed",
            "0d056b188f4d4416afb6b112d778d75e",
            "c456fdaa2bb148da8773dc8cc2cdca20",
            "0ca8960e15bf4451a34878a18057b92b",
            "eeecd7d9624e4413b643a76db8ade38b",
            "00ab925153f840d58a9e95009d586e7c",
            "fe9b5ad60c3749b48ae36ed35fff7db6",
            "574163ef14e1497baeb752fc00101ce3",
            "9483c6b4b023455cae8a29e33d3bf18d",
            "80ed64648900465691e4d9e4ab510ab5",
            "fe61102915394e48b0f7c9f17acbaf46",
            "ee0bdac43eb848b2a85b9a8cb3bfaa68",
            "6a78f78cd6a948d3a72dcac29e6f57cf",
            "9e8e21a3db214cf396a0ff97466e2d5f",
            "89432ccbd2bf45c8a2917aa3fd077bcc",
            "8647fb59a5f14b00947e552f8ee86dc4",
            "125a94ef94ba4e4784b65801147cd4dd",
            "475461d846ab40e4bef71eee55a83ec8",
            "6f18446dc345496bbbdff9bb2f06b272",
            "a800b0ab255a4e83b726c9e1bf01966c",
            "97167d98708a402ba04b904c29acbaff",
            "dcc9c2f669d34dc9827c5ef6ee711add",
            "eba44b811e3742cb84c014c940a380cb",
            "400bd1add89b427c9d0a069ba31b80c6",
            "efc6a0ce188e45a683471d6b3286f17c",
            "2c79d7c6ff1e460e8c53b7dce74cef99",
            "1bdf836a22764c7c911f90a559f34dc3",
            "3ce80f92abae4a4fb4574c0551cc4ec5",
            "27d522e8459044c4adba84ac0b4aeb97",
            "accfe2a0224848e1a7e0ac9b22c85c99",
            "8c0036ad61104c4a9dcf1e873fb8d0a6",
            "1ac7327fb65e448d98554fa44f3aeef1",
            "2dde36cd4cef40cf8f41959bcf599c2d",
            "cf5ebafa10854440b8ac4e2298e3b7c6",
            "95caedbd05eb46a29cd6075e3fd60d41",
            "715bca35a5a74cb8b29d79fabacc609e",
            "8b45fdce63fe4f038cb619d900fcd043",
            "205ad55a20064102806de77249dc74c4",
            "4aef0b61742d4600b3ef809e12983002",
            "15c94f4d92bc4a7c80db2a7532449045",
            "f1e4f799b6384d969864bcf956eb7cae",
            "5b1895d15cf84dda918cd62e4bb58b84",
            "2db5978a37ec443188c90af389ed6dae",
            "d63be7404e4a40eba57c220431e71e68",
            "253d9c46293848c6959f843543cacb06",
            "5c62394e33024e8991cb9f621ff842da",
            "47639169bf774b32ba25386e1e95d868",
            "f63a1c7b474842bebe9ac34a1448b42a",
            "29cee8a221864d849743704203679c8a",
            "25dc10c8666649e49e5dc931734619ce",
            "79d4f0e61947445990a0c040475721ab",
            "91f73a7bf1b949449877edc8d2e5df73",
            "928a58f0b45947b3b7a83fde6e5ce78b",
            "ae5f61bfe19e4aea827313e146b89530",
            "0e757fa9adf241cc8dca8670afa51177",
            "047f298d7e2a4f14a45758f0d35ae939",
            "93c8b350c4f34488a117261a245f5840",
            "53251f18161a4e9a96ed9ef5b085af92",
            "76aec0adac90485a927a5da62ec870e1",
            "14180b0d6c9d4f28a5e12a848c3d6eb5",
            "ef8cb0c025484da4b6951af64d686cff",
            "e8075b7533a9460cad6204df34a405c2",
            "a69316bd5e0e4988bdab844b6dc52614",
            "1f049fa19f80401992a8e4fb4a9f6248",
            "868a491aa0fe4bf8bf1638b06366372f",
            "cf4902699c4641b7b0b9f7d6abe3b412",
            "def62441f68c4069a614f522b6c4a8b2",
            "d93a08f4eefc473fb6f10ba29d319cfe",
            "9737525e7e1c4a3fb2d9b576768723f5",
            "0a261507ef4946e087e03cbce9f4e072",
            "08f94640f7ac4fd987fa291954ad11b4",
            "fb82857f88394cb286a9c5108ae239c8",
            "38263be8eac8463c95e4e2a86d69cc77",
            "c1cdc1acb9eb47a09212b43f1c2553c4",
            "b40f0a4825044de9baec0f8cb7249ffa",
            "c2a0c3e5f64d4f99b591aead36bddb43",
            "699f78e055954fb4b68c302781f5a8a8",
            "ab9aec60743143a59e3691fc789be98d",
            "436178f6ef4c4831a9fd62baad55b8a8",
            "40edf9712b73435ebd580faaaf4414d3",
            "bc166829cb52433a9d8a86c605f2d211",
            "4a261b8a5a8149088c1fe6207f9fe6e8",
            "93e023c56f7449419da650e1fad2e63b",
            "ab127e0d20fe40a882bf1c350bc883f5",
            "21019bcc1f97411eb8b3552026407e86",
            "d23089da82a2492c8a250dc0908d9426",
            "4dcec5b5f640410b9ca49ce9e1d6f7b9",
            "135ebb2678554d9c856e53edd1f4ae70",
            "1998ce1254864ec1a30ec1ce59d08efe",
            "30aadad01bee4b6983cafb018493b4e3",
            "b7490954bd3b49528ac5661f2a5bb55f",
            "030be50315c64c278c6fbec5dc72f50b",
            "4323e4f6e2544ccb9fdd39c353722aaa",
            "8d0103bbf02a4cd9b11729fab9e416d3",
            "aa387a4382ab49eaae6316d67cfa2097",
            "3f69887ec863484b84e9ce30bcf65469",
            "444ce7db8a924b37a2c9b99e713794d5",
            "4480163077764adbb372168912be14d6",
            "45bd332a25c949e7a6d09449c06a70b7",
            "08972b49ed8e480d9207f81f8ce28f88",
            "a1515ae8577d4360a58a36f3d7bc7ab8",
            "9b1180f2bcb2492ba939a41c5a5cd147",
            "07f4bc4a37f948a5a59a2f35975c93c4",
            "9d48dc16078f41ec9e006f96d4454a1f",
            "c6842c816eac4fc7b01dbcb5f8739e62",
            "b845bcb44fdb46c2a5ac4911e99839c7",
            "e124837389784ad1bc5f1de8058a690a",
            "4366fbd4094e473281559f6a7a08a952",
            "70798d31ac8548e4b417e7d0bf0cb3bc",
            "bdc9cf1499bd43b093b2143a367869f4",
            "ea7d95842a6e4c2bb9ea31d0f655db17",
            "0a2dcf85382a456084e3aab644316611",
            "89c924b5e1bd47bb885810c7a4a8a802",
            "6f4549804f5d4d5796a5096428667ce1",
            "67cb236f653447d9a5a6d1a01f550e9e",
            "24f98486fe9a4395b0c0a6f9876511b3",
            "dccbba56ca594596aad27d60dfbb074a",
            "35279af9afda48269724a3a30b333eb0",
            "75caef4af1a24a008449bb191219bd40",
            "b9656849226748d485cdbb7c40c195ba",
            "1aa3267d22e8411db74a309de52b84b2",
            "51591b2dd4814a4ba6b2ef8a72de3be9",
            "5e0a1b7d4df847d28ffc2b9a12af67e4",
            "3ee999c81e5c4269a9a49668246f281b",
            "2b876e77420a4cd89e396500d7d7b658",
            "b5785346359b428caababd017a60b768",
            "2b3c5c3877ac480a87f628d301b35023",
            "91572d7352e445ba84ad27d4c5011585",
            "438c960c32c1449bb7101fe61376c4b8",
            "8de8ddaf6f05440faf6a910cc85201e4",
            "88fbed9bcf6e4a7ca10844c67cee4d33",
            "dd85cc3cbdb2435385e23af8b6f10afe",
            "a7d5d8123269429d9a3377174142bbe0",
            "8055f92235ff40eb9f68ede6df353b0f",
            "b29fb8f3875b4a5ba35b939d6bf99640",
            "be62d48b51fb488bbd78cddb70f54d87",
            "1ae6d6b2ebf24f5986c58a15ed171378",
            "c34c450840d24144873c3c933b18da69",
            "486254c3782840b4b07bf627d0d01f3b",
            "3c4b4b2016ef4e829a95254d47805f9d",
            "f3fad93a05de451684c6ff3ba8388eaa",
            "a0728a109cb84e2ebd3d4594c24e94a9",
            "1bad277d760b45b68323268ddd0ab06f",
            "b1c8c13f257b4bf09b43ec19ff76bb1c",
            "0ce3016740854c8a9d0fa7cc6e7b0084",
            "6913e458ca264ce3b0ed3f272286650a",
            "bf353a5493644f4aae9c13f8a11204b5",
            "08ff64616fc34b269c150b8d8d8e182d",
            "36efd76fa6a84f24a35f190de602199e",
            "e6062b8c5525425f8518f9951746efa5",
            "bc2139ff65184c4c9599b15ae6879612",
            "d67abbadda094cf48be761370cd27ccb",
            "7218e4846b8d46339de4fe309853dab2",
            "22519ee37f1545409a79e805886e44f6",
            "1f4a0e3ac9764c8cbe3bd9aa15432467",
            "1de0185719ea413d9fd6197a17279446",
            "7e84d833c1554e1a8d918560dc02bba9",
            "2758a62b0edc419a88dae360486ebc61",
            "1e3ae57335774e249d43baace7e25f86",
            "ee41b3b7eaff4de89125deefc34106c7",
            "36d907dac8914287b03436e7e3ecbd54",
            "5891bd40ecf943b18bbacd25ec6e8a0c",
            "6b2358e499054350bed414a31bf9493e",
            "13fce2cd627b4ff59e195e00259bcdf3",
            "03a3c1af1b1e4f09aaaa601ff895d404",
            "9eb27d5f9a0b4a79a9665b8325701b91",
            "54ac8270b2f84b48882342987241ec5b",
            "d7ceb076284846a1b4e9f13e3a2c2e7b",
            "dfc2c5450efc47a28a74ed10c4734256",
            "01dac0d3f10e4497acc409b48ec49264",
            "6316e53314e94fd7ae72202eece441a7",
            "1041aa1e36f943fcb64cb599007efb59",
            "030f2e221be141b1bb05ee19925c59e5",
            "ad21e9c7ee174f5880b7914dab0d189e",
            "8388baac24394768965c70187301bea1",
            "2eec1bd63862472f8d1d2a8126e66f1c",
            "97c4e1e64d1845b88dbec503d976f963",
            "d9851c390ab540638227d2f2cdc8fd20",
            "e1b5bd42889c46a59f1bb6fbeaf55922",
            "c2b23fe9dbc44b698aed5af78e9f3e3d",
            "39738d7f7dee49ed82b25374d2426dba",
            "f03278fbe404450ea57072fe1da09090",
            "19e32c7ef96d43e28045398be80b94c0",
            "ec684d3472d941cc9a63053c350b4f7a",
            "d082b3270f274547b1b1f7051304c88b",
            "37e75bd477a848168495fcf7e747ddc2",
            "a9aa1d7f797c430cbdf9574f0e93c527",
            "3ac81779c2fc406e90202df31dc73045",
            "16b60567432749f1912c37973c70df9d",
            "ef1188f53b264ee688f3f585eb25728c",
            "1b97dc25418745a3a4d5987509f92364",
            "658ec8508ac346e18c228dc332fdb149",
            "a09082c0cab84314ba63bafd55ff3304",
            "0a9144ad8bc84f5aa9e7a147bf537c7b",
            "8e85c75d211f42e79ebaa5454b269333",
            "bcabc7e9587040c2a0a5ce1d8ffa8997",
            "6b7de5ed62dd4583bc6f3d5e2f5aede2",
            "4980488a6f1648a7b4cfbb2285fd9cf3",
            "d49cbbeea41645ce945ef2ffcd819ec0",
            "63a6e7c108dc4a269db65bc1e9c75c9c",
            "ec2b4d34a10f4bb6baedbb14891d0471",
            "70af0ba5038445ffa520bdd85a287de2",
            "ba946a4f820243099d21d8d392891885",
            "b7db56dc657344c48f0694aaa9a5e94d",
            "d4ca0548fac348ad8239552ad83e994f",
            "03f2b9be79184f3083afd1aba5690830",
            "5f6b54ac86ec47c9b7e4ad1cfb91a156",
            "1b2fde8d6f9d4cc68ee1ac19bf328b9e",
            "89b3b50426064ebbb6d6d35be60a5c39",
            "98b1420e842243038f54de7919b279d1",
            "e717dc1810e54cb7b13e0729fae04706",
            "50987549dd8940afad6e5ab7f985fd30",
            "7e538bb937364ab4846baab069a7097b",
            "6db4b4a6e814444a806411c99b5c5140",
            "0c0d60ea0ab341fcaedf68c96984e149",
            "d3c34f4c8dbe4abe8f7b56b1be0c7853",
            "ee7e5171d5874c30b0b501b34a9e4541",
            "6eddafa6f448489194f453eb5be092be",
            "4748857cd140414ea3d288e4a3523f3c",
            "e7da41a7a76940d4ba4079e8efc60a0a",
            "2c9c38efcd0f49278fac79b76e94b159",
            "798b946bc4ab4c02aaddb4d70b04f6a1",
            "281458d7a03144599effde0e7fe3955e",
            "d1848a5420094dcb83e005f2a3d955f2",
            "1450b9c4a3fb49df97e6ef3a6dee3849",
            "0db77044ba7d474ba920ca7ba5c605b1",
            "bf86921598c747d5beb08c0156c86df6",
            "712004d5949c4e50a24da85d04eafd3f",
            "5d6269131a2d4b5e8b41f72850cfbe08",
            "14e0127674ec4a7290c8d9d67a70a106",
            "d94954153ff94aca8b507089f13e3c4a",
            "6233568a9b7b41f48384715f2ef739fd",
            "96996052d02b4cdc9778245d81ee3920",
            "4df903a15347450b92bf1ec9ac8b82e8",
            "642f54b456804742b643e43af24f1e7b",
            "49579e0aeb7047aba25f0611f61bd4d2",
            "cef6b30205c54b0ebc5bc32f1d428533",
            "64dc3c1f649848cd993973e5b94c43d8",
            "9994cccaff3f474ebfc009df07c91f7c",
            "9b5cb4d0728e4aefb2ec91539a802490",
            "31f5b52492c441f3a698248cff3f03df",
            "75cc0a1043dc4625a055f48a49a7e796",
            "3dea91a321b94e359aa4981526e05b95",
            "a0df9915ced0402b86f8eb8e663d6126",
            "abc9c2f88e0d4da29dc3c453e3b5cc58",
            "7b82ca7661a94449aff22e210dcb344d",
            "77503523441742288dd3dfc355ae53d6",
            "3d8b14c902d54ddfbcca411dfe5d31ce",
            "900caef263af4bfa9b45243dfc37f62b",
            "3ae0877691ec4b78b799e05f2e3b6d01",
            "a968e8c660a1417a86691ad21a685547",
            "54cde5bad11a45e087ae24f91f9d8afc",
            "4f04381ad6064c7f9f07d251a5e466c0",
            "7afe1060ada243469d74277039c8de9e",
            "80bdd15a69e047919f44b33080f9d411",
            "149bf9c067ec4a25bdaa8c9519de4ca7",
            "f8f22ffb741f481e88d0aa54dd67abe2",
            "d2c7ac203da444d8a2e3f8864f73f2f2",
            "02df7ad8d5dd49b5a17fa590b75208cf",
            "2803f817ab3c447abc8f9e9157ca62f7",
            "9309ad1af63446e49dee6b87463429e7",
            "91549b8e822f4694a242162d17c423ef",
            "6d386ce59ba8441b9df2b4c9e858bedb",
            "20558351218b4e63bc99dc197e25b39d",
            "b1f33aa16b5145a1bca64dee90c5d2d2",
            "cde5398c52dd4def8f1d8f4d6202546b"
          ]
        },
        "id": "qOuOzlZV3BsM",
        "outputId": "18ed55a1-11ba-4be3-895f-3155b1aa9ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 14:01:59] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Starte Framework: hf | b560 (bigscience/bloom-560m)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd01af7c1cd54fd1ac449298b3b2be59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c9a96391db34e6eb90efe6a678f835f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c9a9afb929b484685a7584c05570091"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "124354e6fc594168aa96cee5822ebdd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b51801d71d534411a9bc27ae971b71c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45aa2529d1f84cf7b97de726e394dd7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7340e2ce26804c6fb5a56422dbc8bcb4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92cbafe5d7cf45daa6bdff4f958e1d57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecbd01861fa44af1a277052cd02d6055"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "184febe372134fdfa509f656ca7b84be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "488226225ce04225b44e2337d8fb0e3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "357eb7ce020d4278aea35b0851946c8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ee28a8d82b946d7a0d66f575b7157a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00000-of-00003.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c84d397b6750404495175db180957929"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00001-of-00003.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2352129532aa4a6fa2c6c43a877fa9db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00002-of-00003.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cca673eae6a464da3506a0f487e7eda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/validation-00000-of-00001.parquet:   0%|          | 0.00/474k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1004f7a67d63445cb16b922257464380"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/test-00000-of-00001.parquet:   0%|          | 0.00/509k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4a580e757bb4715aeeadcbbf0ef5959"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e04bde7e1294d7985be8cb958475706"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cedea7c0cf14472cb911c7ea841d7374"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c456fdaa2bb148da8773dc8cc2cdca20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beispiele gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_frameworks/bloom_frameworks_samples_b560_hf.txt\n",
            "\n",
            "### Starte Framework: ort | b560 (bigscience/bloom-560m)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model bigscience/bloom-560m was already converted to ONNX but got `export=True`, the model will be converted to ONNX once again. Don't forget to save the resulting model with `.save_pretrained()`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e8e21a3db214cf396a0ff97466e2d5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efc6a0ce188e45a683471d6b3286f17c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "715bca35a5a74cb8b29d79fabacc609e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47639169bf774b32ba25386e1e95d868"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53251f18161a4e9a96ed9ef5b085af92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9737525e7e1c4a3fb2d9b576768723f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
            "\tlm_head.weight: {'onnx::MatMul_5381'}\n",
            "\ttransformer.word_embeddings.weight: {'transformer.word_embeddings.weight'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40edf9712b73435ebd580faaaf4414d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7490954bd3b49528ac5661f2a5bb55f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b1180f2bcb2492ba939a41c5a5cd147"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beispiele gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_frameworks/bloom_frameworks_samples_b560_ort.txt\n",
            "\n",
            "### Starte Framework: hf | b3b (bigscience/bloom-3b)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89c924b5e1bd47bb885810c7a4a8a802"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ee999c81e5c4269a9a49668246f281b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b29fb8f3875b4a5ba35b939d6bf99640"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6913e458ca264ce3b0ed3f272286650a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.01G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e84d833c1554e1a8d918560dc02bba9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beispiele gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_frameworks/bloom_frameworks_samples_b3b_hf.txt\n",
            "\n",
            "### Starte Framework: ort | b3b (bigscience/bloom-3b)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7ceb076284846a1b4e9f13e3a2c2e7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1b5bd42889c46a59f1bb6fbeaf55922"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef1188f53b264ee688f3f585eb25728c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec2b4d34a10f4bb6baedbb14891d0471"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50987549dd8940afad6e5ab7f985fd30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "281458d7a03144599effde0e7fe3955e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
            "\tlm_head.weight: {'onnx::MatMul_6659'}\n",
            "\ttransformer.word_embeddings.weight: {'transformer.word_embeddings.weight'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4df903a15347450b92bf1ec9ac8b82e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abc9c2f88e0d4da29dc3c453e3b5cc58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "149bf9c067ec4a25bdaa8c9519de4ca7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beispiele gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_frameworks/bloom_frameworks_samples_b3b_ort.txt\n",
            "\n",
            "Ergebnisse (gesamt):\n",
            "                model_id alias             backend precision      time_s  \\\n",
            "0    bigscience/bloom-3b   b3b            hf_eager      fp16   43.787972   \n",
            "1    bigscience/bloom-3b   b3b  ort_cuda(exported)      fp16  369.111210   \n",
            "2  bigscience/bloom-560m  b560            hf_eager      fp16   37.042503   \n",
            "3  bigscience/bloom-560m  b560  ort_cuda(exported)      fp16  143.148229   \n",
            "\n",
            "   energy_kwh    co2_kg  kg_per_kwh  tokens_out         ppl       bleu  \\\n",
            "0    0.001386  0.000371    0.267622          96  238.532132  10.054131   \n",
            "1    0.012881  0.003447    0.267622          96  238.424206  10.092805   \n",
            "2    0.001087  0.000291    0.267622          96  341.261974   5.069542   \n",
            "3    0.004801  0.001285    0.267622          96  339.798549   3.784713   \n",
            "\n",
            "      ram_GB  vram_alloc_GB  vram_reserved_GB  \\\n",
            "0   5.153816       5.601610          6.798828   \n",
            "1  17.829636       0.007935          0.019531   \n",
            "2   2.840424       1.049553          1.062500   \n",
            "3   5.539799       0.007935          0.019531   \n",
            "\n",
            "                                   notes  \n",
            "0  MAX_LEN_CAP=128; pinned=True; nb=True  \n",
            "1  MAX_LEN_CAP=128; pinned=True; nb=True  \n",
            "2  MAX_LEN_CAP=128; pinned=True; nb=True  \n",
            "3  MAX_LEN_CAP=128; pinned=True; nb=True  \n",
            "Gespeichert (gesamt): /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_frameworks/bloom_frameworks_results.csv\n",
            "\n",
            "Per-Phase Übersicht:\n",
            "   alias             backend   phase      time_s  energy_kwh    co2_kg\n",
            "8    b3b            hf_eager  warmup   35.502467    0.001061  0.000284\n",
            "9    b3b            hf_eager     gen    0.879857    0.000034  0.000009\n",
            "10   b3b            hf_eager     ppl    2.264560    0.000072  0.000019\n",
            "11   b3b            hf_eager    bleu    5.141089    0.000219  0.000059\n",
            "12   b3b  ort_cuda(exported)  warmup  314.162379    0.010249  0.002743\n",
            "13   b3b  ort_cuda(exported)     gen    2.124517    0.000110  0.000029\n",
            "14   b3b  ort_cuda(exported)     ppl    5.763575    0.000221  0.000059\n",
            "15   b3b  ort_cuda(exported)    bleu   47.060739    0.002301  0.000616\n",
            "0   b560            hf_eager  warmup    8.042801    0.000224  0.000060\n",
            "1   b560            hf_eager     gen    0.899812    0.000028  0.000007\n",
            "2   b560            hf_eager     ppl    5.732182    0.000169  0.000045\n",
            "3   b560            hf_eager    bleu   22.367708    0.000666  0.000178\n",
            "4   b560  ort_cuda(exported)  warmup   92.969258    0.002939  0.000787\n",
            "5   b560  ort_cuda(exported)     gen    1.812793    0.000069  0.000019\n",
            "6   b560  ort_cuda(exported)     ppl    5.674252    0.000194  0.000052\n",
            "7   b560  ort_cuda(exported)    bleu   42.691927    0.001599  0.000428\n",
            "Gespeichert (per Phase): /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_frameworks/bloom_frameworks_per_phase.csv\n"
          ]
        }
      ]
    }
  ]
}