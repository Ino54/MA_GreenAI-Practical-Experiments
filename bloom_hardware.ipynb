{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ino54/MA_GreenAI-Practical-Experiments/blob/main/bloom_hardware.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHcTVn5x8jGo",
        "outputId": "7eb39d5a-2c6d-4d27-af58-56ea248195e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "# ---------- Requirements ----------\n",
        "%%writefile requirements.txt\n",
        "transformers\n",
        "accelerate\n",
        "datasets\n",
        "evaluate\n",
        "sacrebleu\n",
        "codecarbon>=2.5,<3\n",
        "pynvml>=11.5.0\n",
        "psutil\n",
        "numpy\n",
        "pandas\n",
        "huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -r requirements.txt\n",
        "# potenzielle Konflikte leise entfernen\n",
        "!pip uninstall -y -q google-genai firebase-admin || true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za9rgEWL8-5s",
        "outputId": "d08a1045-f790-42da-d532-bff884616e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.6/517.6 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-genai 1.38.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Hugging Face Login via Colab-Secret ----------\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(hf_token); print(\"Hugging Face Login erfolgreich!\")\n",
        "else:\n",
        "    print(\"WARNUNG: Kein HF_TOKEN gefunden\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsliFF338-2f",
        "outputId": "be8e1425-488c-49fb-dd83-c15615df6875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face Login erfolgreich!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Drive mounten & Zielordner prüfen ----------\n",
        "import os, shutil, time, pathlib, platform, gc, re, math, warnings, inspect\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from google.colab import drive\n",
        "MOUNTPOINT = \"/content/drive\"\n",
        "already = os.path.isdir(os.path.join(MOUNTPOINT, \"MyDrive\"))\n",
        "if not already and os.path.isdir(MOUNTPOINT) and os.listdir(MOUNTPOINT):\n",
        "    backup = f\"/content/drive_stale_{int(time.time())}\"\n",
        "    shutil.move(MOUNTPOINT, backup)\n",
        "    os.makedirs(MOUNTPOINT, exist_ok=True)\n",
        "drive.mount(MOUNTPOINT, force_remount=(not already))\n",
        "\n",
        "work_dir = \"/content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_hardware\"\n",
        "if not os.path.isdir(work_dir):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Zielordner fehlt: {work_dir}\\n\"\n",
        "        \"Bitte diesen Ordner manuell in Google Drive anlegen und das Notebook erneut starten.\"\n",
        "    )\n",
        "os.chdir(work_dir)\n",
        "project_dir = work_dir\n",
        "print(\"Arbeitsordner:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk6zV_338-0H",
        "outputId": "7caa0d5e-8bda-433c-9cfd-a8386999a26b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Arbeitsordner: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_hardware\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Imports & Device ----------\n",
        "import numpy as np, pandas as pd\n",
        "import torch, psutil\n",
        "from contextlib import nullcontext\n",
        "from types import SimpleNamespace\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "# PyTorch Runtimes (Performance)\n",
        "import os as _os\n",
        "_os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # weniger Fragmentierung\n",
        "\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_total_gb = torch.cuda.get_device_properties(0).total_memory/(1024**3)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True  # T4 ignoriert TF32 (ok)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "else:\n",
        "    gpu_name = \"CPU\"; vram_total_gb = 0.0\n",
        "print(f\"Device: {device} | GPU: {gpu_name} | VRAM={vram_total_gb:.1f} GB | Torch {torch.__version__} | Py {platform.python_version()}\")\n",
        "\n",
        "RESULT_BASENAME = \"bloom_hardware\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMuIAGTy8-wW",
        "outputId": "0ea1f2eb-6890-42ff-d8ba-f15b5c3d8452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | GPU: NVIDIA A100-SXM4-80GB | VRAM=79.3 GB | Torch 2.8.0+cu126 | Py 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Flags für Hardware-Optimierung ----------\n",
        "APPLY_TORCH_COMPILE     = False    # für kurze Läufe meist AUS; bei langen Läufen True setzen\n",
        "PINNED_MEM              = True     # Host-Puffer pinnen\n",
        "NON_BLOCKING            = True     # non_blocking Transfers zur GPU"
      ],
      "metadata": {
        "id": "Iqeuk9Kc8-tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- CodeCarbon-Helfer ----------\n",
        "def _cleanup_cc_locks():\n",
        "    for p in [\n",
        "        \"/tmp/.codecarbon.lock\",\n",
        "        _os.path.expanduser(\"~/.codecarbon/codecarbon.lock\"),\n",
        "        \"/content/.codecarbon/codecarbon.lock\",\n",
        "    ]:\n",
        "        try:\n",
        "            if _os.path.exists(p):\n",
        "                _os.remove(p)\n",
        "                print(f\"[CodeCarbon] Lock entfernt: {p}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "_os.environ[\"CODECARBON_CACHE_DIR\"] = f\"/content/.cc_cache_hw_{int(time.time())}\"\n",
        "\n",
        "def _cc_supported_kwargs():\n",
        "    base = dict(log_level=\"error\", output_dir=\".\", measure_power_secs=1, tracking_mode=\"process\")\n",
        "    try:\n",
        "        params = inspect.signature(EmissionsTracker.__init__).parameters\n",
        "        if \"allow_multiple_runs\" in params: base[\"allow_multiple_runs\"] = True\n",
        "        if \"cloud_provider\" in params:      base[\"cloud_provider\"] = \"google\"\n",
        "        if \"cloud_region\" in params:        base[\"cloud_region\"]   = \"europe-west10\"\n",
        "        if \"country_iso_code\" in params:    base[\"country_iso_code\"] = \"DEU\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    return base\n",
        "\n",
        "def make_trk(name, out):\n",
        "    _cleanup_cc_locks()\n",
        "    return EmissionsTracker(project_name=name, output_file=out, **_cc_supported_kwargs())\n",
        "\n",
        "def start(tr):\n",
        "    try:\n",
        "        tr.start(); return True\n",
        "    except Exception:\n",
        "        _cleanup_cc_locks()\n",
        "        try:\n",
        "            tr.start(); return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "def stop(tr, st):\n",
        "    if not st:\n",
        "        return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "    try:\n",
        "        return tr.stop()\n",
        "    except Exception:\n",
        "        return SimpleNamespace(energy_consumed=0.0, emissions=0.0)\n",
        "\n",
        "def unpack(em):\n",
        "    if hasattr(em, \"energy_consumed\") and hasattr(em, \"emissions\"):\n",
        "        try: return float(em.energy_consumed), float(em.emissions)\n",
        "        except: return 0.0, 0.0\n",
        "    if isinstance(em, dict):\n",
        "        e = em.get(\"energy_consumed\", 0.0)\n",
        "        c = em.get(\"emissions\", em.get(\"emissions_kg\", 0.0))\n",
        "        try: return float(e), float(c)\n",
        "        except: return 0.0, 0.0\n",
        "    try: return 0.0, float(em)\n",
        "    except: return 0.0, 0.0\n",
        "\n",
        "def read_energy(path):\n",
        "    try:\n",
        "        if not _os.path.exists(path): return 0.0\n",
        "        df = pd.read_csv(path)\n",
        "        for c in [\"energy_consumed\",\"energy_consumed_kwh\",\"energy_consumed (kWh)\",\"energy (kWh)\"]:\n",
        "            if c in df.columns: return float(df[c].iloc[-1])\n",
        "        for c in df.columns:\n",
        "            if \"energy\" in c.lower() and \"kwh\" in c.lower():\n",
        "                return float(df[c].iloc[-1])\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def measure(phase, fn, prefix):\n",
        "    logfile = _os.path.join(project_dir, f\"{prefix}_{phase}.csv\")\n",
        "    tr = make_trk(f\"{prefix}_{phase}\", logfile)\n",
        "    import time as _t\n",
        "    st = start(tr); t0 = _t.time(); res = fn(); t1 = _t.time()\n",
        "    em = stop(tr, st); ekwh, co2 = unpack(em)\n",
        "    if ekwh == 0.0:\n",
        "        ek = read_energy(logfile)\n",
        "        if ek: ekwh = ek\n",
        "    return {\"phase\": phase, \"time_s\": t1-t0, \"energy_kwh\": ekwh, \"co2_kg\": co2}, res"
      ],
      "metadata": {
        "id": "QrATRj578-qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Eval-Config ----------\n",
        "MODELS = [\n",
        "    (\"bigscience/bloom-560m\", \"bloom560m\", \"b560\"),\n",
        "    (\"bigscience/bloom-3b\",   \"bloom3b\",   \"b3b\"),\n",
        "]\n",
        "EVAL = {\n",
        "    \"max_new_tokens\": 32,\n",
        "    \"ppl\":  {\"name\":\"wikitext\",\"config\":\"wikitext-2-raw-v1\",\"split\":\"test[:1%]\"},\n",
        "    \"bleu\": {\"name\":\"wmt14\",\"config\":\"de-en\",\"split\":\"test[:32]\"},\n",
        "}\n",
        "PROMPTS = [\n",
        "    \"Schreibe einen kurzen Absatz über nachhaltige KI.\",\n",
        "    \"Erkläre in einfachen Worten, was Quantisierung in neuronalen Netzen ist.\",\n",
        "    \"Nenne drei Vorteile von Mixture-of-Experts-Modellen.\"\n",
        "]\n",
        "bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# Eval-Schutz\n",
        "MAX_LEN_CAP = 256  # ggf. 192/128 für noch weniger VRAM\n",
        "\n",
        "def autocast_ctx():\n",
        "    return torch.autocast(device_type=\"cuda\", dtype=torch.float16) if torch.cuda.is_available() else nullcontext()\n",
        "\n",
        "def safe_max_len(tok, model, fallback=2048, upper=100000):\n",
        "    cand = getattr(tok, \"model_max_length\", None)\n",
        "    if isinstance(cand, int) and 0 < cand < upper: return cand\n",
        "    cand = getattr(getattr(model, \"config\", None), \"max_position_embeddings\", None)\n",
        "    if isinstance(cand, int) and 0 < cand < upper: return cand\n",
        "    return fallback\n",
        "\n",
        "def capture_memory():\n",
        "    ram = psutil.Process().memory_info().rss\n",
        "    valloc = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "    vres  = torch.cuda.memory_reserved()  if torch.cuda.is_available() else 0\n",
        "    return ram, valloc, vres\n",
        "\n",
        "def bytes_to_gb(b): return float(b)/(1024**3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d99ec1c5bb4c4aa5b9b162bb7d8bcba0",
            "74fd683b5d1149d08086e3e584a9d6f5",
            "aeaf325a26e642c78cce291cba203f05",
            "17188ba98f484f5c81c45640973d69a9",
            "ceccc21da2094f328129b19786823589",
            "51701ea779644940ac23ffd6a97e57c3",
            "47ea062f754c456c9908500e26829c19",
            "1e672e46b68846fab48c46efbd73146a",
            "fb46f11c3cc0467bbef02a9e61b228cb",
            "9f17d09c84ce48d5ab3fe86ea1b6ff6f",
            "144dcad469af47e394342693b1c20fa7"
          ]
        },
        "id": "HMtW6c1y9bya",
        "outputId": "54b6fcea-dbc1-4e69-e210-88439990fbb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d99ec1c5bb4c4aa5b9b162bb7d8bcba0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Laden + Hardware-Tweaks (BLOOM benötigt eager attention) ----------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "def load_model_fp16_optimized(model_id: str):\n",
        "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    tok.padding_side = \"left\"; tok.pad_token = tok.eos_token\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    # WICHTIG: BLOOM → attn_implementation=\"eager\" (SDPA nicht unterstützt)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        low_cpu_mem_usage=True,\n",
        "        attn_implementation=\"eager\",\n",
        "    )\n",
        "    try:\n",
        "        model.config.use_cache = True\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if APPLY_TORCH_COMPILE and hasattr(torch, \"compile\"):\n",
        "        try:\n",
        "            model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=False)\n",
        "            print(\"[HW] torch.compile aktiv (reduce-overhead).\")\n",
        "        except Exception as e:\n",
        "            print(\"[HW] torch.compile nicht möglich → normal weiter:\", repr(e))\n",
        "\n",
        "    model.eval()\n",
        "    return tok, model"
      ],
      "metadata": {
        "id": "ZVkEZdlL9buj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Utility: Pinned Memory ----------\n",
        "def _pin_and_move(batch, device):\n",
        "    \"\"\"\n",
        "    Erwartet ein Dict mit Tensors; pint Host-Speicher und verschiebt non_blocking.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for k, v in batch.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            if PINNED_MEM and v.device.type == \"cpu\":\n",
        "                try:\n",
        "                    v = v.pin_memory()\n",
        "                except Exception:\n",
        "                    pass\n",
        "            out[k] = v.to(device, non_blocking=NON_BLOCKING)\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def get_model_device(model):\n",
        "    try:\n",
        "        return next(model.parameters()).device\n",
        "    except StopIteration:\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "KLSyQ3Uq9brg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Evaluation (VARIANTE 2: exakte Tokenzählung pro Beispiel) ----------\n",
        "def simple_generate(model, tok, prompts, max_new_tokens=32):\n",
        "    dev = get_model_device(model)\n",
        "    ml = min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    # Batch alle Prompts zusammen -> höherer Durchsatz\n",
        "    enc = tok(prompts, return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "    enc = _pin_and_move(enc, dev)\n",
        "    room = ml - enc[\"input_ids\"].shape[1]\n",
        "    cur_new = max(1, min(max_new_tokens, int(room)))\n",
        "    with torch.inference_mode(), autocast_ctx():\n",
        "        out = model.generate(**enc, max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "\n",
        "    texts, total_gen_tokens = [], 0\n",
        "    max_input_len = int(enc[\"input_ids\"].shape[1])\n",
        "    pad_id = tok.pad_token_id\n",
        "    eos_id = tok.eos_token_id\n",
        "\n",
        "    # genaue Zählung: tokens in der generierten Region [max_input_len: ] pro Beispiel,\n",
        "    # EOS wird als generierter Token gezählt (auch wenn pad_id == eos_id)\n",
        "    for i in range(out.size(0)):\n",
        "        seq = out[i]\n",
        "        gen_slice = seq[max_input_len:]  # nur generierter Teil (gepadde Länge)\n",
        "        gen_i = 0\n",
        "        for t in gen_slice.tolist():\n",
        "            if t == eos_id:\n",
        "                gen_i += 1  # EOS mitzählen\n",
        "                break\n",
        "            if (pad_id is not None) and (t == pad_id):\n",
        "                break\n",
        "            gen_i += 1\n",
        "        total_gen_tokens += gen_i\n",
        "        texts.append(tok.decode(seq, skip_special_tokens=True))\n",
        "\n",
        "    return texts, total_gen_tokens\n",
        "\n",
        "def eval_perplexity(model, tok, ds_cfg):\n",
        "    dev = get_model_device(model)\n",
        "    ds = load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    ml = min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    losses = []\n",
        "    with torch.inference_mode(), autocast_ctx():\n",
        "        for t in ds[\"text\"]:\n",
        "            if not isinstance(t, str) or len(t.strip()) < 4: continue\n",
        "            enc = tok(t, return_tensors=\"pt\", truncation=True, max_length=ml)\n",
        "            enc = _pin_and_move(enc, dev)\n",
        "            out = model(enc[\"input_ids\"], labels=enc[\"input_ids\"])\n",
        "            losses.append(float(out.loss.detach().cpu()))\n",
        "    return math.exp(np.mean(losses)) if losses else None\n",
        "\n",
        "def eval_bleu_llm(model, tok, ds_cfg, max_new_tokens=32, batch_size=8):\n",
        "    dev = get_model_device(model)\n",
        "    ds = load_dataset(ds_cfg[\"name\"], ds_cfg[\"config\"], split=ds_cfg[\"split\"])\n",
        "    ml = min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "    preds, refs = [], []\n",
        "    with torch.inference_mode(), autocast_ctx():\n",
        "        batch_prompts, batch_refs = [], []\n",
        "        for ex in ds:\n",
        "            de, en = ex[\"translation\"][\"de\"], ex[\"translation\"][\"en\"]\n",
        "            prompt = f\"Translate to English:\\nGerman: {de}\\nEnglish:\"\n",
        "            batch_prompts.append(prompt); batch_refs.append(en)\n",
        "            if len(batch_prompts) >= batch_size:\n",
        "                enc = tok(batch_prompts, return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "                enc = _pin_and_move(enc, dev)\n",
        "                room = ml - enc[\"input_ids\"].shape[1]\n",
        "                cur_new = max(1, min(max_new_tokens, int(room)))\n",
        "                out = model.generate(**enc, max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "                for i in range(out.size(0)):\n",
        "                    gen = tok.decode(out[i], skip_special_tokens=True)\n",
        "                    hyp = gen.split(\"English:\")[-1].strip().split(\"\\n\")[0].strip() or gen.strip()\n",
        "                    preds.append(hyp); refs.append([batch_refs[i]])\n",
        "                batch_prompts, batch_refs = [], []\n",
        "        if batch_prompts:\n",
        "            enc = tok(batch_prompts, return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "            enc = _pin_and_move(enc, dev)\n",
        "            room = ml - enc[\"input_ids\"].shape[1]\n",
        "            cur_new = max(1, min(max_new_tokens, int(room)))\n",
        "            out = model.generate(**enc, max_new_tokens=cur_new, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "            for i in range(out.size(0)):\n",
        "                gen = tok.decode(out[i], skip_special_tokens=True)\n",
        "                hyp = gen.split(\"English:\")[-1].strip().split(\"\\n\")[0].strip() or gen.strip()\n",
        "                preds.append(hyp); refs.append([batch_refs[i]])\n",
        "    return float(bleu_metric.compute(predictions=preds, references=refs)[\"score\"])"
      ],
      "metadata": {
        "id": "j7eY4h1q9qRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Run pro Modell ----------\n",
        "def run_once(model_id: str, alias: str):\n",
        "    print(f\"\\n### Starte Hardware-Optimierung: {alias} ({model_id})\")\n",
        "\n",
        "    # Warmup + Laden mit HW-Tweaks\n",
        "    def _do_load():\n",
        "        tok, model = load_model_fp16_optimized(model_id)\n",
        "        # kurze Warmup-Token (batch=2), damit Kernels greifen\n",
        "        dev = get_model_device(model); ml = min(MAX_LEN_CAP, safe_max_len(tok, model))\n",
        "        wenc = tok([\"Warmup token 1\", \"Warmup token 2\"], return_tensors=\"pt\", truncation=True, max_length=ml, padding=True)\n",
        "        wenc = _pin_and_move(wenc, dev)\n",
        "        with torch.inference_mode(), autocast_ctx():\n",
        "            _ = model.generate(**wenc, max_new_tokens=1, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "        return tok, model\n",
        "    m_warm, (tok, model) = measure(\"warmup\", _do_load, f\"{RESULT_BASENAME}_{alias}\")\n",
        "\n",
        "    # GEN (batched)\n",
        "    m_gen, (samples, n_tok) = measure(\"gen\",\n",
        "        lambda: simple_generate(model, tok, PROMPTS, EVAL[\"max_new_tokens\"]),\n",
        "        f\"{RESULT_BASENAME}_{alias}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    # PPL\n",
        "    m_ppl, ppl = measure(\"ppl\",\n",
        "        lambda: eval_perplexity(model, tok, EVAL[\"ppl\"]),\n",
        "        f\"{RESULT_BASENAME}_{alias}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    # BLEU (mini-batched)\n",
        "    m_bleu, bleu = measure(\"bleu\",\n",
        "        lambda: eval_bleu_llm(model, tok, EVAL[\"bleu\"], EVAL[\"max_new_tokens\"], batch_size=8),\n",
        "        f\"{RESULT_BASENAME}_{alias}\"\n",
        "    )\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    # Gesamt und \"steady-state\" (ohne Warmup)\n",
        "    total_time   = m_warm[\"time_s\"] + m_gen[\"time_s\"] + m_ppl[\"time_s\"] + m_bleu[\"time_s\"]\n",
        "    total_energy = m_warm[\"energy_kwh\"] + m_gen[\"energy_kwh\"] + m_ppl[\"energy_kwh\"] + m_bleu[\"energy_kwh\"]\n",
        "    total_co2    = m_warm[\"co2_kg\"]     + m_gen[\"co2_kg\"]     + m_ppl[\"co2_kg\"]     + m_bleu[\"co2_kg\"]\n",
        "\n",
        "    steady_time   = m_gen[\"time_s\"] + m_ppl[\"time_s\"] + m_bleu[\"time_s\"]\n",
        "    steady_energy = m_gen[\"energy_kwh\"] + m_ppl[\"energy_kwh\"] + m_bleu[\"energy_kwh\"]\n",
        "    steady_co2    = m_gen[\"co2_kg\"]     + m_ppl[\"co2_kg\"]     + m_bleu[\"co2_kg\"]\n",
        "\n",
        "    ram, valloc, vres = capture_memory()\n",
        "\n",
        "    per_phase = [m_warm, m_gen, m_ppl, m_bleu]\n",
        "    for p in per_phase:\n",
        "        p[\"alias\"] = alias\n",
        "        p[\"model_id\"] = model_id\n",
        "\n",
        "    row = dict(\n",
        "        model_id=model_id, alias=alias, precision=f\"fp16 (eager, compile={APPLY_TORCH_COMPILE})\",\n",
        "        time_s=total_time, energy_kwh=total_energy, co2_kg=total_co2,\n",
        "        steady_time_s=steady_time, steady_energy_kwh=steady_energy, steady_co2_kg=steady_co2,\n",
        "        kg_per_kwh=(total_co2/total_energy) if total_energy else None,\n",
        "        tokens_out=int(n_tok), ppl=ppl, bleu=bleu,\n",
        "        ram_GB=bytes_to_gb(ram), vram_alloc_GB=bytes_to_gb(valloc), vram_reserved_GB=bytes_to_gb(vres),\n",
        "        notes=f\"attn=eager; compile={APPLY_TORCH_COMPILE}; pinned={PINNED_MEM}; nb={NON_BLOCKING}; MAX_LEN_CAP={MAX_LEN_CAP}\"\n",
        "    )\n",
        "\n",
        "    # Samples speichern\n",
        "    samples_path = os.path.join(project_dir, f\"{RESULT_BASENAME}_samples_{alias}.txt\")\n",
        "    with open(samples_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, txt in enumerate(samples, 1):\n",
        "            f.write(f\"--- Beispiel {i} ({alias}) ---\\n{txt}\\n\\n\")\n",
        "    print(\"Beispiele gespeichert:\", samples_path)\n",
        "\n",
        "    return row, per_phase"
      ],
      "metadata": {
        "id": "rEnoiDpG9sz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Ausführen & Speichern ----------\n",
        "all_rows, all_phases = [], []\n",
        "for mid, long_name, alias in MODELS:\n",
        "    row, phases = run_once(mid, alias)\n",
        "    all_rows.append(row)\n",
        "    all_phases.extend(phases)\n",
        "\n",
        "df  = pd.DataFrame(all_rows).sort_values(\"alias\").reset_index(drop=True)\n",
        "dfp = pd.DataFrame(all_phases)\n",
        "dfp[\"wh_total\"] = dfp[\"energy_kwh\"] * 1000.0\n",
        "dfp[\"phase\"] = pd.Categorical(dfp[\"phase\"], categories=[\"warmup\",\"gen\",\"ppl\",\"bleu\"], ordered=True)\n",
        "\n",
        "out_csv = os.path.join(project_dir, f\"{RESULT_BASENAME}_results.csv\")\n",
        "df.to_csv(out_csv, index=False)\n",
        "\n",
        "out_phase_csv = os.path.join(project_dir, f\"{RESULT_BASENAME}_per_phase.csv\")\n",
        "dfp.to_csv(out_phase_csv, index=False)\n",
        "\n",
        "print(\"\\nErgebnisse (gesamt):\")\n",
        "print(df)\n",
        "print(\"Gespeichert (gesamt):\", out_csv)\n",
        "\n",
        "print(\"\\nPer-Phase Übersicht:\")\n",
        "print(dfp[[\"alias\",\"phase\",\"time_s\",\"energy_kwh\",\"co2_kg\"]].sort_values([\"alias\",\"phase\"]))\n",
        "print(\"Gespeichert (per Phase):\", out_phase_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5a0508752a7a4618836b52926a8a3270",
            "31a80ba30a44462099778e18261f3d43",
            "61e616abf01240f8888ac4a471d0c9f0",
            "365a9951bd734eea97e016ed74b6ffe4",
            "01a8593e115347ceabe1657c01a7c629",
            "193d9e7466d0455f87e0b0385083bdab",
            "dabf5ab12aa5480796f3cbdcbb4509c1",
            "29acad076d9e49dfb928baa6266a1897",
            "734c759f271e429993cb1a629aeabcba",
            "27bbcfba0bc34ff9b3749a1d2b6e9559",
            "11cefe807190406ba4fc8b289fdd8326",
            "d6d57e9ad0bc45a091b2d4dff49b84c0",
            "b6776e55ec1645c1a94f3f2696345459",
            "7a014fd919bd4fdc914e0c80f99572e3",
            "bad7d434b2794bf3961d0555267746c3",
            "83455650ee2b462d92be438fb4c310eb",
            "8397704b77214d9d95d14c16c53f49f1",
            "0a11aa598fc342feac1388bf715f8b98",
            "a69dabdcf0b44e2ca44a134ff712341b",
            "bcb3786dddf043f989e4b258199d6d47",
            "2b0abffe6a5c4085bc2393ba1bf3621c",
            "5ffe5dbca2f747ed964c4a4d730fd767",
            "dd5dd607757e4e5e9b67b8454affb519",
            "4851ca6f362d4bf9abc07d609ebda968",
            "70874f0d461745969e0a5fe9656c310b",
            "f3a866b4416545e7b8f0eaab910b4129",
            "cec8ad7faf6d4fefb3ebc393d972f7a3",
            "16f11e4138154e06961c205810ab5764",
            "85449e948e8f46ee9e427d1157adc547",
            "91c82d25bb2247e6b62ea5971667efc6",
            "6561392a95234cc4916f72292e544fda",
            "608aeb473cc14fb693941bcfc6cea37f",
            "67c376ff229049e689eef6e2e92aaa18",
            "75f702d6af9440bf88c3aa56d4bbede1",
            "9ef637f976104fa6980da827e5754faf",
            "a80bd1b4fa2e49aeaf8d1ac5dbb48e07",
            "feeae11c8b484ec2b30e11cf961f0adc",
            "a238471e597042c79df3ad39b537f161",
            "ab9c79b172714bc3a623fe10917f697e",
            "896d20331d1d483d87f6d835979810d8",
            "9869d5020b9442d386fe7efc4aea13ee",
            "8a1a8c3fc5314a299e047e77e44baa6c",
            "bf05d5e10baa4fd3b6ddea303ddb2256",
            "cea53991a2504ec994a066869b228766",
            "8fed4c4413e84844b866610c1ae2e543",
            "030896ea905c4e33a0b09b5c2b91df8d",
            "d15bf42b50ce45519643198bc71c7189",
            "9186f0fd3a3c45febb6d93cb644e2c92",
            "deff42bd26af4886a08783dffdbb1762",
            "0cb6ffc37d094708b5e9fcbeb9455bad",
            "e518857a466e47369f9d3cdc74a1d7a2",
            "64712ec1db15405191814741d0d58d54",
            "15d8c8dbdc024cd181f985aa30aa2bf3",
            "824cac91a3b24673b7bde396ef2727a3",
            "b2925643f4154e51be28d0286d79bfec",
            "600221004c5f4f048b90cb3a765e0e3d",
            "1de7113124cf4602af457ecf083fe1b9",
            "5348b4204f164d3e8c6794aa5e43fef1",
            "eed15bd31f674d41bfc9b1fd7ad2865a",
            "e2ab002ea2a04cb4a4ed902f68af46c2",
            "135a7ad8a3094e09802782ff61106944",
            "7f5564e90f5b42bf86d5c1aefd6fa38b",
            "c6eb145483f1467f8b57f1fa0f33d37d",
            "a444b28a9e394d9885afb28f773125a1",
            "6b6537229d5047f48a753133eb6e09a4",
            "0dbefa9d3778408d9448737a38e9e47f",
            "458d903b5cf34cdb80e2dd123aebb8d7",
            "b5e26e135d9c47319e831ebdd424edbf",
            "4013c4c807544ec9be7c041438840155",
            "d6ab95fd212f487f9d2ef6b91307d6e3",
            "a3d30f90f7ee4301bbe6f5cedfb1ad52",
            "a48cc4e71b014bd2a199b074f9cd1145",
            "e0ac63da1ff54591b2d643641287cbbf",
            "53391bc961ae447c996ac275ed915b91",
            "a964752441484606ba4f430343ffaf06",
            "107c31760b5a471aa72faa04e49829df",
            "127aab0f8ac44da1b4ebcc3d8e63cb23",
            "1f249cbbec6a4ef99196b6b6d34bbed3",
            "6b1964f18bd54ef6a14d075dd1cac24c",
            "33c69223ee5d48d6b5c8fdecbb0c01e0",
            "323d7ea2a61f4732a6d49afad0f5eadd",
            "b4b6423782a04699a0562d536a30325e",
            "4a079110f7e94aaebfd402503d9bc1bf",
            "257e9e1e57bb48cf995df453b8a420ca",
            "d30345ffe824470db00a52f9d5f1ad14",
            "d567ae487fd647be9c7764888a5c206e",
            "23f0c5a7aed74bd7ad4856866f4540a6",
            "d4b824c3169944a1a144351f4131fde6",
            "e0d30ab7700a46a183619babc3f897f0",
            "92f16384bc7b4e67b285b97642abcc79",
            "3223f642d0164394b2060f62a4fab662",
            "3530f0f0dd73414785f00fb73b3f054a",
            "6325ddba501e4854b3e766dc90a6ae0d",
            "4558874906174a5284e09d1060cdbc2f",
            "3e97781f19cd45b5aedb88ad56c4df85",
            "ac439ec83ad341ed8a435b9ab0f4f4d2",
            "337ea26297e945ce8266bcfe1e6cf6a3",
            "1b0f926d66004c9291eece3da2f2de9b",
            "2564cbef53af4857b99ed8b0d5e4f631",
            "43524b873bdb414e9bbadb61b3083a69",
            "e9d9f51e981c4e49bd08d9cf0f90b5a5",
            "c1dfffa1216e4fc0b958fa710a248f51",
            "063818092bb34fa99eb4c7a848097295",
            "9d3d733d43224cada6252a1e7b634494",
            "819f0dbfc1f14496854d2bdef176fdeb",
            "2fb05494a30041cd838c46363aa72177",
            "ceb5b48dd3f34ea8ae0b3049ebfc940d",
            "068315d5461a4156b7953339b1986719",
            "4667dbc0485a457d89f0ee428ff7b520",
            "140def21d3934de28b61c7fb205d223d",
            "eb8e9c969d624109a2a65803b28eb508",
            "ee85dd2cfdb24a54b0396d35284b8e06",
            "9e2e59d221064c37a964988a4e535606",
            "a93464521350473792cf255a1779f314",
            "066e09fb65a2468f8cb907548820f20f",
            "db58bab90a4b4dadb057aca116568f45",
            "268c6fb5ca184491a25fb8ce9e298262",
            "dd08e342e2514c89bdfb1c09d6fbc463",
            "e4d11800969341cebb6757331bd178c1",
            "e4d336454e9d45b3ba5e2d979d6aedf2",
            "73a923fd631e4727a40bf76de0b298f9",
            "db1d000968434d489b8dc0e1653bff21",
            "d4e35cefa6714894a84ea3219efbd59c",
            "cc18f3e1bebb47fb86ea3663e0eef3dc",
            "775fac0f41d9419592666b2d1cbb9f1e",
            "ddf88c02c3ab4d8dad35fa4b578fd7bc",
            "1a17cf7acf6a4732930ffa92622bdf9d",
            "f4083df03b324ef5b845e62b8178c8cf",
            "79fd0cf43ba749bd95f7be86ffa78573",
            "97d9786b7c0844c8b941a277854ecbff",
            "95abfeca9c61442ab2578ea1af85bcd9",
            "4f6f1191db9a4fe086ddb16e432d128d",
            "33241b38e4d44d4684f0a13d7877a3d5",
            "9981a07f5a134fffa134c574e5217eed",
            "0a9042ac99f24a079aee198cf5fb7558",
            "e9b725d350dc40d8ac4acb48a00c93d0",
            "d2d532be3974458dbf5bc754a28cd187",
            "fe8941171b334cc99d2d3432adb428ae",
            "ca1c6b5354864e838f58bef31d5fdd7d",
            "770d4b8a19184f8f955d396f9eee83f1",
            "2433d5d954ce4155920dd6ab639add79",
            "47aa68372b45441b87ddf5ecb5788108",
            "475aa8e34ded4584b8f4d457b37ccede",
            "454cec3e28ca4b658b4bdd6d87cbc372",
            "699cd28fea29449599079bc8ca5e559e",
            "1828242555f645f7a99a53368bee388c",
            "bc9207e2201b4197bb0bc2e925a8257d",
            "8dd98f623ce94be783ac64e209668c64",
            "2b43c1b7ed2443a48e75d76798fde366",
            "2b41050933854471881a397b100f826a",
            "57e7d538c5d247e48bb35add4543c399",
            "b27d239da77348539aa66d607eb1a5dd",
            "d99296614f4c42b293932294e4843484",
            "48ff79dee05b40b8a944eceba9c3766c",
            "0623542f823c48449015a81ebc81970a",
            "edc6936b23ed4f239bc5d60ea7e3d097",
            "5c91b2d4e72943a588b36c55d6a58d98",
            "c2106e7e76ea42d595029d1ce6dbe98a",
            "3237bc4cb2a1411abd7558fb82622e7b",
            "24d73ccb6f764e569717da7e3dad29d3",
            "2bd14f78890945d4837c4f3ca157a52f",
            "6bc7abd8c5164f8c820a04b9d6b4ef96",
            "9caad59fbf054b5b8867d874f1eb5ae3",
            "021f21b3c69a4a19a522b3005892c261",
            "243c441bdf21437da9d552fa58836f55",
            "63a8d5d187734812b97806f8d86df042",
            "d82d92f5e0a946aa8db5d2d97440c266",
            "784697dea1af4b5d97415946cd34fbe7",
            "53c7dcb0dca44516ab5e3e701adedec2",
            "5cbf026ae7de4bba8517f3f7ae69d489",
            "f248f16349d945a28ebc46247fc94224",
            "eb02eff3bd9f4779a32adf7ca8d136c0",
            "8bb2c39508f7484dab0fd1779b98ee95",
            "827e93228aba42df9bc87bc79a9f9201",
            "fddeca728c0d453d880e5e655018b00e",
            "7fc171cf396a4848ac809ae985dd9dd9",
            "7abce722b8c544609ee0c54f9cbae2fb",
            "27d40d136eef4c32929f84bc4d6c9fa2",
            "1b5be45954c14a929b01ccdc8dc0d4de",
            "160ad0edf971464990c1c0a31f11f2f5",
            "99b5183ae9044898ab0b2127cfdcb3fd",
            "64a02e3e6daa4e8782c6f61ebac833a8",
            "d55b272c11e24824b69c99c6a14ba9d5",
            "554e0be96e2e42fd95cbc69b5cc6f943",
            "0d1195025681405a8b4039d6d1786c98",
            "6bf0650c77c7464cb4d392ce76751793",
            "9f03d0ff59f6409c84bfbc6c8fbca487",
            "e837d880a11d458297440570e5232b08",
            "df10cf2352ef467a8be9af2ab9b717d6",
            "726ed2e4e29a45aaada174fb4f153b0c",
            "5846f65c000344429d572c51d7e8aff5",
            "da9eb5395abf424fbe6526b47c8f6885",
            "50b2fb9a81d24b99b71b1ef2cff7c318",
            "a529d69a57ef4ea5a240648b6ad7b8ff",
            "9aa51aa717534cca8ef949442565e591",
            "46d8d0135fac4b56833933b4976e46ec",
            "ea312fc8f89e4f429cdef380b50413fd",
            "1d8ce5c75fde4f1b964befcf23098a64",
            "1635369fb8eb4ad8962caa60e0c2f9df",
            "a0960155c8fa4bad9c8affcde5f8bffc",
            "9e4466e48ffc4bdfbf3eb7a4815c0a42",
            "91675a8ee5b641fc8f477ce7d133d763",
            "c9f31526bec34b248dd492016d4d3e79",
            "a09b1066671c4b81a9a6b00bbdd67f89",
            "a440065f5d264241a3e8d4991d284be2",
            "c7b8f7cea8094c38a361d020cab28eb0",
            "5ff0f5c94d764aaf8915ff911550228b",
            "48021b6b861b4745b26eab08307bc5f3",
            "6b16e1d69ca34374880efc71a59e3e2d",
            "7b820ddb3a2f4efda47f24bed3f5cb3e",
            "b91264123e204511824b41c683243868",
            "534c19e989ce4839a0c2db0fe613f4ad",
            "09f4911448ab4e8fa712b2e48b9c94f8",
            "0c0a12b05248434abd2e5b8ac46e81c8",
            "cb675bc9805e42488a8e9896fa8a280e",
            "a0f8bb7e0fe9401fae333733506539ac",
            "1364f18fd1264c959af68200a708cb7f",
            "efa77316dad840ecb9651f1baf8eb9f4",
            "28266bf94f4e46f7a2f3b833bde1ca4a",
            "fdb2cffefc99431699b3fa28074b5fd5",
            "6ff41db968004756b7df218daa691c5c",
            "5b8e5db7bc3c4b1c8b3c042faeb6df5d",
            "76d7d2acf19b41a69012272bec78b5fe",
            "0ba86f112dc8420eb2ce938d5214c7c9",
            "e7e04371f908414c846f3e1a1b389fab",
            "72c16452d8f34762af2ac08eb2dc180a",
            "96cba7ba88f5486abaa783ac472d456e",
            "c0051ac721db4fb19a02ffcde4baf332",
            "e578adf1694241428ccec0fcf83c8d0a",
            "ce0386c49e624663b97d85be81a7140b",
            "f4b6fca90c23406c81dd049da82a3f92",
            "382c5fac714a4c10a236f65babd4f84e",
            "f04a7547a1ae4d1d95ea2c0c647a818b",
            "5632fefac34e4a5986812ec7e792c893",
            "097cf5f36af64169af99f59866bb53c4",
            "77f87e950ee442198947f3a564d6dd1d",
            "374ddb6bef654289b400f8b22f5989ca",
            "571fae4c9eaa48ba9d207a7df76c8b78",
            "2b381ce0a1784adfa8132e7acf250568",
            "0aefaa29fa234eefac76788a869e6bd1",
            "327c0cbf6b02417cba1efca787e2c89c",
            "a0ceb064e77b45bf847e60bf5d68808f",
            "6ec6324f86b146cabeb3bd35664629cc",
            "498257f63dfe470ca59bd929f946007f",
            "49fdc1e05fc3467692bd4cb887d1ac8b",
            "b23997c832f542b8b623b5d52d068598",
            "8f61151f7af149838fac109854f0cf1c",
            "bbd69292544940b68e4fe6383367cd4c",
            "93fc30f77f5a48c3b4bd748587ef7695",
            "47075213592a4ffa8e91267231c76a8e",
            "82643e5e4db44c91ad264eaaddf06cd0",
            "b69b40bf8d244383808bdde490a9ebec",
            "3578d3cdc1464f909c8f85d7eabec38d",
            "874d983e3aa94bfca37edda15355df79",
            "bb8d106bc95e4d14a7788047ac10f6ee",
            "84a0ef8f34a64b38abc14cd0f78c405e",
            "93f9023a80ec4631bb3d4fd48118c3f9",
            "7b38b723a06b4be1b286179a627313e4",
            "019dbf4f6b9f4764b1076870fc777ffc",
            "de6878f34b3b4f538fde3cf4e3fbae8f",
            "5822f9c3980e4de4a4fc63dd2f64094c",
            "68c1d3d783e842e1b95ade2660ab2d6c",
            "9070cf9df5234ed9b031a5a4ff0962d6",
            "17ba3b7adce84190a858ee8f1cdb5ef7",
            "62105298ad9c47269ad6009c4c9aa0ca",
            "fc09ecb3f9744272b56dc5d374c88a50",
            "806df9afce364a5295e0ed832073b926",
            "f0b3f370d2d444b58fda6dab4053c427",
            "db665c022b464c15bd4cfe4a0d92272f",
            "a585fedc02354aa8a2cc928f2b4d94b0",
            "bc97df0f95394a13a20b794e2d99bf5d",
            "8d4f89294a2e4eb3b70eef03b2fc4306",
            "377d201cc1b247dbbcdfb33d06b2c2a0",
            "acd38641ad844448a15ad89243bd83f3",
            "6517b1dc6f994e12befe6c6374b3f570",
            "7fc77843ee2f40daa1bfe5da36f98f2f",
            "897e3bc171554943bb0a1033e8d8e21f",
            "77f45e043e964ddfa91d59067ccc2bbe",
            "4d254747bae94e8fadf88fc83db702af",
            "bde7584f910d4472ba360b444e8fe913",
            "35bd4b1b1c3f44b2a45b0418e04dfb6f",
            "1e5082a712c0489794ac6db4d57f0a2b",
            "cb028e6a307e4e1ea56582dc06df421a",
            "e735a20fe7b94cbba19780e791746e47",
            "d838b6d84ab94f508a62f0a0f962bb10",
            "3babb45396a949a581c3eddd2f86f7c0"
          ]
        },
        "id": "frHTCv1i9xxr",
        "outputId": "607f9424-a467-4dd7-cf80-382fd06cc1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 14:31:53] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Starte Hardware-Optimierung: b560 (bigscience/bloom-560m)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a0508752a7a4618836b52926a8a3270"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6d57e9ad0bc45a091b2d4dff49b84c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd5dd607757e4e5e9b67b8454affb519"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75f702d6af9440bf88c3aa56d4bbede1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fed4c4413e84844b866610c1ae2e543"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "600221004c5f4f048b90cb3a765e0e3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "458d903b5cf34cdb80e2dd123aebb8d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f249cbbec6a4ef99196b6b6d34bbed3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0d30ab7700a46a183619babc3f897f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43524b873bdb414e9bbadb61b3083a69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb8e9c969d624109a2a65803b28eb508"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db1d000968434d489b8dc0e1653bff21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33241b38e4d44d4684f0a13d7877a3d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00000-of-00003.parquet:   0%|          | 0.00/280M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "454cec3e28ca4b658b4bdd6d87cbc372"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00001-of-00003.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0623542f823c48449015a81ebc81970a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/train-00002-of-00003.parquet:   0%|          | 0.00/273M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63a8d5d187734812b97806f8d86df042"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/validation-00000-of-00001.parquet:   0%|          | 0.00/474k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7abce722b8c544609ee0c54f9cbae2fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "de-en/test-00000-of-00001.parquet:   0%|          | 0.00/509k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e837d880a11d458297440570e5232b08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1635369fb8eb4ad8962caa60e0c2f9df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b820ddb3a2f4efda47f24bed3f5cb3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ff41db968004756b7df218daa691c5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beispiele gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_hardware/bloom_hardware_samples_b560.txt\n",
            "\n",
            "### Starte Hardware-Optimierung: b3b (bigscience/bloom-3b)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "382c5fac714a4c10a236f65babd4f84e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ec6324f86b146cabeb3bd35664629cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "874d983e3aa94bfca37edda15355df79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62105298ad9c47269ad6009c4c9aa0ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.01G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fc77843ee2f40daa1bfe5da36f98f2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beispiele gespeichert: /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_hardware/bloom_hardware_samples_b3b.txt\n",
            "\n",
            "Ergebnisse (gesamt):\n",
            "                model_id alias                    precision     time_s  \\\n",
            "0    bigscience/bloom-3b   b3b  fp16 (eager, compile=False)  45.777770   \n",
            "1  bigscience/bloom-560m  b560  fp16 (eager, compile=False)  48.226489   \n",
            "\n",
            "   energy_kwh   co2_kg  steady_time_s  steady_energy_kwh  steady_co2_kg  \\\n",
            "0    0.001468  0.00048      12.156497           0.000451       0.000001   \n",
            "1    0.001443  0.00016      37.375516           0.001135       0.000015   \n",
            "\n",
            "   kg_per_kwh  tokens_out         ppl       bleu    ram_GB  vram_alloc_GB  \\\n",
            "0    0.326743          96  222.807096  10.054131  3.251450       5.601610   \n",
            "1    0.110682          96  316.370096   5.069542  2.621548       1.049553   \n",
            "\n",
            "   vram_reserved_GB                                              notes  \n",
            "0          6.798828  attn=eager; compile=False; pinned=True; nb=Tru...  \n",
            "1          1.062500  attn=eager; compile=False; pinned=True; nb=Tru...  \n",
            "Gespeichert (gesamt): /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_hardware/bloom_hardware_results.csv\n",
            "\n",
            "Per-Phase Übersicht:\n",
            "  alias   phase     time_s  energy_kwh        co2_kg\n",
            "4   b3b  warmup  33.621273    0.001016  4.785424e-04\n",
            "5   b3b     gen   0.878295    0.000034  8.172183e-08\n",
            "6   b3b     ppl   4.711761    0.000153  3.643496e-07\n",
            "7   b3b    bleu   6.566440    0.000264  6.269695e-07\n",
            "0  b560  warmup  10.850973    0.000308  1.450642e-04\n",
            "1  b560     gen   0.790765    0.000025  1.199659e-05\n",
            "2  b560     ppl  11.785256    0.000356  8.467271e-07\n",
            "3  b560    bleu  24.799495    0.000753  1.790159e-06\n",
            "Gespeichert (per Phase): /content/drive/MyDrive/LLM-Effizienz/4_3_Effizienzstrategien/bloom_hardware/bloom_hardware_per_phase.csv\n"
          ]
        }
      ]
    }
  ]
}